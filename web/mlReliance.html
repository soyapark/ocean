<html>
  <head>
      <script src="https://www.gstatic.com/firebasejs/8.6.8/firebase-app.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.6.8/firebase-firestore.js"></script>
      
      <!-- TODO: Add SDKs for Firebase products that you want to use
      ¬† ¬† ¬†https://firebase.google.com/docs/web/setup#available-libraries -->
      
      <script>
      ¬† 
      </script>

      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
      <script type="text/javascript" src="script.js"></script>

      <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="navbar">
      <div id="move-container"><select name="cars" id="cars">
      <option value="volvo">‚öì</option>
      <option value="saab">Section</option>
      <option value="mercedes">‚öì & Section</option>
    </select> ‚¨ÖÔ∏è ‚û°Ô∏è</div>
      <div id="bridge-container">üåâ <span class='bridge-snippet'>hello world</span>
      <button class="close" onclick="cancelBridge()">X</button> </div>
      
    </div>

    <div class="sidebar">
      <div id="bib-bridge"></div>
      <div class="viewer"></div>
    </div>

    <div id="container">
        Blur text <input type="checkbox" id="myCheck" onclick="blurText(event)">
        <main> 
            <section class="front-matter"> 
             <section> 
              <header class="title-info"> 
               <div class="journal-title"> 
                <h1> <span class="title">Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks</span> <br /> <span class="subTitle"></span> </h1> 
               </div> 
              </header> 
              <div class="authorGroup"> 
               <div class="author"> 
                <span class="givenName">Zhuoran</span> 
                <span class="surName">Lu</span>, Purdue University, United States, 
                <a href="mailto:lu800@purdue.edu">lu800@purdue.edu</a> 
               </div> 
               <div class="author"> 
                <span class="givenName">Ming</span> 
                <span class="surName">Yin</span>, Purdue University, United States, 
                <a href="mailto:mingyin@purdue.edu">mingyin@purdue.edu</a> 
               </div> 
              </div> 
              <br /> 
              <div class="pubInfo"> 
               <p>DOI: <a href="https://doi-org.libproxy.mit.edu/10.1145/3411764.3445562" target="_blank">https://doi-org.libproxy.mit.edu/10.1145/3411764.3445562</a> <br />CHI '21: <a href="https://doi-org.libproxy.mit.edu/10.1145/3411764" target="_blank">CHI Conference on Human Factors in Computing Systems</a>, Yokohama, Japan, May 2021</p> 
              </div> 
              <div class="abstract"> 
               <p>This paper addresses an under-explored problem of AI-assisted decision-making: when objective performance information of the machine learning model underlying a decision aid is absent or scarce, how do people decide their reliance on the model? Through three randomized experiments, we explore the heuristics people may use to adjust their reliance on machine learning models when performance feedback is limited. We find that the level of agreement between people and a model on decision-making tasks that people have high confidence in significantly affects reliance on the model if people receive no information about the model's performance, but this impact will change after aggregate-level model performance information becomes available. Furthermore, the influence of high confidence human-model agreement on people's reliance on a model is moderated by people's confidence in cases where they disagree with the model. We discuss potential risks of these heuristics, and provide design implications on promoting appropriate reliance on AI.</p> 
              </div> 
              <div class="CCSconcepts"> 
               <ccs2012>
                <small> <span style="font-weight:bold;">CCS Concepts:</span> ‚Ä¢ <strong>Human-centered computing ‚Üí Empirical studies in HCI</strong>; ‚Ä¢ <strong>Computing methodologies ‚Üí Machine learning</strong>;</small> 
               </ccs2012> 
              </div> 
              <br /> 
              <div class="classifications"> 
               <div class="author"> 
                <span style="font-weight:bold;"><small>Keywords:</small></span> 
                <span class="keyword"><small>Machine learning</small>, </span> 
                <span class="keyword"> <small>appropriate reliance</small>, </span> 
                <span class="keyword"> <small>human-AI interaction</small></span> 
               </div> 
               <br /> 
               <div class="AcmReferenceFormat"> 
                <p><small> <span style="font-weight:bold;">ACM Reference Format:</span> <br />Zhuoran Lu and Ming Yin. 2021. Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks. In <em>CHI Conference on Human Factors in Computing Systems (CHI '21), May 8‚Äì13, 2021, Yokohama, Japan.</em> ACM, New York, NY, USA 16 Pages. <a href="https://doi-org.libproxy.mit.edu/10.1145/3411764.3445562" class="link-inline force-break" target="_blank">https://doi-org.libproxy.mit.edu/10.1145/3411764.3445562</a></small></p> 
               </div> 
              </div> 
             </section> 
            </section> 
            <section class="body"> 
             <section id="sec-2"> 
              <header> 
               <div class="title-info"> 
                <h2> <span class="section-number">1</span> INTRODUCTION</h2> 
               </div> 
              </header> 
              <p>AI-assisted decision-making has become increasingly ubiquitous over the past few years. From music recommendation&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0056">56</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0060">60</a>] to financial risk assessment&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>] to medical diagnosis&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>], various AI-driven decision aids, which are often powered by machine learning (ML) models, have been built and applied in a wide range of domains to aid humans in making better decisions. As a result, an increasingly large number of people are interacting with and potentially influenced by these ML models in their decision-making.</p> 
              <p>A critical step in advancing our knowledge of people's usage of AI-driven decision aids is to understand how do people determine how much they could rely on the ML models underlying these decision aids and adopt the recommendations they provide. Perhaps intuitively, an important kind of information that people utilize to calibrate their reliance on an ML model is the objective feedback on the model's performance. Indeed, previous research has shown that people adjust their reliance on an ML model based on its accuracy&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0064">64</a>], and correctness feedback on the level of individual decision-making task further helps people build ‚Äúmental models‚Äù of the ML model by understanding when the model is likely to err&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>]. Information on model's confidence, which is the model's own estimate of its likelihood of being correct, is also shown to significantly affect how much people would rely on the model&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0065">65</a>].</p> 
              <p>However, there are cases where external performance feedback of an ML model is not readily available. For example, model designers may fail to transparently communicate the model's performance to its end-users. Significant time delays may exist before one can meaningfully evaluate the performance of a model (e.g., ML models assisting college admission, long-term investment, and matchmaking). Sometimes, it is even impossible to fully observe the model's performance due to the decisions made (e.g., decide not to admit a student into college, thus observing how this student would do if admitted becomes impossible). An interesting question, then, is when salient performance feedback of an ML model is <em>absent</em> or <em>scarce</em> (e.g., performance information is not available or only available on the aggregate level) during people's interaction with an ML model, how would people decide how much to rely on the model? With limited performance feedback, are there any <em>heuristics</em> that people utilize to adjust their reliance on an ML model?</p> 
              <p>While few research has systematically examined how people would rely on an ML model when they have little information about how well the model performs, decades of research in social psychology on how people take the advice from other people into consideration reveals some consistent behavior pattern&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0050">50</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0063">63</a>]‚Äîfor example, the phenomenon of ‚Äú<em>naive realism</em>‚Äù&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0061">61</a>] suggests that people often consider their own judgment to be objective reflections of reality and tend to discount advice that is more different from their own opinion, while the ‚Äú<em>agreement-in-confidence heuristic</em>‚Äù&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>] further proposes that people may also utilize their own internal decision confidence in deciding how much to down-weight the opinion from a disagreeing advisor. Inspired by these findings, we hypothesize that people may adopt a similar strategy in determining how much to rely on an ML model when there is limited performance feedback. For example, people may believe their own decisions on those tasks that they feel confident about to be ‚Äúcorrect.‚Äù As a result, they may form a subjective evaluation of an ML model's performance based on how often the model agrees with themselves on tasks that they are confident‚Äîthe more often the model matches people's decisions, the more accurate people feel the model is, and the more they would be willing to rely on it.</p> 
              <p>To validate this hypothesis and thoroughly examine how people adjust their reliance on an ML model when they have limited access to performance feedback of the model, in this study, we conducted a series of three human-subject randomized experiments, focusing on answering the following questions:</p> 
              <ul class="list-no-style"> 
               <li id="list1" label="‚Ä¢">When people receive no information about an ML model's performance, does the level of agreement between people and the model on tasks that people have high confidence in affect people's reliance on the model?<br /></li> 
               <li id="list2" label="‚Ä¢">If so, does it continue to do so after people have had the opportunity to obtain some aggregate information about the model's performance (e.g., the model's overall accuracy on a set of decision-making tasks) in practice?<br /></li> 
               <li id="list3" label="‚Ä¢">In the real world, people may encounter both cases that they feel confident and cases that they are not confident when interacting with an ML model. How does the people's own confidence in those cases that they agree or disagree with the model change their reliance on the model?<br /></li> 
              </ul> 
              <p>In our experiments, subjects were recruited from Amazon Mechanical Turk to make a sequence of predictions on the outcomes of speed dating events with the help from ML models. The prediction tasks were divided into two phases. In both our first and second experiment, subjects in different experimental treatments worked on exactly the same tasks, and the first phase was composed of only tasks that people would feel confident about their own decisions according to a pilot study we conducted. Then, in the first experiment, subjects were randomly assigned to one of the three treatments, and the ML model used in different treatments made different predictions in the first phase, leading to varying levels of agreement between subjects and the model on tasks that subjects had high confidence in. In addition, subjects received no performance feedback about the ML model at any point throughout the experiment. Our second experiment was completely analogous to the first experiment, except that we revealed the model's overall accuracy in Phase 1‚Äîwhich was designed to be either relatively low (50%) or high (80%)‚Äîto subjects between the two phases. Finally, in our third experiment, in order to vary people's confidence in their agreement or disagreement with the model, we used different tasks in the first phase of the experiment across the 4 experimental treatments, which were arranged in a 2 &times; 2 design along two factors‚Äîpeople's confidence in their own decisions when they agree with the model (high vs. low), and people's confidence in their own decisions when they disagree with the model (high vs. low). In all three experiments, we measured subject's reliance on the model through their willingness to follow the model's recommendations in Phase 2, and we also collected information on subject's perceptions of the model on a variety of reliance-related factors through surveys.</p> 
              <p>Our results show that when there is limited performance feedback about an ML model, people adjust their reliance on the model in a sophisticated way‚ÄîWhen they have no information about a model's performance at all, people's reliance on the model is significantly influenced by the level of agreement between the model and themselves on tasks that they have high confidence in. However, once people have obtained some aggregate-level performance information about the model, people's reliance on the model is mostly affected by the model's observed performance, but not the level of high confidence agreement with the model any more. Finally, we detect an interaction effect between people's confidence in their agreement with a model and people's confidence in their disagreement with the model in influencing people's reliance on the model. For example, having high confidence disagreement with an ML model would reduce people's reliance on the model if people agree with this model mostly on tasks that they are confident, but it brings about no impact on reliance if people agree with this model mostly on tasks that they have low confidence in.</p> 
              <p>Taken together, our study shows that with limited performance feedback of an ML model, people indeed adopt some heuristics to adjust their reliance on the model based on the level of agreement between the model and themselves. Moreover, how the human-model agreement/disagreement affects reliance on the model varies with people's confidence in their agreement and disagreement with the model, and it may also change after people have obtained some aggregate information about the model's performance in practice. We highlight that the usage of these heuristics have both benefits and risks. Notably, when people are not aware of their own limitations in decision making (e.g., biases or poor degree of calibration), they may show over-reliance on models sharing the same biases as themselves and show under-reliance on complementary models. On the other hand, by overly relying on a model's observed performance in practice to gauge how much to rely on the model, people might miss the opportunity to leverage useful information carried in their agreement and disagreement with the model to further differentiate reliable models from the unreliable ones. We conclude by discussing the design implications and limitations of our work.</p> 
             </section> 
             <section id="sec-3"> 
              <header> 
               <div class="title-info"> 
                <h2> <span class="section-number">2</span> RELATED WORK</h2> 
               </div> 
              </header> 
              <p>The rapid development of automation over the past decades has inspired active research in understanding reliance on automation&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0049">49</a>]. For example, researchers have identified that reliance on automated systems could be affected by factors related to humans (e.g. trust disposition, affective process)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>], factors related to the automation (e.g. error types)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0052">52</a>], as well as factors related to the context (e.g. time pressure, risk)&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>]. Theoretical models and frameworks have been proposed to explain human's reliance on the automation in various situations&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0057">57</a>]. In practice, however, collaborations between humans and automation often fail to achieve optimal outcomes due to human's inappropriate reliance on the automation&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0043">43</a>], including both over-reliance on the automation when it performs poorly (i.e., misuse) and under-reliance on the automation when it is well-functioning (i.e., disuse).</p> 
              <p>More recently, with the widespread usage of machine learning (ML) based technologies in aiding human decision-making, a new line of research examining human's reliance on ML emerges&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0055">55</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0064">64</a>]. Compared to traditional automated systems, ML models are built on a massive amount of data. As a result, they often become highly sophisticated and sometimes even opaque in order to capture the complex nonlinearity in real-world data, and uncertainty is baked into every prediction of ML models. Correspondingly, studies on how people rely on ML models often emphasize on people's capability of utilizing external, explicit information about a model, such as explanations and uncertainty qualifications of the model's decisions to adjust their reliance on the model. For example, it was shown that people's reliance on an ML model is affected by the model's stated accuracy on a set of held-out data&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>], the model's observed accuracy on real-world trials&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0064">64</a>], and the model's confidence associated with each individual recommendation&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0065">65</a>]. On the other hand, while a number of studies have shown the promise that explanations of an ML model enhance people's understanding of the model and therefore lead to more appropriate reliance on the model&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0048">48</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0062">62</a>], the impact of model intelligibility and transparency on reliance also seems to vary across decision-making tasks of different properties, explanations of different types, and people with different characteristics&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0047">47</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0053">53</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0065">65</a>].</p> 
              <p>In comparison to the growing literature on how people's reliance on an ML model is affected by external information about the model, much less attention has been paid to understand how people's reliance on an ML model is determined when people have limited access to such information. In a different context of human advice-taking, however, psychologists have revealed rich insights into how people decide whether to rely on another human advisor's advice during their decision making without objective feedback on the quality of the advice. For example, research has identified that people exhibit a degree of ‚Äú<em>naive realism</em>‚Äù&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0061">61</a>], that is, they tend to believe they perceive the world objectively and people who disagree with themselves are uninformed or biased. As a result, people often discount or even ignore the advice provided by advisors who disagree with them&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0063">63</a>]. In practice, it was shown that using the frequency of agreement between oneself and an advisor as a proxy can help reliably estimate the quality of the advisor's advice when judgements between the person and the advisor are independent, but could also cause systematic errors in decision-making when the person suffers from the ‚Äú<em>false-consensus effect</em>‚Äù&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0050">50</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0063">63</a>] and shows more trust to advisors who simply share the same biases as themselves&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>]. Moreover, as humans often use their own confidence as a way to express their estimated likelihood of making correct decisions&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>], it was found that both the advisor and advisee's confidence in their own decisions, as well as the advisee's confidence in their agreement with the advisor, affects the likelihood of the advice being taken&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0054">54</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0058">58</a>]. In particular, previous study showed that when the agreement between advisor and advisee mostly occurs on cases that the advisee is confident, the advisee shows higher levels of trust towards the advisor, and also gets influenced by the advisor more&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>].</p> 
              <p>Inspired by these findings, we hypothesize that when interacting with an ML model, people may also use their agreement with the model as a heuristic to gauge how much they could rely on the model when performance feedback of the model is limited. To this end, while a few recent studies have examined how the alignment between a model's explanations and human logic affects people's reliance on the model&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0041">41</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0066">66</a>], there is no systematic investigation into how the agreement between people and a model on decisions they make affects reliance, thus our study fills in this gap.</p> 
             </section> 
             <section id="sec-4"> 
              <header> 
               <div class="title-info"> 
                <h2> <span class="section-number">3</span> EXPERIMENT 1</h2> 
               </div> 
              </header> 
              <p>In Experiment 1, we set out to understand how do people adjust their reliance on a machine learning (ML) model when they have <em>not</em> obtained any feedback on how well the model performs. We speculate one heuristic that people may adopt in such a scenario is to use the level of agreement between the ML model and themselves on tasks that <em>they are highly confident about their own predictions</em> as a proxy to estimate the performance of the model and adjust their levels of reliance accordingly. Specifically, we hypothesize that:</p> 
              <ul class="list-no-style"> 
               <li id="list4" label="‚Ä¢"><strong>[H1]</strong> When people receive no information about an ML model's performance, the level of agreement between people and the model on tasks that people have high confidence in significantly affects people's reliance on the model; the higher the level of agreement, the more people rely on the model.<br /></li> 
              </ul> 
              <p>To formally test this hypothesis, we conducted a randomized experiment where human subjects were recruited from Amazon Mechanical Turk (MTurk) to complete a sequence of 30 decision-making tasks, with the help of different ML models, which showed varying levels of agreement with subjects on tasks that subjects had high confidence in.</p> 
              <section id="sec-5"> 
               <header> 
                <div class="title-info"> 
                 <h3> <span class="section-number">3.1</span> Experimental Task</h3> 
                </div> 
               </header> 
               <p>The decision-making task that we asked subjects to complete in this experiment is to predict the outcome of speed dating events. This task is suitable for our experimental study because human subjects do not need to possess specialized knowledge to make predictions on such a task, and subjects may feel confident about their own predictions on some of these tasks. Moreover, it also represents a realistic scenario where people can be assisted by an ML model to make better decisions. Similar tasks have been used in previous research to understand how performance information of an ML model affects people's trust and reliance on ML models&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0064">64</a>].</p> 
               <p>Specifically, in each task, the human subject was presented with a set of information about one participant in a speed dating event and his/her date, including a total of 22 features on (1) basic demographics of the participant and the date (e.g., gender, age, race), (2) the participant's preferences in romantic partners (e.g., out of 100 points, how many points does the participant assign to attractiveness when evaluating whether a person is an ideal romantic partner or not?), and (3) the participant's impression of the date (e.g., the participant's rating of the attractiveness of the date). After the subject reviewed these information, she was asked to make a binary prediction on whether the participant would like to meet with the date again in the future, following the steps as listed below:</p> 
               <ul class="list-no-style"> 
                <li id="list5" label="‚Ä¢">First, the subject was asked to make an initial prediction <em>on her own</em> about whether the participant in this event wanted to see his or her date again.<br /></li> 
                <li id="list6" label="‚Ä¢">Then, the subject was presented with the information on an ML model's binary prediction on this task, and accordingly, whether the model agreed with her or not.<br /></li> 
                <li id="list7" label="‚Ä¢">Finally, the subject was asked to submit a <em>final</em> prediction.<br /></li> 
               </ul> 
               <figure id="fig1"> 
                <img src="https://dl-acm-org.libproxy.mit.edu/cms/attachment/9798b47e-da24-4158-803e-3081fd5c401b/chi21-519-fig1.jpg" class="img-responsive" alt="The interface of the prediction task" aria-describedby="fig002" /> 
                <p style="display:none" id="fig002">From top to bottom, the figure shows the profile of the participant and her dates, the first question to be answered by the subject, the prediction of the ML, and the second question to be answered.</p> 
                <figcaption> 
                 <span class="figure-number">Figure 1:</span> 
                 <span class="figure-title">Interface of the prediction task.</span> 
                </figcaption> 
               </figure> 
               <p>Figure&nbsp;<a class="fig" href="#fig1">1</a> shows an example of the interface of the prediction task in our experiment. The information on speed dating participants and their dates that we showed to subjects in each task were taken from real speed dating events that were conducted in the experimental study of Fisman et al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>], which also provided us with the ground truth answer of whether participants in each speed dating event were willing to see each other again.</p> 
               <p><strong>Task Instance Categorization.</strong> To enable our experimental manipulation (i.e., varying the level of agreement between an ML model and subjects on tasks that subjects are highly confident; will be detailed in Section&nbsp;<a class="sec" href="#sec-6">3.2</a>), we first conducted a pilot study on a set of 214 speed dating outcome prediction tasks to collect necessary information on them, including what the subjects‚Äô majority prediction on a task is, whether the majority prediction is correct, and how confident subjects are about their predictions. A total of 315 subjects were recruited from MTurk to take our pilot study, in which they needed to complete a random sample of 20 prediction tasks, and for each task, they were also asked to indicate their confidence level in their prediction. Based on these data, for each task, we determined subjects‚Äô majority prediction on it, the accuracy of the majority prediction, as well as the average confidence of subjects‚Äô predictions on the task. For more details of the pilot study, see the supplementary materials (SM) Section 2.</p> 
               <p>Using the first quartile and third quartile of the average confidence on a task as the thresholds, we identified a set of 54 tasks that subjects are highly confident about their predictions (i.e., subjects‚Äô average confidence is above Q3) and 52 tasks that subjects have low confidence in their predictions (i.e., subjects‚Äô average confidence is below Q1). Note that when subjects are highly confident about their own predictions, they are <em>not</em> necessarily correct. This could be caused by a number of reasons, including subjects overestimating or underestimating the importance of certain features in influencing the speed dating outcome, subjects‚Äô over-confidence in their own predictions, and a degree of uncertainty inherent in predicting human behavior.</p> 
              </section> 
              <section id="sec-6"> 
               <header> 
                <div class="title-info"> 
                 <h3> <span class="section-number">3.2</span> Experimental Design</h3> 
                </div> 
               </header> 
               <p>Each subject completed exactly the <em>same</em> 30 prediction tasks in our experiment, which were divided into two phases. Phase 1 consisted of the first 20 tasks in the sequence, and we varied the level of agreement between people and the ML model on these tasks to create our experimental treatments. Phase 2 included the rest 10 tasks, and was designed for measuring how much people would be willing to rely on the ML model in their decision-making.</p> 
               <p><strong>Experimental Treatments.</strong> Three treatments were created in this experiment. Each treatment was associated with a unique ML model, and the three models differed on how often their predictions agreed with those of subjects‚Äô on <em>Phase 1</em> tasks. Importantly, the 20 prediction tasks of Phase 1 were all selected from those <em>high</em> confidence tasks that we identified from our pilot study. To make it easy to control the level of agreement between the ML models and the human subjects, the 20 tasks we included in Phase 1 further satisfied an additional criterion that for each task, our pilot study had shown that at least 80% of subjects made the <em>same</em> prediction on it. Thus, depending on whether people's majority prediction on a task was correct or not, Phase 1 included both tasks on which most subjects would make <strong>c</strong>orrect predictions with <strong>h</strong>igh confidence (‚ÄúCH‚Äù tasks) and tasks on which most subjects would make <strong>i</strong>ncorrect predictions with <strong>h</strong>igh confidence (‚ÄúIH‚Äù tasks). </p>
               <figure id="fig2"> 
                <img src="https://dl-acm-org.libproxy.mit.edu/cms/attachment/2cbfb5ff-805f-41ad-b0f2-d4ccf71f6fa2/chi21-519-fig2.jpg" class="img-responsive" alt="Predictions of the three ML models on Phase 1 tasks of Experiment 1 in three treatments" aria-describedby="fig003" /> 
                <p style="display:none" id="fig003">Four kinds of tasks were used in Phase 1 of Experiment 1 as follow: majority correct with high confidence, ground truth = 1(CH1), majority correct with high confidence, ground truth = 0(CH0), majority incorrect with high confidence, ground truth = 1(IH1), majority incorrect with high confidence, ground truth = 0(IH0). In the high agreement treatment, the model made positive predictions on 10 CH1 and 4 IH0, and negative predictions on rest of the tasks. In the medium agreement treatment, the model made positive predictions on 10 CH1, 3 CH0, and 1 IH0, and negative predictions on rest of the tasks. In the low agreement treatment, the model made positive predictions on 8 CH1, 4 CH0, and 2 IH1, and negative predictions on rest of the tasks.</p> 
                <figcaption> 
                 <span class="figure-number">Figure 2:</span> 
                 <span class="figure-title">Predictions of the three ML models on Phase 1 tasks of Experiment 1. Each row represents the predictions made by the ML model in one treatment: ‚ÄúA‚Äù means the model agreed with the predictions made by the majority of people on the tasks, and ‚ÄúD‚Äù means the model disagreed with the majority predictions. Bars with stripes represent tasks on which the model's prediction agreed with people's majority prediction. Bars with the green (red) outline are tasks where the model made correct (wrong) predictions. Bars with the yellow background are tasks where the model made a positive prediction (i.e., predicted 1, the participant wants to see the date again). On the 20 tasks in Phase 1, the three models have the same accuracy of 70%, the same positive prediction rate of 70%, but different levels of agreement with people's majority predictions.</span> 
                </figcaption> 
               </figure> 
               <p></p> 
               <p>Given our knowledge of what the majority of people would predict for each task, we then created three experimental treatments where the ML model used in these treatments was designed to make the <em>same</em> predictions as the majority of people on 40% (low agreement), 70% (medium agreement), or 100% (high agreement) of the 20 Phase 1 tasks, respectively<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>. To single out the effect of the human-model agreement on reliance, we minimized the differences across treatments by ensuring that the three ML models had exactly the <em>same accuracy</em> and the <em>same positive prediction rate</em><a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> on these 20 tasks (both accuracy and positive prediction rate were set at 70%). Practically, we did this by first setting the ML model's prediction to be the same as people's majority prediction on all Phase 1 tasks in order to create the ‚Äúhigh agreement‚Äù treatment. The other two treatments were then created by ‚Äúflipping‚Äù the model's predictions on some tasks, hence decrease the level of agreement between people and the model. Importantly, whenever we had the model disagree with people's majority prediction on a task that the majority prediction is correct, we also needed to have the model disagree with people's majority prediction on another task that the majority prediction is wrong (hence the model accuracy was kept at the same level across treatments). Similarly, changing the model's prediction on a task where the majority of people made a positive prediction was always accompanied by changing the model's prediction on another task for which people's majority prediction was negative (hence the model's positive prediction rate was kept at the same level across treatments).</p> 
               <p>Figure&nbsp;<a class="fig" href="#fig2">2</a> illustrates the details for the 20 task instances that we selected to use in Phase 1 of this experiment, as well as the ML models‚Äô predictions on them in the three treatments. With such design, we expect that human subjects who are assigned to the high agreement treatment find that on Phase 1 tasks, there is a higher level of agreement between the predictions made by the model and the <em>initial</em> predictions made by themselves, compared to subjects who are assigned to the medium agreement treatment, who in turn have a higher level of agreement with the model than subjects in the low agreement treatment.</p> 
               <p>Lastly, Phase 2 was designed to measure whether the perceived level of agreement between people and an ML model (in Phase 1) affects people's reliance on the model. Thus, ML models in the three treatments were designed to make exactly the <em>same</em> predictions on the 10 tasks in Phase 2. To maximize the possible effect, all Phase 2 tasks were selected from those task instances that people had <em>low</em> confidence in their own predictions according to the pilot study. To minimize the difference between the two phases, we made sure that the model's accuracy and positive prediction rate in Phase 2 were still 70%, which was the same as that in Phase 1.</p> 
               <p><strong>Experimental Procedure.</strong> Our experiment was posted as a HIT on MTurk that was open to U.S. workers only, and each worker was allowed to take the HIT only once. Each HIT contained exactly the <em>same</em> 30 prediction tasks separated in two phases, and the order of tasks was randomized within each phase. Besides, an attention check question was added at a random position in the task sequence, and the subject was asked to select some pre-specified options in the attention check question.</p> 
               <p>Upon arrival at the HIT, each subject was first presented with an instruction of the task interface. Then, the subject needed to answer three qualification questions to show that she understood the meanings of various information presented to her on the task interface, and she could only proceed on after correctly answering all these questions. Once qualified, the subject would be randomly assigned to one of the three experimental treatments to work on the sequence of 30 prediction tasks. On each task, the subject was asked to make initial and final predictions following the three-step procedure described above. On tasks in Phase 1, subjects in different treatments saw the ML model making <em>different</em> predictions, leading to different perceived levels of agreement with the model. The subject did <em>not</em> receive any feedback on whether her own prediction or the model's prediction was correct on each of the tasks. After completing all 20 tasks in Phase 1, the subject was asked to take a pause and reflect on her experience in the first phase of the experiment by filling in a few survey questions. In particular, the subject was asked to first make a guess about the model's accuracy in Phase 1. Then, the subject rated on a 7-point Likert scale about how much she agrees with each of the following statements from 1 (strongly disagree) to 7 (strongly agree):</p> 
               <ul class="list-no-style"> 
                <li id="list8" label="‚Ä¢"><strong>(Competence)</strong> The recommendation that the machine learning model provides to me is as good as that which a highly competent person could provide.<br /></li> 
                <li id="list9" label="‚Ä¢"><strong>(Reliability)</strong> The machine learning model provides the reliable recommendation to me in each task.<br /></li> 
                <li id="list10" label="‚Ä¢"><strong>(Understandability)</strong> I understand how the machine learning model will assist me with predictions I have to make.<br /></li> 
                <li id="list11" label="‚Ä¢"><strong>(Faith)</strong> If I am not sure about my prediction in a task, I have faith that the machine learning model will provide the best solution.<br /></li> 
               </ul> 
               <p>After completing the survey, the subject proceeded to Phase 2 to work on the rest of 10 prediction tasks. Again, the subject was asked to make initial and final predictions on each of the Phase 2 tasks, <em>without</em> receiving any feedback on their own accuracy or the model's accuracy on any task. Finally, before submitting the HIT, the subject was asked to answer the following question to indicate her overall trust in the ML model on a 7-point Likert scale, with 1 representing ‚Äúdo not trust the model at all‚Äù and 7 representing ‚Äúfully trust the model.‚Äù:</p> 
               <ul class="list-no-style"> 
                <li id="list12" label="‚Ä¢"><strong>(Overall trust)</strong> Overall, how much do you trust the predictions of our machine learning model?<br /></li> 
               </ul> 
               <p>The base payment of our HIT was <font style="normal">$</font>1.5. To motivate subjects to carefully consider whether and how much to rely on the ML model when making their predictions, we also provided a performance-based bonus to subjects: after the subject completed the HIT, we randomly selected one prediction task in the sequence to check whether the subject's <em>final</em> prediction on that task was correct. If so, the subject would receive a <font style="normal">$</font>1 bonus on top of the base payment. Note that in this experiment, we <em>never</em> provided any feedback to subjects on the accuracy of the ML model on any of the tasks.</p> 
              </section> 
              <section id="sec-7"> 
               <header> 
                <div class="title-info"> 
                 <h3> <span class="section-number">3.3</span> Experimental Data and Analysis Methods</h3> 
                </div> 
               </header> 
               <p>In total, 301 subjects passed the attention check and completed our HIT. For each subject, we recorded her initial prediction and final prediction on each of the 30 prediction tasks. Besides, we also recorded the subject's responses to our survey questions both between the two phases and at the end of the HIT.</p> 
               <p>Similar to previous studies&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0064">64</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0065">65</a>], based on the information we collected in our experiment, we operationalized the measurement of reliance on ML models using two metrics which evaluate subject's willingness to follow the model's predictions in Phase 2:</p> 
               <ul class="list-no-style"> 
                <li id="list13" label="‚Ä¢"><strong>Agreement fraction</strong>: in Phase 2, the fraction of tasks on which subject's <em>final</em> prediction agreed with the model's prediction<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>.<br /></li> 
                <li id="list14" label="‚Ä¢"><strong>Switch fraction</strong>: in Phase 2, the fraction of tasks on which subject's final prediction agreed with the model's prediction, among all tasks that subject's initial prediction was different from that of the model's.<br /></li> 
               </ul> 
               <p>Intuitively, the higher agreement fraction and switch fraction are, the more subjects rely on the ML model in their decision-making. Since Phase 2 tasks were exactly the same across the three treatments and the model in different treatment also made exactly the same predictions on these tasks, differences in agreement fraction and switch fraction across treatments can be causally attributed to the subject's perceived level of agreement with the model in Phase 1. To formally examine whether the level of agreement between people and an ML model in Phase 1 affects people's reliance on the model, we conducted the one-way analysis of variance (ANOVA) on the two measures of reliance (i.e., agreement fraction, switch fraction) across the three treatments<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>.</p> 
               <p>In addition, to explore the mechanisms through which the human-model agreement on tasks that people have high confidence in affects people's reliance on the model, we also analyzed the data on subject's perceived accuracy, technical competence, reliability, and understandability of the model, as well as subject's faith and overall trust in the model, all of which are previously identified as key constructs that would affect people's reliance in an automated system&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0051">51</a>]. In particular, we conducted one-way ANOVA on subject's perceived model accuracy and overall trust on the model. We also conducted proportion tests on the proportion of subjects who agreed/disagreed with the four statements regarding the competence, reliability, understandability of the model, and their faith in the model to see whether subject's perceptions on these aspects differ significantly across treatments<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a>. Following all these tests, post-hoc Tukey HSD tests or pairwise proportion tests were then used to identify pairs of treatments in which subjects exhibited significant differences in their reliance on the model or their reliance-related perceptions in the model (we adjusted p-values in post-hoc analyses to control for a family-wise error rate of 0.05).</p> 
              </section> 
              <section id="sec-8"> 
               <header> 
                <div class="title-info"> 
                 <h3> <span class="section-number">3.4</span> Experimental Results</h3> 
                </div> 
               </header> 
               <p>To begin with, we checked whether we successfully created different levels of agreement between subjects and the ML model in different treatments. The average fraction of Phase 1 tasks on which subjects‚Äô initial predictions were the same as the model's predictions was 0.446, 0.673, and 0.902 for low, medium, and high agreement treatment, respectively, and one-way ANOVA test confirmed that the difference was statistically significant (<em>p</em> &lt; 0.001). This means that the agreement level between people and the model was successfully varied across the three treatments in this experiment. </p>
               <figure id="fig3"> 
                <img src="https://dl-acm-org.libproxy.mit.edu/cms/attachment/e73a62cd-fbc5-4c71-ad95-5f28cb8c8afa/chi21-519-fig3.jpg" class="img-responsive" alt="Figure 3" aria-describedby="fig004" /> 
                <p style="display:none" id="fig004">The figures show consistent upward trends. Among the three treatments, higher the agreement level, higher the agreement fraction and switch fraction on all Phase 2 tasks and Phase 2 tasks where the model made correct predictions. On Phase 2 tasks where the model made wrong predictions, treatments with medium and high level of agreement(‚Äùmedium‚Äù treatment and ‚Äùhigh‚Äù treatment) have higher agreement fraction than the treatment with low level of agreement(‚Äùlow‚Äù treatment), and ‚Äùhigh‚Äù treatment has higher switch fraction than ‚Äùmedium‚Äù and ‚Äùlow‚Äù treatment.</p> 
                <figcaption> 
                 <span class="figure-number">Figure 3:</span> 
                 <span class="figure-title">The average values of agreement fraction and switch fraction in Phase 2 across three treatments in Experiment 1, on all Phase 2 tasks (Fig.&nbsp;3a, 3d), on the subset of Phase 2 tasks where the ML model was correct (Fig.&nbsp;3b, 3e), and on the subset of Phase 2 tasks where the ML model was incorrect (Fig.&nbsp;3c, 3f). Error bars represent the standard errors of the mean. Upward trends are observed in all plots, indicating H1 is supported.</span> 
                </figcaption> 
               </figure> 
               <strong>Does High-Confidence Agreement Affects Reliance When Performance Feedback is Absent?</strong> We plot the comparisons of subjects‚Äô average agreement fractions and average switch fractions on all Phase 2 tasks across the three treatments in Figures&nbsp;3a and&nbsp;3d, respectively. Visually, it is clear that when subjects did not know how well a model performs, they were more likely to follow the predictions of the model if there was a higher level of agreement between the model and themselves on tasks that they are highly confident about their own predictions. Our one-way ANOVA test results confirmed that the differences in subject's reliance on the model across treatments were statistically significant (agreement fraction: 
               <em>F</em>(2, 298) = 3.67, 
               <em>p</em> = 0.027; switch fraction: 
               <em>F</em>(2, 298) = 5.24, 
               <em>p</em> = 0.006). Post-hoc pairwise comparisons further suggested that the differences in subjects‚Äô reliance on the ML model were significant between the low agreement and high agreement treatments (
               <em>p</em> = 0.037, Cohen's 
               <em>d</em> = 0.34 for agreement fraction; 
               <em>p</em> = 0.004, 
               <em>d</em> = 0.49 for switch fraction).
               <p></p> 
               <p>A closer look into the data further suggests that higher levels of high-confidence agreement between an ML model and people lead to increased reliance on the model <em>regardless of</em> whether the model's predictions are correct or not. For example, Figures&nbsp;3b and&nbsp;3e show subject's reliance on the ML model on Phase 2 tasks where the model made <em>correct</em> predictions, while Figures&nbsp;3c and&nbsp;3f compare subject's reliance on the ML model across the three treatments on Phase 2 tasks where the model was <em>wrong</em>. Although it seems that subjects tended to rely on the model more when the model was correct, we observed the same increasing trend with respect to how the level of human-model agreement on high confidence tasks affects people's reliance on the ML model, both when the model was correct and when the model was wrong. Indeed, on Phase 2 tasks where the model made correct predictions, the differences in subject's switch fraction across the three treatments were found to be statistically significant (<em>F</em>(2, 295) = 5.56, <em>p</em> = 0.005), so do the differences in subject's agreement fraction on Phase 2 tasks where the model made wrong predictions (<em>F</em>(2, 298) = 4.94, <em>p</em> = 0.008).</p> 
               <p><strong>Exploring Why High-Confidence Agreement Affects Reliance.</strong> To gain some insights into why agreement level between a subject and the model in Phase 1 significantly affects subject's reliance on the model, we proceed to examine how such agreement influenced subject's perceptions of the model on a variety of aspects that relate to reliance behaviors.</p> 
               <p>We found that, in fact, without observing a model's performance, subjects who experienced a higher level of agreement with the model on tasks that they had high confidence in tended to consider the model to be <em>more</em> accurate, <em>more</em> competent, <em>more</em> reliable, more understandable and they also have <em>more</em> faith and trust in the model. One-way ANOVA and proportion test results again showed that the differences in subject's perceptions on all these reliance-related aspects were consistently and significantly different across treatments (<em>p</em> &lt; 0.001). This provides a solid explanation for why subjects in different treatments show different levels of reliance on the models in Phase 2, despite all models made exactly the same predictions on Phase 2.</p> 
               <p>Overall, the results we obtained from our Experiment 1 support <strong>H1</strong>. That is, we indeed found that people tend to rely on an ML model more if the model has a higher level of agreement with them on cases that they feel confident about. Our data suggest that people may have used the level of such high confidence agreement as a proxy to gauge the key properties of the ML model, such as the model's accuracy, competence and reliability, which then guide people to adjust their reliance on the model accordingly.</p> 
              </section> 
             </section> 
             <section id="sec-9"> 
              <header> 
               <div class="title-info"> 
                <h2> <span class="section-number">4</span> EXPERIMENT 2</h2> 
               </div> 
              </header> 
              <p>Experiment 1 shows that the agreement level between people and the model on decision-making tasks that people are highly confident about their own decisions significantly affects people's reliance on the model when they receive no information about the ML model's performance at all. However, such agreement may not necessarily be an accurate approximation of the performance of the ML model. Naturally, one may wonder whether the level of high-confidence agreement between people and a model still affects people's reliance on the model <em>after</em> people obtaining some feedback on how well the model performs, such as the aggregate information of the model's accuracy on a set of decision-making tasks. </p>
              <figure id="fig4"> 
               <img src="https://dl-acm-org.libproxy.mit.edu/cms/attachment/7006fbc0-bd2f-4fc5-9a8c-325dfd5b5659/chi21-519-fig13.jpg" class="img-responsive" alt="Different predictions of the four ML models on Phase 1 tasks of Experiment 2 in four treatments" aria-describedby="fig005" /> 
               <p style="display:none" id="fig005">Four kinds of tasks are used in Phase 1 of Experiment 1 as follow: majority correct with high confidence, ground truth = 1(CH1), majority correct with high confidence, ground truth = 0(CH0), majority incorrect with high confidence, ground truth = 1(IH1), majority incorrect with high confidence, ground truth = 0(IH0). In each treatment, there are the same 20 tasks consist of 10 CH1, 4 CH0, 2 IH1, and 4IH0. In the low agreement low accuracy treatment, the model made positive predictions on 6 CH1, 3 CH0, 2IH1, and 3 IH0, and negative predictions on the rest of tasks. In the low agreement high accuracy treatment, the model made positive predictions on 8 CH1, 2 CH0, and 4 IH0, and negative predictions on the rest of the tasks. In the low agreement high accuracy treatment, the model made positive predictions on 9 CH1, 3 CH0, and 2 IH1, and negative predictions on the rest of the tasks. In the high agreement high accuracy treatment, the model made positive predictions on 10 CH1, 1 CH0, 1 IH1, and 2 IH0, and negative predictions on rest of the task.</p> 
               <figcaption> 
                <span class="figure-number">Figure 4:</span> 
                <span class="figure-title">Predictions of the ML models on Phase 1 tasks in the four treatments of Experiment 2. Each row represents a unique model used in one treatment. ‚ÄúA‚Äù means the model agreed with the predictions made by the majority of people on the tasks, and ‚ÄúD‚Äù means the model disagreed with the majority predictions. Bars with stripes represent tasks on which the model's prediction agreed with people's majority prediction. Bars with the green (red) outline are tasks where the model made correct (wrong) predictions. Bars with the yellow background are tasks where the model made a positive prediction (i.e., predicted 1, the participant wants to see the date again).</span> 
               </figcaption> 
              </figure> 
              <p></p> 
              <p>To answer this question, we conducted our second experiment: Subjects were again asked to complete the same sequence of 30 prediction tasks as those used in Experiment 1, but this time, between the two phases, subjects received feedback on the model's overall accuracy in Phase 1. In addition to varying the level of human-model agreement on Phase 1 tasks, we also controlled the model's accuracy in Phase 1 to be either relatively low or high to understand whether and how the impact of high confidence human-model agreement is moderated by the level of model performance in influencing people's reliance on the model. We hypothesize that:</p> 
              <ul class="list-no-style"> 
               <li id="list15" label="‚Ä¢"><strong>[H2]</strong> The level of agreement between people and an ML model on tasks that people have high confidence in still significantly affects people's reliance on the model after the model's accuracy is observed in practice, regardless of the level of the observed accuracy; the higher the level of agreement, the more people rely on the model.<br /></li> 
              </ul> 
              <section id="sec-10"> 
               <header> 
                <div class="title-info"> 
                 <h3> <span class="section-number">4.1</span> Experimental Design</h3> 
                </div> 
               </header> 
               <p><strong>Experimental Treatments.</strong> Subjects were randomly assigned to one of the four treatments in Experiment 2, which were arranged in a 2 &times; 2 factorial design differing along two dimensions:</p> 
               <ul class="list-no-style"> 
                <li id="list16" label="‚Ä¢"><em>the level of high confidence human-model agreement in Phase 1</em>: the model agreed with the majority of people on 50% of the 20 Phase 1 tasks (low agreement) or 80% of the 20 Phase 1 tasks (high agreement).<br /></li> 
                <li id="list17" label="‚Ä¢"><em>the model's overall accuracy in Phase 1</em>: either 50% (low accuracy) or 80% (high accuracy.)<br /></li> 
               </ul> 
               <p>We associated each treatment with a unique ML model. To minimize the differences across treatments, we made sure that the four ML models had the <em>same</em> positive prediction rate of 70%. Figure&nbsp;<a class="fig" href="#fig4">4</a> illustrates the details on the ML models‚Äô predictions on the 20 tasks of Phase 1 across the four treatments. Similar to that in Experiment 1, when the model's accuracy in Phase 1 was fixed, we varied the agreement level between people and the model in Phase 1 by starting from the high agreement treatment and then coupling the addition of disagreement on tasks where the majority is correct with the addition of disagreement on tasks where the majority is wrong. In addition, we were able to obtain treatments with the same level of agreement between people and the model but different levels of model accuracy by controlling how often the agreement occurs on tasks where the majority of people are correct. Finally, on each Phase 2 task, all ML models in different treatments made the same prediction.</p> 
               <p><strong>Experimental Procedure.</strong>We again posted this experiment as a HIT on MTurk to U.S. workers only. It had an identical procedure as that of Experiment 1 except for the following differences: (1) Workers who had participated in Experiment 1 were not allowed to participate in this experiment; (2) After completing the 20 tasks in Phase 1, the subject was first shown a feedback screen with information about the ML model's overall accuracy on the tasks in Phase 1 (by design, it was either 50% or 80%) before she was asked to make an assessment on the four statements on the competence, reliability, understandability of the model as well as her faith in the model. In addition, the subject was <em>not</em> asked to guess the model's accuracy in Phase 1 this time since the model's accuracy was already revealed to the subject. </p>
               <figure id="fig5"> 
                <img src="https://dl-acm-org.libproxy.mit.edu/cms/attachment/65db66ca-f63b-44b8-9ffb-d883b51e8abd/chi21-519-fig14.jpg" class="img-responsive" alt="Figure 5" aria-describedby="fig006" /> 
                <p style="display:none" id="fig006">For agreement fraction, the two treatments with high accuracy are higher than two treatments with low accuracy. The two treatments with low agreement level, have slightly higher agreement fraction than those two with high agreement. For switch fraction, the high-accuracy low-agreement treatment takes the top spot, followed by high-accuracy low-agreement, low-accuracy high-agreement and low-accuracy low-agreement.</p> 
                <figcaption> 
                 <span class="figure-number">Figure 5:</span> 
                 <span class="figure-title">The average values of agreement fraction and switch fraction in Phase 2 across four treatments in Experiment 2, on all Phase 2 tasks. Error bars represent the standard errors of the mean. Without observing a clear and consistent upward trend in either plot, H2 is not supported.</span> 
                </figcaption> 
               </figure> 
               <p></p> 
              </section> 
              <section id="sec-11"> 
               <header> 
                <div class="title-info"> 
                 <h3> <span class="section-number">4.2</span> Experimental Results</h3> 
                </div> 
               </header> 
               <p>In total, we collected valid data from 466 subjects who completed our second experiment and answered the attention check question correctly. We used the same metrics and followed the same methods as discussed in Section&nbsp;<a class="sec" href="#sec-7">3.3</a> to conduct our statistical analysis, except for that all one-way ANOVAs were replaced by two-way ANOVAs now given the two-factor design of this experiment. Again, as a validity check of our design, we confirmed that subjects in the high agreement treatments found the ML model's predictions agreed with their own predictions in Phase 1 tasks significantly more than subjects in the low agreement treatments (<em>p</em> &lt; 0.001).</p> 
               <p><strong>Does High-Confidence Agreement Still Affect Reliance After Some Performance Feedback is Obtained?</strong> Figures&nbsp;5a and&nbsp;5b compare subject's agreement fractions and switch fractions on all Phase 2 tasks across the 4 treatments, respectively. Using two-way ANOVA tests, we found that after observing the model's performance in practice, only the model's observed accuracy has a significant impact on people's reliance on the model in Phase 2 (agreement fraction: <em>F</em>(1, 462) = 17.69, <em>p</em> &lt; 0.001, Cohen's <em>d</em> = 0.39; switch fraction: <em>F</em>(1, 462) = 12.28, <em>p</em> &lt; 0.001, Cohen's <em>d</em> = 0.32), but <em>not</em> the level of agreement between a subject and the model in Phase 1. We also detected a significant interaction between the level of high confidence human-model agreement and the model's accuracy on influencing subject's switch fraction in Phase 2 (<em>F</em>(1, 462) = 4.80, <em>p</em> = 0.029). Looking into this interaction in more depth, we further found a surprising trend‚Äîwhile a higher level of human-model agreement in Phase 1 seems to have little effect on subject's reliance on the model after the model's low performance is observed, it actually leads to a <em>decrease</em> in subject's reliance on the model in Phase 2 after they observe the model's accuracy is relatively high, though post-hoc pairwise comparison suggests the decrease is not significant (<em>p</em> = 0.166). Similar patterns have been consistently observed when we looked into subject's reliance on the ML model on Phase 2 tasks where the model was correct and on Phase 2 tasks where the model was incorrect separately (see SM Section 4.1 for additional analyses).</p> 
               <p><strong>Exploring Why High-Confidence Agreement No Longer Affects Reliance.</strong>We now turn to see how people's reliance on the model might have been influenced by their perceptions of the model (i.e., model competence, reliability, understandability, faith and trust in the model) after they obtain some feedback about the model performance. Detailed results are included in SM Section 4.2.</p> 
               <p>Overall, we found that after subjects observed the model's accuracy in Phase 1 was 50%, the fraction of subjects who agreed/disagreed that the model was competent, reliable, understandable, and they have faith in the model was <em>not</em> affected by the level of agreement that subjects had with the model in Phase 1. Further, subject's self-reported overall trust in the model was not affected by the level of human-model agreement in Phase 1 either. In fact, after a low performance of the ML model is observed, people tend to believe the model's competence, reliability, and understandability are at relatively low levels, and their faith and trust in the model are also low, which may all have led to their limited reliance on the model regardless of how often the model agrees with them previously on cases that they feel confident about.</p> 
               <p>On the other hand, after subjects observed the model's accuracy in Phase 1 was 80%, subjects still perceived the model that has a higher level of agreement with themselves in Phase 1 to be more competent, more reliable, and more understandable, they had a marginally increased trust in the model, though they did not show a higher level of faith in the model. Interestingly, such perceptions are not reflected in people's reliance behavior‚Äîin fact, we even find higher levels of high confidence human-model agreement results in a non-significant, but seemly consistent, decrease in people's reliance on the model when the model's performance is high. A possible explanation for this phenomenon is that people's reliance behavior may be not only affected by people's perceptions of the model but also be influenced by people's belief about <em>their own capability</em> on the decision-making tasks. In our experiment, subjects only obtained the <em>aggregate</em> information of the model's overall accuracy in Phase 1, but not the fine-grained performance feedback on the level of individual tasks, making it impossible for them to reason about their own performance in Phase 1 precisely<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a>. Thus, when a model has a low level of agreement with a subject but still turns out to be highly accurate, the subject may suspect that she was <em>not</em> competent at the prediction tasks herself. As a result, the subject might have increased her reliance on the model as compared to the case when the model has a high level of agreement with her.</p> 
               <p>In sum, our results in Experiment 2 do not support <strong>H2</strong>. We did not find sufficient evidence that the level of agreement between people and an ML model on tasks that people have high confidence in still influences people's reliance on the model after people have obtained some aggregate performance information of the model.</p> 
              </section> 
             </section> 
             <section id="sec-12"> 
              <header> 
               <div class="title-info"> 
                <h2> <span class="section-number">5</span> EXPERIMENT 3</h2> 
               </div> 
              </header> 
              <p>In the previous two experiments, we examine the effects of the human-model agreement on people's reliance on the model by varying the level of agreement between people and an ML model on tasks that people are <em>highly confident</em> about their own predictions. As a result, in these experiments, both agreement and disagreement between subjects and the ML model always occurred on high confidence tasks. However, when people interact with an ML model in the real world, they may be highly confident about their own predictions on some tasks, while not confident on the others, thus they may agree or disagree with an ML model both on high confidence tasks and low confidence tasks. An interesting question, then, is when people receive no information about an ML model's performance, whether and how people's confidence in the tasks that they agree or disagree with the model changes their reliance on the model. For example, when people agree with the model on tasks that they are not confident about their own predictions, do they have a similar level of reliance on the model as when they agree with the model on tasks that they are highly confident? Similarly, does disagreement with a model on low confidence tasks make people rely on the model to a similar degree as when they disagree with the model on high confidence tasks?</p> 
              <p>To answer these questions, we conducted a third experiment in which we controlled the level of agreement between people and an ML model to be <em>similar</em> across treatments, while varying whether the agreement/disagreement occurs on tasks that people have high confidence or not. We hypothesize that:</p> 
              <ul class="list-no-style"> 
               <li id="list18" label="‚Ä¢"><strong>[H3]</strong>: When people receive no information about an ML model's performance, people's confidence in their own predictions on tasks that they <em>agree</em> with the model significantly affects people's reliance on the model; the higher the confidence, the more people rely on the model.<br /></li> 
               <li id="list19" label="‚Ä¢"><strong>[H4]</strong>: When people receive no information about an ML model's performance, people's confidence in their own predictions on tasks that they <em>disagree</em> with the model significantly affects people's reliance on the model; the higher the confidence, the less people rely on the model.<br /></li> 
              </ul> 
              <figure id="fig6"> 
               <img src="https://dl-acm-org.libproxy.mit.edu/cms/attachment/f6a1a8ac-b997-4ed3-9ca4-4889b960489a/chi21-519-fig28.jpg" class="img-responsive" alt="Predictions of the four ML models on Phase 1 tasks of Experiment 3 in four treatments" aria-describedby="fig007" /> 
               <p style="display:none" id="fig007">Six kinds of tasks were used in Phase 1 of Experiment 1 as follow: majority correct with high confidence, ground truth = 1(CH1), majority correct with high confidence, ground truth = 0(CH0), majority incorrect with high confidence, ground truth = 1(IH1), majority incorrect with high confidence, ground truth = 0(IH0), majority correct with low confidence, ground truth = 1 (CL1), and majority incorrect with low confidence, ground truth = 0(IL0). The HA-HD treatment consisted of 8 CH1, 2 IH0, and 2 CH0, and made positive predictions on 6 CH1. The HA-LD treatment consisted of 6 CH1, 2 CL1, 2 IL0, and 2 CH0, and made positive predictions on 6 CH1. The LA-HD treatment consisted of 2 CH1, 6 CL1, 2 IH0, and 2 CH0, and made positive predictions on 6 CL1. The LA-LD treatment consisted of 8 CL1, 2 IL0, and 2 CH0, and made positive predictions on 6 CL1.</p> 
               <figcaption> 
                <span class="figure-number">Figure 6:</span> 
                <span class="figure-title">Predictions of the 4 ML models on Phase 1 tasks of Experiment 3. Each row represents the predictions made by one ML model (each model was used in an experimental treatment): ‚ÄúA‚Äù means the model agreed with the predictions made by the majority of people on the tasks, and ‚ÄúD‚Äù means the model disagreed with the majority prediction. Bars with stripes represent tasks on which the model's prediction agreed with people's majority prediction. Bars with the green (red) outline are tasks where the model made correct (wrong) predictions. Bars with the yellow background are tasks where the model made a positive prediction (i.e., predicted 1, the participant wants to see the date again). On the 12 tasks in Phase 1, the four models have the same accuracy of 83.3%, the same positive prediction rate of 50%, and the same level of agreement with people's majority predictions (i.e., agree on 66.7% of the tasks), though agreement/disagreement occurs on tasks where people have different levels of confidence in their own predictions.</span> 
               </figcaption> 
              </figure> 
              <section id="sec-13"> 
               <header> 
                <div class="title-info"> 
                 <h3> <span class="section-number">5.1</span> Experimental Design</h3> 
                </div> 
               </header> 
               <p><strong>Experimental Treatments.</strong> Same as before, each subject was still asked to complete a sequence of prediction tasks divided into two phases, where Phase 1 was used for creating the treatment and Phase 2 was designed for measuring reliance.</p> 
               <p>We had two goals in mind when designing experimental treatments for Experiment 3: First, we hope to maintain <em>a similar level of agreement</em> (and therefore disagreement) between people and the ML model in Phase 1 for subjects in different treatments‚Äîwithout such control, it would be difficult to tell whether differences in people's reliance on the model across different treatments are due to differences in people's confidence in the tasks where agreement or disagreement occurs, or due to the varying levels of agreement between people and the model. Second, we aim to understand the impact of people's confidence in their own predictions during an <em>agreement</em> with the model on their reliance on the model, <em>separately</em> from that impact during a <em>disagreement</em>. This implies that we can <em>not</em> design our experimental treatments by having subjects in different treatments work on the same sequence of Phase 1 tasks, which are composed of both low confidence and high confidence tasks‚Äîin that case, since both the task instances in Phase 1 and the designed agreement level between people and the model need to be fixed across treatments, we would have to couple the decrease of human-model agreement on high confidence tasks with the increase of agreement on low confidence tasks. As a result, we would not be able to tell whether changes in people's reliance on the model, if any, are resulted from the increase of high confidence disagreement or the increase of low confidence agreement.</p> 
               <p>With these goals in mind, we used <em>different</em> tasks in Phase 1 for different treatments. We created a set of 4 experimental treatments arranged in a 2 &times; 2 design along two factors:</p> 
               <ul class="list-no-style"> 
                <li id="list20" label="‚Ä¢"><em>human's confidence on agreement with the model in Phase 1</em>: We included 6 tasks in Phase 1 on which the ML model would make the same predictions as the majority of people, and we varied whether on these tasks, people are highly confident about their own predictions (i.e., <strong>h<em>igh confidence</em> a</strong><em>greement, ‚ÄúHA‚Äù</em>) or not confident about their own predictions (i.e., <strong>l<em>ow confidence</em> a</strong><em>greement, ‚ÄúLA‚Äù</em>). Practically, we selected 6 high confidence tasks and 6 low confidence tasks from the pilot study. In high (low) confidence agreement treatments, the 6 high (low) confidence tasks were included in Phase 1, and the model's predictions on these tasks always <em>agreed</em> with the prediction given by the majority of people.<br /></li> 
                <li id="list21" label="‚Ä¢"><em>human's confidence on disagreement with the model in Phase 1</em>: We included another 4 tasks in Phase 1 on which the ML model would make different predictions than the majority of people, and we varied whether on these tasks, people are highly confident about their own predictions (i.e., <strong>h<em>igh confidence</em> d</strong><em>isagreement, ‚ÄúHD‚Äù</em>) or not confident about their own predictions (i.e., <strong>l<em>ow confidence</em> d</strong><em>isagreement, ‚ÄúLD‚Äù</em>). Similar as before, we selected another set of 4 high confidence tasks and 4 low confidence tasks from the pilot study. In high (low) confidence disagreement treatments, the 4 high (low) confidence tasks were included in Phase 1, and the model's predictions on these tasks always <em>disagreed</em> with the prediction given by the majority of people.<br /></li> 
               </ul> 
               <p>Again, we controlled the accuracy and positive prediction rate for ML models in different treatments to be the same to minimize the differences across treatments. After adding this constraint, we found that the feasible combination of tasks we could select for Phase 1 turned out to only include task instances where the majority of people would make a positive prediction according to our pilot study. To minimize subject's tendency to make some unnecessary negative predictions in the experiment‚Äîwhich may cause varying levels of agreement between subjects and the model across treatments‚Äîwe further added the same set of two task instances to all four treatments, and on these two tasks, the majority of people would correctly make negative predictions. Together with the previous 10 tasks, subjects in each treatment would see a total of 12 tasks in Phase 1. Figure&nbsp;<a class="fig" href="#fig6">6</a> shows the details on the task instances we selected for Phase 1 in each of the 4 treatments, as well as the ML models‚Äô predictions on them. We expect following such design, subjects in different treatments have roughly similar levels of agreement with the ML model in Phase 1, but their confidence in the tasks where they agree or disagree with the model differs.</p> 
               <p>Finally, we included in Phase 2 the same 10 low confidence tasks as those used in Experiments 1 and 2, and the ML models in different treatments made the same prediction on each of these tasks.</p> 
               <p><strong>Experimental Procedure.</strong>This experiment had an identical procedure as that of Experiment 1 except for the following changes: (1) Workers who had participated in the previous two experiments were not allowed to participate in this experiment; (2) After introducing the task interface to subjects, we gave subjects a preview of possible prediction tasks that they might work on by showing each subject the same set of 4 profiles of speed dating events which represented task instances that people would likely be confident or not confident about their own predictions; this was intended to give subjects a sense of the range of confidence they might have in their own predictions on different tasks. Subjects were asked to review these profiles for at least 30 seconds before proceeding on to answer the qualification questions; (3) After completing Phase 1, in addition to report her perceptions of the model (e.g., guess model's accuracy, evaluate statement on model's competence, etc.), the subject was also asked to make a guess about the accuracy of her <em>own</em> independent predictions in Phase 1 (i.e., the predictions that she made in each task before seeing the model's predictions); (4) The base payment of the HIT was <font style="normal">$</font>1.2.</p> 
              </section> 
              <section id="sec-14"> 
               <header> 
                <div class="title-info"> 
                 <h3> <span class="section-number">5.2</span> Experimental Results</h3> 
                </div> 
               </header> 
               <p>We collected valid data from 402 subjects who completed this experiment and answered the attention check question correctly.</p> 
               <p>Despite our best effort, we found that there was still a small but significant difference in the actual level of agreement between subjects and the model in Phase 1 across the 4 experimental treatments. The average number of Phase 1 tasks on which a subject's initial predictions were the same as those of the model's was 7.46, 8.27, 6.61, and 7.33 for the HA‚ÄìHD, HA‚ÄìLD, LA‚ÄìHD, and LA‚ÄìLD treatments, respectively, and one-way ANOVA test indicates that the difference is statistically significant (<em>p</em> &lt; 0.001). To enable a valid comparison across our experimental treatments, we then searched for a <em>range</em> such that when limiting our analyses only to those subjects whose actual agreement with the model in Phase 1 (i.e., the number of Phase 1 tasks that their initial predictions were the same as those of the model's) fall into this range, the actual level of human-model agreement in Phase 1 across the 4 treatments was statistically the <em>same</em>. The maximum such range that we could find was [0, 8]. That is, when focusing on the subset of subjects who agreed with the ML model on at most 8 tasks in Phase 1, we did not find a statistically significant difference in the actual level of agreement between subjects and the model across the 4 experimental treatments (<em>p</em> &gt; 0.05). In total, 308 subjects (76% of all subjects) belong to this subset. Therefore, in the following, we restrict our analyses to this subset of subjects. We confirmed that, within this subset, subjects‚Äô confidence in the tasks that they agreed or disagreed with the model across different treatments aligned well with our expectations (see SM Section 5.1 for more details).</p> 
               <p><strong>Exploring How Confidence on Agreement/Disagreement Affects Reliance.</strong> We start by examining how subject's confidence in their own predictions when they agree or disagree with the model affects their reliance on the model. Figures&nbsp;7a and&nbsp;7b compare the average values of agreement fraction and switch fraction on all Phase 2 tasks for subjects across the 4 treatments, respectively. A first glance at these figures suggests that there seem to be <em>interaction</em> effects between the two factors‚Äîsubject's confidence on agreement and subject's confidence on disagreement‚Äîin influencing subject's reliance on the model. That is, whether high confidence human-model agreement increases people's reliance on the model, as compared to low confidence agreement, seems to be dependent on whether people disagree with the model on tasks that they are confident about their own predictions or not. Indeed, two-way ANOVA test results confirm that the interactions between the two factors are statistically significant for both two measures of reliance (agreement fraction: <em>F</em>(1, 304) = 4.93, <em>p</em> = 0.027; switch fraction: <em>F</em>(1, 304) = 7.66, <em>p</em> = 0.006). </p>
               <figure id="fig7"> 
                <img src="https://dl-acm-org.libproxy.mit.edu/cms/attachment/6b559451-f9de-4f47-ba02-f50bfc8b1cf4/chi21-519-fig29.jpg" class="img-responsive" alt="Figure 7" aria-describedby="fig008" /> 
                <p style="display:none" id="fig008">The agreement fraction and switch fraction show the similar trend. Among the four treatments, their two measure of reliance ranked decreasingly as follow: HA-LD, LA-LD close to LA-HD, and then HA-HD.</p> 
                <figcaption> 
                 <span class="figure-number">Figure 7:</span> 
                 <span class="figure-title">The average values of agreement fraction and switch fraction in Phase 2 across four treatments in Experiment 3, on all Phase 2 tasks. Error bars represent the standard errors of the mean. Without observing blue lines are consistently above red lines, H3 is not supported. Without observing consistent upward trends, H4 is not supported.</span> 
                </figcaption> 
               </figure> 
               <p></p> 
               <p>More specifically, in Figure&nbsp;<a class="fig" href="#fig7">7</a>, we observed a trend that when having low confidence disagreement with the model, subject's high confidence in their agreement with the model leads to higher reliance on the model (i.e., subjects in the HA‚ÄìLD treatment relied on the model more than those in the LA‚ÄìLD treatment), but this trend was reversed when subjects had high confidence disagreement with the model. Post-hoc Tukey HSD tests, however, suggest that subject's confidence in their agreement with the model only had a marginal impact on switch fraction when subjects had low confidence disagreement with the model (<em>p</em> = 0.094). Meanwhile, with respect to how people's confidence in their <em>disagreement</em> with a model affects their reliance on the model, we found that subjects in the HA‚ÄìHD treatment relied on the model significantly less than subjects in the HA‚ÄìLD treatment (agreement fraction: <em>p</em> = 0.005, Cohen's <em>d</em> = 0.28; switch fraction: <em>p</em> = 0.002, Cohen's <em>d</em> = 0.27), but there was no significant difference in subject's reliance on the model between the LA‚ÄìHD and LA‚ÄìLD treatments.</p> 
               <p>Again, when we looked into subject's reliance on the model within Phase 2 tasks where the model made correct/wrong predictions separately, we observed the same interaction patterns. For more details, see SM Section 5.2.</p> 
               <p><strong>Understanding the Interaction.</strong> Next, we turn our attention to data on the subject's perceptions of the model and themselves in different treatments to explore why people's confidence in their agreement or disagreement with an ML model affects their reliance on the model in the way that we have observed above. </p>
               <figure id="fig8"> 
                <img src="https://dl-acm-org.libproxy.mit.edu/cms/attachment/62c806de-cb8e-4131-89c0-5c098508f4bf/chi21-519-fig31.jpg" class="img-responsive" alt="Figure 8" aria-describedby="fig009" /> 
                <p style="display:none" id="fig009">Different trends lying in subjects‚Äô perception of model accuracy, their own accuracy and the difference between the accuracy of the model's and their own. For their perception of model accuracy, the four treatments ranked decreasingly as follow: HA-LD, LA-LD, then HA-HD slightly higher than LA-LD. For their perception of their own accuracy, the two treatments with HA are higher than those two with LA, and the two treatments with HD are higher higher than those two with LD. The accuracy difference has the similar trend as the subject's reliance, where the four treatments ranked decreasing as follow: HA-LD, LA-LD, LA-HD, and then HA-HD.</p> 
                <figcaption> 
                 <span class="figure-number">Figure 8:</span> 
                 <span class="figure-title">Average values of subjects‚Äô perception of model accuracy, their own accuracy, and the difference between the accuracy of the model's and their own, in Phase 1 of Experiment 3 (constrained to subjects who agreed with the ML model's predictions on at most 8 tasks in Phase 1). Error bars represent the standard errors of the mean.</span> 
                </figcaption> 
               </figure> 
               <p></p> 
               <p>To begin with, we plot the average values of the subject's perceived model accuracy in Phase 1 across the 4 treatments in Figure&nbsp;8a . To give a more complete picture, we also plot subject's perceived accuracy of their own in Phase 1 in Figure&nbsp;8b, and Figure&nbsp;8c presents the comparison on subject's perceived difference between the model's accuracy and their own accuracy across treatments. Interestingly, it appears that compared to subjects in the HA treatments, subjects in the LA treatments tended to believe the model to be less accurate, but they also perceived themselves as less accurate on the prediction tasks. In addition, compared to subjects in the HD treatments, subjects in the LD treatments seemed to believe the model to be more accurate while perceiving themselves as less accurate. Therefore, when we put both perceptions together and examine subjects‚Äô perceived accuracy difference between the model and themselves in Figure&nbsp;8c, we found a similar cross-over interaction pattern as that in subjects‚Äô reliance on the model.</p> 
               <p>Moreover, we also found the similar pattern that people's confidence in their agreement and disagreement with the model interact with each other in influencing people's perceptions of the model's competence, the model's understandability, as well as people's overall trust in the model (see SM Section 5.3 for detailed figures and analyses). Notably, though, the cross-over interaction pattern was not observed in subject's perception in the reliability of a model or their faith in a model. To the contrary, we found that compared to low confidence disagreement, high confidence disagreement consistently results in a significantly lower perceived level of model reliability (<em>p</em> &lt; 0.001), while low confidence human-model agreement leads to higher faith in the model compared to high confidence human-model agreement (<em>p</em> = 0.042).</p> 
               <p>As an attempt to connect our observations on how people's confidence in their agreement and disagreement with the model affects their perceptions and reliance on the model together, we conjecture that human-model agreement/disagreement with varying levels of human confidence may influence people's reliance on an ML model from different perspectives. For example, people might use the number of high confidence disagreement they have experienced with a model to determine whether the model makes any ‚Äúobvious mistakes‚Äù (as people may tend to believe they are correct on tasks that they feel confident about), which largely dominates their perceptions of the reliability of the model. For those models that do not make obvious mistakes, a larger number of high confidence human-model agreement possibly serve as a piece of evidence for people to believe the model has the capability to provide correct recommendations, again based on the assumption that people are themselves correct on tasks that they have high confidence in. Moreover, although low confidence human-model agreement may provide limited clues for people to gauge the correctness of the model as people are not sure whether their own predictions are correct or not, it may make people feel the model shares a similar reasoning process as themselves, thus increase their faith in the model based on such similarity. People's final level of reliance on the model, then, is jointly determined by multiple factors including these reliance-related perceptions, and future studies are needed to rigorously study the relationships between them.</p> 
               <p>To summarize, our results in Experiment 3 do not support <strong>H3</strong> or <strong>H4</strong>. Rather, the experimental data we obtained indicate that when people decide how much to rely on an ML model without knowing any information about the model's performance, they take an <em>integrated</em> view to examine their confidence in both their agreement with the model and their disagreement with the model together. As a result, we observed that how people's confidence in their agreement (disagreement) with an ML model affects their reliance on the model is dependent on their confidence in those cases where they disagree (agree) with that model.</p> 
              </section> 
             </section> 
             <section id="sec-15"> 
              <header> 
               <div class="title-info"> 
                <h2> <span class="section-number">6</span> DISCUSSION</h2> 
               </div> 
              </header> 
              <p>Via three experiments, we examined the possible heuristics that people may use to adjust their reliance on an ML model when performance feedback about the model is limited. Table&nbsp;<a class="tbl" href="#tab1">1</a> summarizes the design and findings of the three experiments. In this section, we provide discussions on the potential benefits and risks brought by people's usage of these heuristics, and steps that can be taken to reduce the risks. We also provide design implications for enhancing human-AI collaboration and discuss the limitations of our work.</p> 
              <div class="table-responsive" id="tab1"> 
               <div class="table-caption"> 
                <span class="table-number">Table 1:</span> 
                <span class="table-title">Summary of the design and findings of the three experiments.</span> 
               </div> 
               <table class="table"> 
                <thead> 
                 <tr> 
                  <th style="text-align:left;"></th> 
                  <th style="text-align:left;"><strong>Experiment 1</strong></th> 
                  <th style="text-align:left;"><strong>Experiment 2</strong></th> 
                  <th style="text-align:left;"><strong>Experiment 3</strong></th> 
                 </tr> 
                </thead> 
                <tbody> 
                 <tr style="border-bottom: 2pt solid #000000;"> 
                  <td style="text-align:left;">Hypothesis to test</td> 
                  <td style="text-align:left;"><strong>H1</strong>: High-confidence human-model agreement affects reliance when performance is unknown.</td> 
                  <td style="text-align:left;"><strong>H2</strong>: High-confidence human-model agreement affects reliance after performance feedback is obtained.</td> 
                  <td style="text-align:left;"><strong>H3/H4</strong>: Human's confidence on their agreement/disagreement with the model affects reliance when performance is unknown.</td> 
                 </tr> 
                 <tr style="border-bottom: 2pt solid #000000;"> 
                  <td style="text-align:left;">Independent variables</td> 
                  <td style="text-align:left;"><em>Human-model agreement level in Phase 1</em>: low, medium, high</td> 
                  <td style="text-align:left;"><em>Agreement level in Phase 1</em>: low, high; <em>Model accuracy in Phase 1</em>: 50%, 80%</td> 
                  <td style="text-align:left;"><em>Confidence on agreement in Phase 1</em>: low, high; <em>Confidence on disagreement in Phase 1</em>: low, high</td> 
                 </tr> 
                 <tr style="border-bottom: 2pt solid #000000;"> 
                  <td style="text-align:left;">Phase 1</td> 
                  <td style="text-align:left;">20 high-confidence tasks same tasks, but different ML predictions across treatments</td> 
                  <td style="text-align:left;">20 high-confidence tasks same tasks, but different ML predictions across treatments</td> 
                  <td style="text-align:left;">12 high-confidence and low-confidence tasks different tasks for different treatments</td> 
                 </tr> 
                 <tr style="border-bottom: 2pt solid #000000;"> 
                  <td style="text-align:left;">Phase 2</td> 
                  <td style="text-align:left;">10 low-confidence tasks same tasks and same ML predictions for all treatments</td> 
                  <td style="text-align:left;">10 low-confidence tasks same tasks and same ML predictions for all treatments</td> 
                  <td style="text-align:left;">10 low-confidence tasks same tasks and same ML predictions for all treatments</td> 
                 </tr> 
                 <tr style="border-bottom: 2pt solid #000000;"> 
                  <td style="text-align:left;">Phase 1 accuracy revealed?</td> 
                  <td style="text-align:left;">No</td> 
                  <td style="text-align:left;">Yes</td> 
                  <td style="text-align:left;">No</td> 
                 </tr> 
                 <tr style="border-bottom: 2pt solid #000000;"> 
                  <td style="text-align:left;">Findings</td> 
                  <td style="text-align:left;"><strong>H1</strong> supported</td> 
                  <td style="text-align:left;"><strong>H2</strong> not supported Model accuracy affects reliance</td> 
                  <td style="text-align:left;"><strong>H3</strong>, <strong>H4</strong> not supported Interactions between the two factors detected</td> 
                 </tr> 
                </tbody> 
               </table> 
              </div> 
              <p><strong>Benefits and risks of the heuristics.</strong>We find that without any information about an ML model's performance, people seem to use the level of high confidence agreement between the model and themselves to decide how much to rely on the model. Such heuristics can bring about both benefits and risks. On the one hand, if people are highly calibrated in their own predictions‚Äîfor example, if people are almost always correct when they are confident‚Äîand the ML model does not have substantial performance disparity on tasks with different properties (e.g., tasks that humans are confident vs. not confident), such heuristic indeed allows people to estimate the performance of an ML model accurately and thus establish an appropriate level of reliance on the model, even without access to any objective information about the model performance. On the other hand, there also exist conditions where such heuristic fails to help people rely on an ML model appropriately. For example, people may overestimate their own accuracy on tasks that they are confident due to factors like their inaccurate interpretation of the decision-making task and their cognitive biases (e.g., the Dunning‚ÄìKruger effect&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>]). As a result, by comparing the model's predictions to their own on tasks that they have high confidence in, people may systematically underestimate an ML model's accuracy, which potentially leads to underreliance. Furthermore, when people's predictions are <em>not independent</em> of those of the model's and they still use the level of high confidence agreement between the model and themselves to calibrate their reliance on the model, people may overly rely on models that share the same biases as themselves while showing a degree of underreliance on models that are complementary to themselves.</p> 
              <p>The usage of such heuristics may raise additional risks if the ML model's performance has systematic discrepancy on different data, which in fact is not rare in real-world applications of ML models&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0059">59</a>]. In such scenario, by adjusting the reliance on an ML model simply based on the level of human-model agreement on high confidence tasks, people may inappropriately over-generalize their perceived model performance on tasks with certain characteristics to other tasks with different characteristics. In fact, this risk has been partly reflected in our experiment as we observed that once seeing a high level of agreement between the model and themselves on tasks that they are confident, people tend to rely on the ML model more no matter whether its prediction is correct or not.</p> 
              <p>Interestingly, we find that once observing an ML model's performance, people tend to use this observed performance rather than their perceived level of high confidence agreement with the model to decide their reliance on the model. This behavior also poses both benefits and risks. On the positive side, when the observed model performance is an accurate reflection of the model's overall performance in the real world, it enables people to correct their over-reliance on models that share the same biases as themselves while overcoming their under-reliance on models that are complementary to themselves. However, the model's observed performance on a small set of decision-making tasks does not necessarily accurately reflect the model's average performance in practice. Thus, simply adjusting one's reliance based on the observed model performance on a limited number of real-world trials makes people overlook useful cues about model performance that is carried in their agreement and disagreement with the model, and may again result in inappropriate reliance on the model.</p> 
              <p>Finally, our observations on how people utilize their own confidence in cases where they agree and disagree with an ML model to adjust their reliance on the model, again, reveal some potential risks. Notably, when people and an ML model mostly agree with each other on cases that people lack confidence, people's confidence in their disagreement with the model no longer influences their reliance on the model. This indicates the possibility that for people who feel they lack expertise in a decision-making task, they might not be sufficiently alarmed by the high confidence disagreement between the ML model and themselves, and may therefore show inappropriate over-reliance on the model.</p> 
              <p><strong>The need of enabling people to understand their own decision-making performance.</strong>Using high confidence human-model agreement to estimate the performance of an ML model would not be reliable if people are not calibrated in their own decisions (i.e., if people's confidence in their decisions does not accurately reflect their accuracy). A critical step towards helping people to make reliable estimation, thus, is to facilitate people's understanding of their own decision-making performance. This includes raising people's awareness of biases in their own decision-making&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>], designing methods to allow people assess their own independent decision-making performance (e.g., ask people to make decisions on historical data and provide performance feedback), and encouraging people to serve as their own devil's advocate by challenging themselves to probe why they make certain decisions and how they might possibly be wrong&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>]. Only if people have a solid understanding of when their own decision-making accuracy is high and when not, obtaining an accurate estimate of the ML model's performance without performance feedback would become possible.</p> 
              <p><strong>The need of helping people better understand ML model's performance.</strong> Our experimental results suggest that people may have a tendency to over-generalize the performance of an ML model, which is either estimated or observed by themselves, between tasks with different characteristics. To overcome this problem, efforts should be made to improve people's capability of interpreting the ML model's performance. For example, tutorials can be provided to end-users of ML models to increase their general knowledge of ML, including that ML models are often learned from data, and depending on the quality of the data, ML models may exhibit different performance on different data&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>]. Designers of ML models should transparently report properties of a model to people, including its intended uses and potential limitations&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0040">40</a>]. The uncertainty inherent in the process of model performance estimation process should also be properly communicated to people, so that people can better understand how much they can generalize the model performance they have observed to other contexts&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>].</p> 
              <p><strong>Implications for enhancing human-AI collaborations.</strong> Our work also provides implications for designing better ML models to enhance the partnership between humans and machines. In the context of AI-assisted decision-making, the ultimate goal is to optimize the human-AI joint team decision-making performance&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>], and achieving this goal requires humans and AI to complement each other. However, our results in this work indicate when performance feedback of the AI is limited, people might be unwilling to rely on the complementary AI partner, due to the limited agreement between them. Our findings suggest one possible way to minimize such undesirable human reaction to AI is to maximize the agreement between humans and AI on those cases that humans are confident and <em>actually correct</em>, for example, by integrating expert knowledge into the development of ML models&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>]. Model designers can also provide explanation of the model decision when high confidence human-model disagreement occurs, to help people better understand why the disagreement exists and mitigate the potential negative impact such disagreement brings about.</p> 
              <p>On the conceptual level, our findings echos that of other work in human-AI collaboration&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>] and reiterates that the design and development of ML-based decision aids should <em>not</em> be isolated from the people who will use them. Only by taking people's perceptions and reactions to the ML model into consideration can the decision aids release their full potential to improve the joint performance of the human-AI team. However, critically, we emphasize that the consideration of human behavior in the AI development process should <em>not</em> be used for nudging people into blindly relying on the AI, which would be unethical and even dangerous.</p> 
              <p><strong>Limitations and future work.</strong>Our study was conducted with laypeople (i.e., subjects recruited from Amazon Mechanical Turk) on one specific type of prediction task. Cautions should be used when generalizing results in this work to different settings, such as how the agreement level between an ML model and an expert would affect the expert's reliance on the model on some tasks involving significant higher levels of stakes (e.g., ML assists doctors in medical decision-making). More experimental studies should be carried out in the future with different populations on different types of tasks to understand to what extent the results reported here can be generalized. In addition, people's perception of their agreement with a model may involve not only how frequently the model agrees with themselves on decisions for specific cases, but also how consistent the model's predicted orderings of different cases are compared to their own's, which may relate to the perceived internal consistency of the model. Separating the impact of the frequency of human-model agreement on reliance from the impact of human-model agreement on the relative ordering of different cases on reliance would be a challenging but exciting future work.</p> 
             </section> 
             <section id="sec-16"> 
              <header> 
               <div class="title-info"> 
                <h2> <span class="section-number">7</span> CONCLUSION</h2> 
               </div> 
              </header> 
              <p>In this paper, we present our initial attempt to uncover the heuristics that people adopt to adjust their reliance on machine learning models in AI-assisted decision making, when objective performance feedback of the models is limited. Via three randomized human-subject experiments, we show that people tend to use their levels of agreement with a model on cases that they are highly confident as a proxy to estimate the model performance and adjust their reliance on the model accordingly, but such agreement shows limited impact in influencing reliance after people have observed the model's performance in practice. Moreover, holding the level of human-model agreement constant, how people's confidence in their agreement with a model affects their reliance on the model depends on their confidence in those cases where they disagree with the model. We highlight people's usage of these heuristics may raise risks of inappropriate reliance on ML models, and we discuss both actions that can be taken to reduce these risks and possible directions to improve human-AI collaboration by taking human's heuristics into account.</p> 
             </section> 
            </section> 
            <section class="back-matter"> 
             <section id="sec-17"> 
              <header> 
               <div class="title-info"> 
                <h2>ACKNOWLEDGMENTS</h2> 
               </div> 
              </header> 
              <p>We are grateful to the anonymous reviewers who provided many helpful comments. We thank the support of the National Science Foundation under grant IIS-1850335 on this work. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors alone.</p> 
             </section> 
             <section id="ref-001"> 
              <header> 
               <div class="title-info"> 
                <h2 class="page-brake-head">REFERENCES</h2> 
               </div> 
              </header> 
              <ul class="bibUl"> 
               <li id="BibPLXBIB0001" label="[1]">Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul&nbsp;N Bennett, Kori Inkpen, <em>et al.</em> 2019. Guidelines for human-ai interaction. In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. 1‚Äì13.</li> 
               <li id="BibPLXBIB0002" label="[2]">Ricardo Baeza-Yates. 2018. Bias on the web. <em><em>Commun. ACM</em></em> 61, 6 (2018), 54‚Äì61.</li> 
               <li id="BibPLXBIB0003" label="[3]">Gagan Bansal, Besmira Nushi, Ece Kamar, Walter&nbsp;S Lasecki, Daniel&nbsp;S Weld, and Eric Horvitz. 2019. Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance. In <em>Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</em>, Vol.&nbsp;7. 2‚Äì11.</li> 
               <li id="BibPLXBIB0004" label="[4]">Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel&nbsp;S Weld, Walter&nbsp;S Lasecki, and Eric Horvitz. 2019. Updates in human-ai teams: Understanding and addressing the performance/compatibility tradeoff. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol.&nbsp;33. 2429‚Äì2437.</li> 
               <li id="BibPLXBIB0005" label="[5]">Gagan Bansal, Tongshuang Wu, Joyce Zhu, Raymond Fok, Besmira Nushi, Ece Kamar, Marco&nbsp;Tulio Ribeiro, and Daniel&nbsp;S Weld. 2020. Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance. (2020). arxiv:<a class="link-inline force-break" href="https://arxiv-org.libproxy.mit.edu/abs/2006.14779">2006.14779</a></li> 
               <li id="BibPLXBIB0006" label="[6]">Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In <em>Conference on fairness, accountability and transparency</em>. 77‚Äì91.</li> 
               <li id="BibPLXBIB0007" label="[7]">Carrie&nbsp;J Cai, Jonas Jongejan, and Jess Holbrook. 2019. The effects of example-based explanations in a machine learning interface. In <em>Proceedings of the 24th International Conference on Intelligent User Interfaces</em>. 258‚Äì262.</li> 
               <li id="BibPLXBIB0008" label="[8]">Eric&nbsp;T Chancey, James&nbsp;P Bliss, Yusuke Yamani, and Holly&nbsp;AH Handley. 2017. Trust and the compliance‚Äìreliance paradigm: The effects of risk, error bias, and reliability on trust and dependence. <em><em>Human factors</em></em> 59, 3 (2017), 333‚Äì345.</li> 
               <li id="BibPLXBIB0009" label="[9]">Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O'Connell, Terrance Gray, F&nbsp;Maxwell Harper, and Haiyi Zhu. 2019. Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders. In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. 1‚Äì12.</li> 
               <li id="BibPLXBIB0010" label="[10]">Stephen&nbsp;R Dixon, Christopher&nbsp;D Wickens, and Jason&nbsp;S McCarley. 2007. On the independence of compliance and reliance: Are automation false alarms worse than misses?<em><em>Human factors</em></em> 49, 4 (2007), 564‚Äì572.</li> 
               <li id="BibPLXBIB0011" label="[11]">David Dunning. 2014. We are all confident idiots. <em><em>Pacific Standard</em></em> 7(2014), 46‚Äì54.</li> 
               <li id="BibPLXBIB0012" label="[12]">Mary&nbsp;T Dzindolet, Scott&nbsp;A Peterson, Regina&nbsp;A Pomranky, Linda&nbsp;G Pierce, and Hall&nbsp;P Beck. 2003. The role of trust in automation reliance. <em><em>International journal of human-computer studies</em></em> 58, 6 (2003), 697‚Äì718.</li> 
               <li id="BibPLXBIB0013" label="[13]">Mary&nbsp;T Dzindolet, Linda&nbsp;G Pierce, Hall&nbsp;P Beck, and Lloyd&nbsp;A Dawe. 1999. Misuse and disuse of automated aids. In <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em>, Vol.&nbsp;43. SAGE Publications Sage CA: Los Angeles, CA, 339‚Äì339.</li> 
               <li id="BibPLXBIB0014" label="[14]">Philipp Ecken and Richard Pibernik. 2016. Hit or miss: what leads experts to take advice for long-term judgments?<em><em>Management Science</em></em> 62, 7 (2016), 2002‚Äì2021.</li> 
               <li id="BibPLXBIB0015" label="[15]">Andre Esteva, Brett Kuprel, Roberto&nbsp;A Novoa, Justin Ko, Susan&nbsp;M Swetter, Helen&nbsp;M Blau, and Sebastian Thrun. 2017. Dermatologist-level classification of skin cancer with deep neural networks. <em><em>Nature</em></em> 542, 7639 (2017), 115‚Äì118.</li> 
               <li id="BibPLXBIB0016" label="[16]">Raymond Fisman, Sheena&nbsp;S Iyengar, Emir Kamenica, and Itamar Simonson. 2006. Gender differences in mate selection: Evidence from a speed dating experiment. <em><em>The Quarterly Journal of Economics</em></em> 121, 2 (2006), 673‚Äì697.</li> 
               <li id="BibPLXBIB0017" label="[17]">SM Fleming and ND Daw. 2016. Self-evaluation of decision performance: A general Bayesian framework for metacognitive computation. <em><em>Psychol Rev</em></em> 124(2016), 1‚Äì59.</li> 
               <li id="BibPLXBIB0018" label="[18]">Jorge Galindo and Pablo Tamayo. 2000. Credit risk assessment using statistical and machine learning: basic methodology and risk modeling applications. <em><em>Computational Economics</em></em> 15, 1-2 (2000), 107‚Äì143.</li> 
               <li id="BibPLXBIB0019" label="[19]">Ji Gao and John&nbsp;D Lee. 2006. Extending the decision field theory to model operators‚Äô reliance on automation in supervisory control situations. <em><em>IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans</em></em> 36, 5 (2006), 943‚Äì959.</li> 
               <li id="BibPLXBIB0020" label="[20]">Ji Gao, John&nbsp;D Lee, and Yi Zhang. 2006. A dynamic model of interaction between reliance on automation and cooperation in multi-operator multi-automation situations. <em><em>International Journal of Industrial Ergonomics</em></em> 36, 5(2006), 511‚Äì526.</li> 
               <li id="BibPLXBIB0021" label="[21]">Efstathios&nbsp;D Gennatas, Jerome&nbsp;H Friedman, Lyle&nbsp;H Ungar, Romain Pirracchio, Eric Eaton, Lara&nbsp;G Reichmann, Yannet Interian, Jos&eacute;&nbsp;Marcio Luna, Charles&nbsp;B Simone, Andrew Auerbach, <em>et al.</em> 2020. Expert-augmented machine learning. <em><em>Proceedings of the National Academy of Sciences</em></em> 117, 9 (2020), 4571‚Äì4577.</li> 
               <li id="BibPLXBIB0022" label="[22]">Ben Green and Yiling Chen. 2019. The principles and limits of algorithm-in-the-loop decision making. <em><em>Proceedings of the ACM on Human-Computer Interaction</em></em> 3, CSCW(2019), 1‚Äì24.</li> 
               <li id="BibPLXBIB0023" label="[23]">Dale&nbsp;W Griffin and Lee Ross. 1991. Subjective construal, social inference, and human misunderstanding. In <em>Advances in experimental social psychology</em>. Vol.&nbsp;24. Elsevier, 319‚Äì359.</li> 
               <li id="BibPLXBIB0024" label="[24]">Christoph Hube, Besnik Fetahu, and Ujwal Gadiraju. 2019. Understanding and mitigating worker biases in the crowdsourced collection of subjective judgments. In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. 1‚Äì12.</li> 
               <li id="BibPLXBIB0025" label="[25]">Sarfaraz Hussein, Kunlin Cao, Qi Song, and Ulas Bagci. 2017. Risk Stratification of Lung Nodules Using 3D CNN-Based Multi-task Learning. In Information Processing in Medical Imaging, Marc Niethammer, Martin Styner, Stephen Aylward, Hongtu Zhu, Ipek Oguz, Pew-Thian Yap, and Dinggang Shen(Eds.). Springer International Publishing, Cham, 249‚Äì260.</li> 
               <li id="BibPLXBIB0026" label="[26]">H Kaur, A Williams, and WS Lasecki. 2019. Building shared mental models between humans and ai for effective collaboration. (2019).</li> 
               <li id="BibPLXBIB0027" label="[27]">Yea-Seul Kim, Paula Kayongo, Madeleine Grunde-McLaughlin, and Jessica Hullman. 2020. Bayesian-Assisted Inference from Visualized Data. (2020). arxiv:<a class="link-inline force-break" href="https://arxiv-org.libproxy.mit.edu/abs/2008.00142">2008.00142</a></li> 
               <li id="BibPLXBIB0028" label="[28]">Gang Kou, Yi Peng, and Guoxun Wang. 2014. Evaluation of clustering algorithms for financial risk analysis using MCDM methods. <em><em>Information Sciences</em></em> 275(2014), 1‚Äì12.</li> 
               <li id="BibPLXBIB0029" label="[29]">Justin Kruger and David Dunning. 1999. Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments.<em><em>Journal of personality and social psychology</em></em> 77, 6(1999), 1121.</li> 
               <li id="BibPLXBIB0030" label="[30]">Vivian Lai and Chenhao Tan. 2019. On human predictions with explanations and predictions of machine learning models: A case study on deception detection. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>. 29‚Äì38.</li> 
               <li id="BibPLXBIB0031" label="[31]">John&nbsp;D Lee and Katrina&nbsp;A See. 2004. Trust in automation: Designing for appropriate reliance. <em><em>Human factors</em></em> 46, 1 (2004), 50‚Äì80.</li> 
               <li id="BibPLXBIB0032" label="[32]">Moshe Leshno, Vladimir&nbsp;Ya Lin, Allan Pinkus, and Shimon Schocken. 1993. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. <em><em>Neural networks</em></em> 6, 6 (1993), 861‚Äì867.</li> 
               <li id="BibPLXBIB0033" label="[33]">Varda Liberman, Julia&nbsp;A Minson, Christopher&nbsp;J Bryan, and Lee Ross. 2012. Na&iuml;ve realism and capturing the ‚Äúwisdom of dyads‚Äù. <em><em>Journal of Experimental Social Psychology</em></em> 48, 2 (2012), 507‚Äì512.</li> 
               <li id="BibPLXBIB0034" label="[34]">Duri Long and Brian Magerko. 2020. What is AI Literacy? Competencies and Design Considerations. In <em>Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>. 1‚Äì16.</li> 
               <li id="BibPLXBIB0035" label="[35]">Maria Madsen and Shirley Gregor. 2000. Measuring human-computer trust. In <em>11th australasian conference on information systems</em>, Vol.&nbsp;53. Citeseer, 6‚Äì8.</li> 
               <li id="BibPLXBIB0036" label="[36]">Gary Marks and Norman Miller. 1987. Ten years of research on the false-consensus effect: An empirical and theoretical review.<em><em>Psychological bulletin</em></em> 102, 1 (1987), 72.</li> 
               <li id="BibPLXBIB0037" label="[37]">Charles Marx, Flavio Calmon, and Berk Ustun. 2020. Predictive multiplicity in classification. In <em>International Conference on Machine Learning</em>. PMLR, 6765‚Äì6774.</li> 
               <li id="BibPLXBIB0038" label="[38]">Stephanie&nbsp;M Merritt. 2011. Affective processes in human‚Äìautomation interactions. <em><em>Human Factors</em></em> 53, 4 (2011), 356‚Äì370.</li> 
               <li id="BibPLXBIB0039" label="[39]">Julia&nbsp;A Minson, Varda Liberman, and Lee Ross. 2011. Two to tango: Effects of collaboration and disagreement on dyadic judgment. <em><em>Personality and Social Psychology Bulletin</em></em> 37, 10 (2011), 1325‚Äì1338.</li> 
               <li id="BibPLXBIB0040" label="[40]">Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa&nbsp;Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In <em>Proceedings of the conference on fairness, accountability, and transparency</em>. 220‚Äì229.</li> 
               <li id="BibPLXBIB0041" label="[41]">Mahsan Nourani, Samia Kabir, Sina Mohseni, and Eric&nbsp;D Ragan. 2019. The Effects of Meaningful and Meaningless Explanations on Trust and Perceived System Accuracy in Intelligent Systems. In <em>Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</em>, Vol.&nbsp;7. 97‚Äì105.</li> 
               <li id="BibPLXBIB0042" label="[42]">Besmira Nushi, Ece Kamar, and E. Horvitz. 2018. Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure. In <em>HCOMP</em>.</li> 
               <li id="BibPLXBIB0043" label="[43]">Raja Parasuraman and Victor Riley. 1997. Humans and automation: Use, misuse, disuse, abuse. <em><em>Human factors</em></em> 39, 2 (1997), 230‚Äì253.</li> 
               <li id="BibPLXBIB0044" label="[44]">Raja Parasuraman and Christopher&nbsp;D Wickens. 2008. Humans: Still vital after all these years of automation. <em><em>Human factors</em></em> 50, 3 (2008), 511‚Äì520.</li> 
               <li id="BibPLXBIB0045" label="[45]">Niccolo Pescetelli, Geraint Rees, and Bahador Bahrami. 2016. The perceptual and social components of metacognition.<em><em>Journal of Experimental Psychology: General</em></em> 145, 8 (2016), 949.</li> 
               <li id="BibPLXBIB0046" label="[46]">Niccolo Pescetelli and Nick Yeung. 2018. The role of decision confidence in advice-taking and trust formation. (2018). arxiv:<a class="link-inline force-break" href="https://arxiv-org.libproxy.mit.edu/abs/1809.10453">1809.10453</a></li> 
               <li id="BibPLXBIB0047" label="[47]">Forough Poursabzi-Sangdeh, Daniel&nbsp;G Goldstein, Jake&nbsp;M Hofman, Jennifer&nbsp;Wortman Vaughan, and Hanna Wallach. 2018. Manipulating and measuring model interpretability. (2018). arxiv:<a class="link-inline force-break" href="https://arxiv-org.libproxy.mit.edu/abs/1802.07810">1802.07810</a></li> 
               <li id="BibPLXBIB0048" label="[48]">Marco&nbsp;Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. ‚Äù Why should i trust you?‚Äù Explaining the predictions of any classifier. In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>. 1135‚Äì1144.</li> 
               <li id="BibPLXBIB0049" label="[49]">Victor Riley. 1996. Operator reliance on automation: Theory and data. <em><em>Automation and human performance: Theory and applications</em></em> (1996), 19‚Äì35.</li> 
               <li id="BibPLXBIB0050" label="[50]">Lee Ross, David Greene, and Pamela House. 1977. The ‚Äúfalse consensus effect‚Äù: An egocentric bias in social perception and attribution processes. <em><em>Journal of experimental social psychology</em></em> 13, 3 (1977), 279‚Äì301.</li> 
               <li id="BibPLXBIB0051" label="[51]">Julian Sanchez, Arthur&nbsp;D Fisk, and Wendy&nbsp;A Rogers. 2004. Reliability and age-related effects on trust and reliance of a decision support aid. In <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em>, Vol.&nbsp;48. Sage Publications Sage CA: Los Angeles, CA, 586‚Äì589.</li> 
               <li id="BibPLXBIB0052" label="[52]">Julian Sanchez, Wendy&nbsp;A Rogers, Arthur&nbsp;D Fisk, and Ericka Rovira. 2014. Understanding reliance on automation: effects of error type, error distribution, age and experience. <em><em>Theoretical issues in ergonomics science</em></em> 15, 2 (2014), 134‚Äì160.</li> 
               <li id="BibPLXBIB0053" label="[53]">James Schaffer, John O'Donovan, James Michaelis, Adrienne Raglin, and Tobias H&ouml;llerer. 2019. I can do better than your AI: expertise and explanations. In <em>Proceedings of the 24th International Conference on Intelligent User Interfaces</em>. 240‚Äì251.</li> 
               <li id="BibPLXBIB0054" label="[54]">Kelly&nbsp;E See, Elizabeth&nbsp;W Morrison, Naomi&nbsp;B Rothman, and Jack&nbsp;B Soll. 2011. The detrimental effects of power on confidence, advice taking, and accuracy. <em><em>Organizational behavior and human decision processes</em></em> 116, 2(2011), 272‚Äì285.</li> 
               <li id="BibPLXBIB0055" label="[55]">Keng Siau and Weiyu Wang. 2018. Building trust in artificial intelligence, machine learning, and robotics. <em><em>Cutter Business Technology Journal</em></em> 31, 2 (2018), 47‚Äì53.</li> 
               <li id="BibPLXBIB0056" label="[56]">Aaron Van&nbsp;den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep content-based music recommendation. In <em>Advances in neural information processing systems</em>. 2643‚Äì2651.</li> 
               <li id="BibPLXBIB0057" label="[57]">Kees Van&nbsp;Dongen and Peter-Paul Van&nbsp;Maanen. 2013. A framework for explaining reliance on decision aids. <em><em>International Journal of Human-Computer Studies</em></em> 71, 4 (2013), 410‚Äì424.</li> 
               <li id="BibPLXBIB0058" label="[58]">Lyn&nbsp;M Van&nbsp;Swol and Janet&nbsp;A Sniezek. 2005. Factors affecting the acceptance of expert advice. <em><em>British journal of social psychology</em></em> 44, 3 (2005), 443‚Äì461.</li> 
               <li id="BibPLXBIB0059" label="[59]">Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. 2019. Racial faces in the wild: Reducing racial bias by information maximization adaptation network. In <em>Proceedings of the IEEE International Conference on Computer Vision</em>. 692‚Äì702.</li> 
               <li id="BibPLXBIB0060" label="[60]">Xinxi Wang and Ye Wang. 2014. Improving content-based and hybrid music recommendation using deep learning. In <em>Proceedings of the 22nd ACM international conference on Multimedia</em>. 627‚Äì636.</li> 
               <li id="BibPLXBIB0061" label="[61]">Andrew Ward, L Ross, E Reed, E Turiel, and T Brown. 1997. Naive realism in everyday life: Implications for social conflict and misunderstanding. <em><em>Values and knowledge</em></em>(1997), 103‚Äì135.</li> 
               <li id="BibPLXBIB0062" label="[62]">Fumeng Yang, Zhuanyi Huang, Jean Scholtz, and Dustin&nbsp;L Arendt. 2020. How do visual explanations foster end users‚Äô appropriate trust in machine learning?. In <em>Proceedings of the 25th International Conference on Intelligent User Interfaces</em>. 189‚Äì201.</li> 
               <li id="BibPLXBIB0063" label="[63]">Ilan Yaniv. 2004. Receiving other people's advice: Influence and benefit. <em><em>Organizational behavior and human decision processes</em></em> 93, 1 (2004), 1‚Äì13.</li> 
               <li id="BibPLXBIB0064" label="[64]">Ming Yin, Jennifer Wortman&nbsp;Vaughan, and Hanna Wallach. 2019. Understanding the effect of accuracy on trust in machine learning models. In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. 1‚Äì12.</li> 
               <li id="BibPLXBIB0065" label="[65]">Yunfeng Zhang, Q&nbsp;Vera Liao, and Rachel&nbsp;KE Bellamy. 2020. Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making. (2020). arxiv:<a class="link-inline force-break" href="https://arxiv-org.libproxy.mit.edu/abs/2001.02114">2001.02114</a></li> 
               <li id="BibPLXBIB0066" label="[66]">Zijian Zhang, Jaspreet Singh, Ujwal Gadiraju, and Avishek Anand. 2019. Dissonance between human and machine understanding. <em><em>Proceedings of the ACM on Human-Computer Interaction</em></em> 3, CSCW(2019), 1‚Äì23.</li> 
              </ul> 
             </section> 
            </section> 
            <section id="foot-001" class="footnote"> 
             <header> 
              <div class="title-info"> 
               <h2>FOOTNOTE</h2> 
              </div> 
             </header> 
             <p id="fn1"> <a href="#foot-fn1"><sup>1</sup></a>Predictions of ML models on each task were artificially set based on the designed level of human-model agreement in different treatments. However, we note that in reality, different ML models can be trained to match such predictions, as sophisticated models are shown to be able to approximate any continuous function&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>]. Empirical studies also showed that real-world prediction problems often admit competing models that make wildly conflicting predictions&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>].</p> 
             <p id="fn2"> <a href="#foot-fn2"><sup>2</sup></a>That is, the fraction of tasks where the model predicted ‚Äúthe participant wants to see the date‚Äù was kept the same across treatments.</p> 
             <p id="fn3"> <a href="#foot-fn3"><sup>3</sup></a>Agreement fraction concerns how often a subject's <em>final</em> prediction son <em>Phase 2</em> tasks agreed with the model, thus it is a measure of reliance. This should not be confused with the level of agreement between a subject and the ML model, which examines how often a subject's <em>initial</em> predictions agree with the model on <em>Phase 1</em> tasks.</p> 
             <p id="fn4"> <a href="#foot-fn4"><sup>4</sup></a>Additional non-parametric tests and regression analyses show consistent results. See details in SM Section 3.</p> 
             <p id="fn5"> <a href="#foot-fn5"><sup>5</sup></a>On a 7-point Likert scale, we considered subjects who provided a rating of 1‚Äì3 as disagreeing with the statement, and subjects who provided a rating of 5‚Äì7 as agreeing with the statement. Due to a mistake in Experiments 1 and 2, for subject's evaluations on the competence, reliability, understandability, and faith statements, ratings of 6 and 7 were all recorded as 5. Thus, we were not able to conduct more fine-grained analyses on these data for Experiments 1 and 2.</p> 
             <p id="fn6"> <a href="#foot-fn6"><sup>6</sup></a>It is possible in real life that only aggregate-level information of model performance is available due to various reasons like privacy concerns.</p> 
             <div class="bibStrip"> 
              <p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from <a href="mailto:permissions@acm.org">permissions@acm.org</a>.</p> 
              <p><em>CHI '21, May 08‚Äì13, 2021, Yokohama, Japan</em></p> 
              <p>&copy; 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.<br /> ACM ISBN 978-1-4503-8096-6/21/05‚Ä¶$15.00.<br />DOI: <a class="link-inline force-break" target="_blank" href="https://doi-org.libproxy.mit.edu/10.1145/3411764.3445562">https://doi-org.libproxy.mit.edu/10.1145/3411764.3445562</a> </p> 
             </div> 
            </section> 
           </main>  
    </div>  
  </body>
</html>