<html>
  <head>
      <script src="https://www.gstatic.com/firebasejs/8.6.8/firebase-app.js"></script>
      <script src="https://www.gstatic.com/firebasejs/8.6.8/firebase-firestore.js"></script>
      
      <!-- TODO: Add SDKs for Firebase products that you want to use
           https://firebase.google.com/docs/web/setup#available-libraries -->
      
      <script>
        
      </script>

      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
      <script type="text/javascript" src="script.js"></script>

      <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="navbar">
      <div id="move-container"><select name="cars" id="cars">
      <option value="volvo">⚓</option>
      <option value="saab">Section</option>
      <option value="mercedes">⚓ & Section</option>
    </select> ⬅️ ➡️</div>
      <div id="bridge-container"><img src="imgs/bridge.jpeg" width="50" alt="bridge"/> <span class='bridge-snippet'>hello world</span>
      <button class="close" onclick="cancelBridge()">X</button> </div>
      
    </div>

    <div class="sidebar">
      <div id="bib-bridge"></div>
      <div class="viewer"></div>
    </div>

    <div id="container">
        Blur text <input type="checkbox" id="myCheck" onclick="blurText(event)">
        <main>
        <section class="front-matter">
            <section>
                <header class="title-info">
                    <div class="journal-title">
                        <h1>
                            <span class="title">LipType: A Silent Speech Recognizer Augmented with an Independent Repair Model</span>
                            <br/>
                            <span class="subTitle"></span>
                        </h1>
                    </div>
                </header>
                <div class="authorGroup">
                    <div class="author">
                        <span class="givenName">Laxmi</span>
                        <span class="surName">Pandey</span>
                        , Human Computer Interaction Group University of California, Merced, United States, 
                               
                        <a href="mailto:lpandey@ucmerced.edu">lpandey@ucmerced.edu</a>
                    </div>
                    <div class="author">
                        <span class="givenName">Ahmed Sabbir</span>
                        <span class="surName">Arif</span>
                        , Human-Computer Interaction Group University of California, Merced, United States, 
                               
                        <a href="mailto:asarif@ucmerced.edu">asarif@ucmerced.edu</a>
                    </div>
                </div>
                <br/>
                <div class="pubInfo">
                    <p>
                        DOI: 
                        <a href="https://doi.org/10.1145/3411764.3445565" target="_blank">https://doi.org/10.1145/3411764.3445565</a>
                        <br/>
                        CHI '21: 
                        <a href="https://doi.org/10.1145/3411764" target="_blank">CHI Conference on Human Factors in Computing Systems</a>
                        , Yokohama, Japan, May 2021
                    </p>
                </div>
                <div class="abstract">
                    <p>
                        Speech recognition is unreliable in noisy places, compromises privacy and security when around strangers, and inaccessible to people with speech disorders. Lip reading can mitigate many of these challenges but the existing silent speech recognizers for lip reading are error prone. Developing new recognizers and acquiring new datasets is impractical for many since it requires enormous amount of time, effort, and other resources. To address these, first, we develop LipType, an optimized version of LipNet for improved speed and accuracy. We then develop an independent repair model that processes video input for poor lighting conditions, when applicable, and corrects potential errors in output for increased accuracy. We then test this model with LipType and other speech and silent speech recognizers to demonstrate its effectiveness.
                    </p>
                </div>
                <div class="CCSconcepts">
                    <ccs2012>
                        <small>
                            <span style="font-weight:bold;">CCS Concepts:</span>
                             • 
                            <strong>Computing methodologies → Computer vision</strong>
                            ; • 
                            <strong>Human-centered computing → Text input</strong>
                            ;
                        </small>
                    </ccs2012>
                </div>
                <br/>
                <div class="classifications">
                    <div class="author">
                        <span style="font-weight:bold;">
                            <small>Keywords:</small>
                        </span>
                        <span class="keyword">
                            <small>Silent speech recognition; deep learning; language modeling; text input.</small>
                        </span>
                    </div>
                    <br/>
                    <div class="AcmReferenceFormat">
                        <p>
                            <small>
                                <span style="font-weight:bold;">ACM Reference Format:</span>
                                <br/>
                                Laxmi Pandey and Ahmed Sabbir Arif. 2021. LipType: A Silent Speech Recognizer Augmented with an Independent Repair Model. In 
                                <em>CHI Conference on Human Factors in Computing Systems (CHI '21), May 8–13, 2021, Yokohama, Japan.</em>
                                 ACM, New York, NY, USA 19 Pages. 
                                <a href="https://doi.org/10.1145/3411764.3445565" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3411764.3445565</a>
                            </small>
                        </p>
                    </div>
                </div>
            </section>
        </section>
        <section class="body">
            <figure id="fig1">
                <img src="https://dl.acm.org/cms/attachment/07e80c51-41e6-467c-b3ab-bbaf2f1a5ffb/chi21-522-fig1.jpg" class="img-responsive" alt="An overview of LipType" aria-describedby="fig002"/>
                <p style="display:none" id="fig002">The figure shows an overview of LipType, comprising of seven components: the first shows a female participant inputting text on the smartphone using silent speech, the second shows an image sequence of her facial recording, the third shows the light enhancement, the fourth shows the enhanced lip sequences with landmarks, the fifth shows a deep neural network processing, the sixth shows the error correction, and the seventh shows the recognized text “This is a very good idea”.</p>
                <figcaption>
                    <span class="figure-number">Figure 1:</span>
                    <span class="figure-title">An overview of LipType: Automatic segmentation of lip sequences and its classification into text with an end-to-end deep neural network.</span>
                </figcaption>
            </figure>
            <section id="sec-2">
                <header>
                    <div class="title-info">
                        <h2>
                            <span class="section-number">1</span>
                             INTRODUCTION
                        </h2>
                    </div>
                </header>
                <p>
                    There are numerous scenarios where speech is not a viable mode of communication. First, the surroundings may not be favorable for speech-based communication: a person could be near a busy market or in a crowded restaurant where the surrounding noise makes speech difficult to recognize. Second, a person may not wish to speak out loud because of privacy and security concerns or could be in a public setting where others do not want to be disturbed, such as in a library or museum. Finally, and most importantly, many people have difficulties in speaking or are unable to speak entirely due to a range of speech and neurological disorders
                    <a class="fn" href="#fn1" id="foot-fn1">
                        <sup>1</sup>
                    </a>
                    . Although many augmentative and alternative communication (AAC) devices are available to help them vocalize, these devices produce unnatural sounding vocalization. This prevents users from communicating effectively with other humans and technologies like voice-controlled virtual assistants. Hence, the development of better communication methods is needed to improve this population's accessibility to the fellow humans and latest technological advancements. A system that can understand speech by visually interpreting the movements of the speaker's lips, known as lip reading or silent speech recognition (Fig.&nbsp; 
                    <a class="fig" href="#fig1">1</a>
                    ), can mitigate many of these challenges. Since developing a new system and acquiring new datasets require an enormous amount of time, effort, and other resources, in this work we exploited a state-of-the-art silent speech recognizers, LipNet [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>
                    ]. Based on preliminary investigations, we found out that LipNet and other existing recognizers have substantially slower response time due to their architecture. Besides, they do not perform well under poor lighting conditions and tend to make lexical and linguistic errors with varying speaking rates and accents [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0078">78</a>
                    ]. As an initial step, we developed an independent repair model to enable LipNet and other existing recognizers to perform well in dark and dusky lighting conditions where the frontal pose of the user is not clearly visible to extract meaningful information, and to compensate for the recognition errors made by the recognizer. More reliable speech and silent speech systems could potentially be used as a medium for input and interaction with various computer systems, incorporated in day-to-day usage.
                </p>
                <p>
                    The contribution of this work is thus threefold. First, the development of LipType, an optimized version of LipNet for improved speed and accuracy. Second, the development of an independent repair model, a multi-stage pipeline compensating for poor lighting conditions and potential recognition errors for increased accuracy of speech and silent speech recognizers. Third, an empirical demonstration of the the repair model's effectiveness on multiple speech and silent speech recognizers. Further, the source code
                    <a class="fn" href="#fn2" id="foot-fn2">
                        <sup>2</sup>
                    </a>
                     and dataset
                    <a class="fn" href="#fn3" id="foot-fn3">
                        <sup>3</sup>
                    </a>
                     used in this work are freely available for future research and development in the area.
                </p>
            </section>
            <section id="sec-3">
                <header>
                    <div class="title-info">
                        <h2>
                            <span class="section-number">2</span>
                             RELATED WORK
                        </h2>
                    </div>
                </header>
                <p>This work intersects with four areas of interest: silent speech recognition, low-light image enhancement, recognition error correction, and silent input and interaction on mobile devices.</p>
                <section id="sec-4">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">2.1</span>
                                 Silent Speech Recognition
                            </h3>
                        </div>
                    </header>
                    <p>
                        There is a rich literature on silent speech recognition. Here, we only discuss the works that are closely related to ours (see [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0111">111</a>
                        ] for a comprehensive review). Recently, there have been attempts to apply deep learning to silent speech recognition [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0093">93</a>
                        ]. However, most of these approaches perform only at phoneme- or word-level. Koller et&nbsp;al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0064">64</a>
                        ] trained an image classifier using convolutional neural network (CNN) to differentiate between visemes
                        <a class="fn" href="#fn4" id="foot-fn4">
                            <sup>4</sup>
                        </a>
                         on a sign language dataset of signers mouthing words. Noda et&nbsp;al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0075">75</a>
                        ] also used CNN to predict phonemes in spoken Japanese. Tamura et&nbsp;al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0097">97</a>
                        ] used deep bottleneck features (DBF) to encode shallow input features, such as latent dirichlet allocation (LDA) and GA-based informative feature (GIF) [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0099">99</a>
                        ] for word recognition. Petridis and Pantic [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0081">81</a>
                        ] also used DBF to encode every video frame and trained a long short-term memory (LSTM) classifier for word-level classification. Wand et&nbsp;al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0102">102</a>
                        ], on the other hand, used an LSTM with histogram of oriented gradient (HoG) input features to recognize words. Chung and Zisserman [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>
                        ] developed CNN architectures for classifying multi-frame time series of lip movements. LipNet [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>
                        ] is an end-to-end model for phrase-level lip reading by predicting character sequences (further discussed in a later section). Afouras et&nbsp;al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                        ] also enabled phrase-level lip reading by utilizing an encoder-decoder structure with multi-head attentions. Chung et&nbsp;al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>
                        ] developed the Watch, Listen, Attend and Spell (WLAS) network that uses dual attention mechanism for visual attention to transcribe videos of mouth motion to characters.
                    </p>
                </section>
                <section id="sec-5">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">2.2</span>
                                 Low-Light Image Enhancement
                            </h3>
                        </div>
                    </header>
                    <p>
                        The problems of underexposed low-light images are very common, solutions to mitigate it have been a popular research topic. Researchers have developed a variety of techniques that can improve image quality. The classical image enhancement methods involve two categories: i) retinex-based methods, which are based on retinex theory [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0065">65</a>
                        ]. Recent examples of these approaches are Lime [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>
                        ], naturalness preserved enhancement [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0104">104</a>
                        ], Retinex [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0056">56</a>
                        ], and simultaneous reflectance and illumination estimation [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>
                        ]. ii) histogram equalization methods, which manipulate the gray levels of individual pixels based on the image histogram. Recent examples include contextual and variational contrast enhancement [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>
                        ], weighted thresholded histogram equalization [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>
                        ], and layered difference representation [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0066">66</a>
                        ]. In recent years, several methods based on deep learning image processing techniques have been proposed. One successful example is the developed pipeline for processing low-light images based on end-to-end training of a fully-convolutional network [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>
                        ]. However, they reported that their model showed imperfect results for humans faces. Another work [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0105">105</a>
                        ] utilizes encoder-decoder network to achieve the low-light enhancement for real under exposed images. Other works [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0067">67</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0106">106</a>
                        ] have also showed the effectiveness of deep learning methods on low light image enhancement.
                    </p>
                </section>
                <section id="sec-6">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">2.3</span>
                                 Recognition Error Correction
                            </h3>
                        </div>
                    </header>
                    <p>
                        Automatic detection and correction of recognition errors have become an important research area. The aim is to automatically detect and partially or fully correct errors, regardless of the recognition system used. Zhou et al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0110">110</a>
                        ] addressed the issue of error detection in recognition systems using data-mining classifiers such as naive Bayes (NB), neural networks (NN), and support vector machines (SVM). These classifiers were trained to identify errors using confidence scores and linguistic information present in the recognized output. Another work [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>
                        ] proposed extraction of additional features from the confusion networks to estimate correctness probability using logistic regression. Pellegrini et al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0079">79</a>
                        ] investigated the use of a Markov chains (MC) classifier with two states: error state and correct state, to model errors. Chen et al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>
                        ] proposed a system for error detection in conversational spoken language translation. This system uses additional features provided as the feedback of statistical machine translation (SMT), including SMT confidence estimates, posteriors from named entity detection (NED) and an automated word boundary detector to verify the word boundaries of recognition output, in order to improve error detection and correction. Sarma et al. [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0089">89</a>
                        ] built a recognition error detector and corrector using co-occurrence analysis. In the same context, Bassil and Semaan [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>
                        ] proposed a post-editing ASR error correction method based on Microsoft N-Gram dataset for detecting and correcting spelling errors generated by recognition systems. The detection process detects on-word spelling errors in reference with the Microsoft N-Gram dataset, and the correction process generates correction suggestions for the detected word errors by selecting the best candidate for the correction using contextual information. Other works [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0068">68</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0080">80</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0091">91</a>
                        ] have explored non-decoder based post-processing error detection and correction.
                    </p>
                </section>
                <section id="sec-7">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">2.4</span>
                                 Silent Input and Interaction on Mobile Devices
                            </h3>
                        </div>
                    </header>
                    <p>
                        Silent input enables users to interact with mobile devices using speech commands without the need to produce any audible sound. There have been several previous attempts in achieving silent speech interaction. These works have explored silent input and interaction techniques using different sensors (e.g., electromagnetic articulography (EMA) [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>
                        ], electroencephalogram (EEG) [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0083">83</a>
                        ], electromyography (EMG) [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0057">57</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0058">58</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0059">59</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0072">72</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0090">90</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0103">103</a>
                        ], ultrasound imaging [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0051">51</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0052">52</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0061">61</a>
                        ], [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>
                        ], vibrational sensors of glottal activity [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0074">74</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0077">77</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0086">86</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0098">98</a>
                        ], speech motor cortex implants [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>
                        ] and non-audible murmur (NAM) microphone [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0047">47</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0048">48</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0073">73</a>
                        ]) to recover the speech content produced without vibration of the vocal folds by detecting tongue, facial, and throat movements. Another research [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0084">84</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0095">95</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0096">96</a>
                        ] used a brain-computer interface (BCI) with intracortical microelectrode to predict users’ intended speech information directly from the brain activity involved during speech production. Another work [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0053">53</a>
                        ] used a multimodal imaging system for speech recognition, focusing on lip visualization. Another work [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0060">60</a>
                        ] presented a wearable interface, AlterEgo, which utilizes EMG sensors placed on face to capture the neuromuscular signals. However, these prior works use an invasive setup, impeding the adaptability of these solutions in real-world scenarios. More recently, improvements have been made in silent speech recognition by incorporating advanced machine learning techniques and computer vision technologies [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0082">82</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0093">93</a>
                        ]. One recent research [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0094">94</a>
                        ] developed an interaction technique that allows users to issue commands on their smartphone through silent speech. They used front camera as a natural sensor to capture the motion of the lips, and recognize it into text.
                    </p>
                </section>
            </section>
            <section id="sec-8">
                <header>
                    <div class="title-info">
                        <h2>
                            <span class="section-number">3</span>
                             LIPTYPE: AN OPTIMIZED LIPNET MODEL
                        </h2>
                    </div>
                </header>
                <p>
                    We used LipNet as the backbone model based on a study comparing LipNet [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>
                    ], LCANet [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0107">107</a>
                    ], Transformer [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                    ], and WAS [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>
                    ] models. The former two are trained on GRID dataset [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                    ], the latter two on LRS dataset [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                    ]. In an evaluation with 50 random videos from the respective datasets, LipNet and LCANet yielded similar WER (&nbsp;4%), while Transformer and WAS were more error-prone (&gt; 49% WER). Of the two best performed models, we picked LipNet as it is more widely used than LCANet.
                </p>
                <p>
                    LipNet [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>
                    ] is an existing end-to-end sentence-level model that maps a variable-length sequence of video frames to text, making use of a deep 3-dimensional convolutional neural network (3D-CNN) [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0055">55</a>
                    ], a recurrent network, and the connectionist temporal classification loss. The model was trained on grid dataset comprising of highly constrained vocabulary. Although LipNet has proven to be promising, it has several limitations. First, LipNet is focused on capturing spatial and temporal information using deep 3D-CNN that neglects the hidden information between channel correlations in spatial and temporal directions [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>
                    ], limiting the performance of the architecture. Further, the use of a deep 3D-CNN unnecessarily increases computational complexity and memory intensiveness. We address these issues in LipType, an optimized version of LipNet for improved speed and accuracy.
                </p>
                <p>
                    In LipType, we combined a shallow 3D-CNN (1-layer) and a deep 2D-CNN (34-layer ResNet [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>
                    ]) integrated with squeeze and excitation (SE) [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0050">50</a>
                    ] blocks (SE-ResNet) to capture both spatial and temporal information. We used this hybrid-CNN model to address the limitations of 3D-CNN that it neglects the information between channel correlations and increases computational complexity, as well as 2D-CNN's inability to capture temporal information. SE-ResNet adaptively recalibrates channel-wise feature responses by explicitly modelling inter-dependencies between the channels to improve the quality of feature representations. Moreover, it is computationally lightweight and imposes only a slight increase in model complexity and computational burden [
                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0050">50</a>
                    ]. Thus, we hypothesize that the proposed hybrid frontend module will reduce the overall computational complexity of LipNet and improve its performance.
                </p>
                <section id="sec-9">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">3.1</span>
                                 The Network
                            </h3>
                        </div>
                    </header>
                    <p>
                        The LipType network consists of two sub-modules (or sub-networks): a 
                        <em>spatiotemporal feature extraction</em>
                         frontend that takes a sequence of video frames and outputs one feature vector per frame and a 
                        <em>sequence modeling</em>
                         module that inputs the sequence of per-frame feature vectors and outputs a sentence character by character, as shown in Fig.&nbsp; 
                        <a class="fig" href="#fig2">2</a>
                        . We describe these modules in the following sections. 
                    </p>
                    <figure id="fig2">
                        <img src="https://dl.acm.org/cms/attachment/1f693412-cc05-4cd1-875f-b8052d345387/chi21-522-fig2.jpg" class="img-responsive" alt="Network architecture of LipType" aria-describedby="fig003"/>
                        <p style="display:none" id="fig003">The Figure describes the network architecture of LipType. It consists of eight components, connected in the following order: Video Frames, Spatio-temporal Convolution (3D CNN), Squeeze and Excitation Residual Network (2D SE-ResNet), Bidirectional-GRU, Linear Layer, Softmax, CTC Loss, and Decoding. A sequence of video frames is fed to a 3D CNN, followed by 2D SE-ResNet for spatiotemporal feature extraction. The extracted features are processed by two Bi-GRUs, followed by a linear layer and a softmax. The network is trained entirely end-to-end with CTC loss.</p>
                        <figcaption>
                            <span class="figure-number">Figure 2:</span>
                            <span class="figure-title">
                                Architecture of LipType: a sequence of 
                                <em>T</em>
                                 frames is fed to a 1-layer 3D CNN, followed by 34-layer 2D SE-ResNet for spatiotemporal feature extraction. The extracted features are processed by two Bi-GRUs, followed by a linear layer and a softmax. The network is trained entirely end-to-end with CTC loss.
                            </span>
                        </figcaption>
                    </figure>
                    <p></p>
                    <section id="sec-10">
                        <p>
                            <em>
                                <span class="section-number">3.1.1</span>
                                 Spatiotemporal Feature Extraction.
                            </em>
                             It starts with the extraction of a mouth-centred cropped image of size H:100 &times; W:50 pixels per video frame. For this, videos are first pre-processed using DLib face detector [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0062">62</a>
                            ] and the iBug face landmark predictor [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0088">88</a>
                            ] with 68 facial landmarks combined with Kalman Filtering. Then, a mouth-centred cropped image is extracted by applying affine transformations. The sequence of 
                            <em>T</em>
                             mouth-cropped frames are then passed to 3D-CNN, with a kernel dimension of T:5 &times; W:7 &times; H:7, followed by Batch Normalization (BN) [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0054">54</a>
                            ] and Rectified Linear Units (ReLU) [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>
                            ]. The extracted feature maps are then passed through 34-layer 2D SE-ResNet that gradually decreases the spatial dimensions with depth, until the feature becomes a single dimensional tensor per time step.
                        </p>
                    </section>
                    <section id="sec-11">
                        <p>
                            <em>
                                <span class="section-number">3.1.2</span>
                                 Sequence Modeling.
                            </em>
                             The extracted features are processed by 2-Bidirectional Gated Recurrent Units (Bi-GRUs) [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>
                            ]. Each time-step of the GRU output is processed by a linear layer, followed by a softmax layer over the vocabulary, then an end-to-end model is trained with connectionist temporal classification (CTC) loss [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0040">40</a>
                            ]. The softmax output is decoded with a left-to-right beam search [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>
                            ] using Stanford-CTC's decoder [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0070">70</a>
                            ] and 5-gram character language model [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0041">41</a>
                            ] to recognize the spoken utterances. The model is capable of mapping variable-length video sequences to text sequences.
                        </p>
                    </section>
                </section>
            </section>
            <section id="sec-12">
                <header>
                    <div class="title-info">
                        <h2>
                            <span class="section-number">4</span>
                             EXPERIMENT 1: LIPTYPE MODEL
                        </h2>
                    </div>
                </header>
                <p>We conducted an experiment to compare the performance of LipNet and LipType.</p>
                <section id="sec-13">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">4.1</span>
                                 Dataset
                            </h3>
                        </div>
                    </header>
                    <p>
                        For a fair comparison between the two models, we trained the LipType model on the same GRID dataset [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                        ] on which the LipNet model was trained. It comprises of short and formulaic video clips of a person's face when uttering a highly constrained vocabulary in a specific order (
                        <em>N</em>
                         = 34). Similar to a previous experiment investigating the performance of LipNet with overlapped speakers [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>
                        ], this experiment used 21,635 videos for training and 7,140 videos for evaluation.
                    </p>
                </section>
                <section id="sec-14">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">4.2</span>
                                 Implementation
                            </h3>
                        </div>
                    </header>
                    <p>
                        To avoid any potential confounding factor, we trained both models from scratch with the same training parameters. The number of frames was fixed to 75. Longer image sequences were truncated and shorter sequences were padded with zeros. We applied a channel-wise dropout [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0092">92</a>
                        ] of 0.5. The model was trained end-to-end by the Adam optimizer [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0063">63</a>
                        ] for 60 epochs with a batch size of 50. The learning rate was set to 10
                        <sup>− 4</sup>
                        . The network was implemented based on the Keras deep-learning platform with TensorFlow [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>
                        ] as the backend. We trained and tested both models on NVIDIA GeForce 1080Ti GPU board.
                    </p>
                </section>
                <section id="sec-15">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">4.3</span>
                                 Performance Metrics
                            </h3>
                        </div>
                    </header>
                    <p>We used the following metrics to benchmark the proposed framework.</p>
                    <ul class="list-no-style">
                        <li id="list1" label="•">
                            <strong>Word error rate (WER)</strong>
                             is the minimum number of operations required to transform the predicted text to the ground truth, divided by the number of words in the ground truth. It is calculated using the following equation, where 
                            <em>S</em>
                             is the number of substitutions, 
                            <em>D</em>
                             is the number of deletions, 
                            <em>I</em>
                             is the number of insertions, and 
                            <em>N</em>
                             is the number of words in the ground truth. 
                                    
                            <div class="table-responsive" id="Xeq1">
                                <div class="display-equation">
                                    <span class="tex mytex">\begin{equation} WER = \frac{S+D+I}{N} \end{equation} </span>
                                    <br/>
                                    <span class="equation-number">(1)</span>
                                </div>
                            </div>
                            <br/>
                        </li>
                        <li id="list2" label="•">
                            <strong>Words per minute (WPM)</strong>
                             is a commonly used text entry metric that signifies the rate in which words (= 5 chars) are entered [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>
                            ]. It is calculated using the following equation, where 
                            <em>T</em>
                             is the number of recognized words, 
                            <em>t</em>
                             is the sum of speaking time and computation time in seconds, the constant 60 is the number of seconds per minute, and the factor of one fifth accounts for the average length of a word in the English language. 
                                    
                            <div class="table-responsive" id="Xeq2">
                                <div class="display-equation">
                                    <span class="tex mytex">\begin{equation} WPM = \frac{\left| T \right|-1}{t} \times 60 \times \frac{1}{5} \end{equation} </span>
                                    <br/>
                                    <span class="equation-number">(2)</span>
                                </div>
                            </div>
                            <br/>
                        </li>
                        <li id="list3" label="•">
                            <strong>Computation time (CT)</strong>
                             is the total time required by the model to predict a phrase. It does not include the time users take to speak a phrase.
                            <br/>
                        </li>
                    </ul>
                    <figure id="fig3">
                        <img src="https://dl.acm.org/cms/attachment/ad971b43-4b4d-4791-b011-6a9c667a701a/chi21-522-fig3.jpg" class="img-responsive" alt="Performance evaluation of LipType" aria-describedby="fig004"/>
                        <p style="display:none" id="fig004">Three bar plots labeled (a), (b), and (c). Plot (a) shows mean Word Error Rate (Word Error Rate (WER)) from 0 to 6 on the Y-axis against LipNet and LipType on the X-axis. Plot (b) shows mean Words per Minute (Words per Minute (WPM)) from 0 to 8 on the Y-axis against LipNet and LipType on the X-axis. Plot (c) shows mean Computation Time (Computation Time (CT)) from 0 to 18 on the Y-axis against LipNet and LipType on the X-axis.</p>
                        <figcaption>
                            <span class="figure-number">Figure 3:</span>
                            <span class="figure-title">Performance comparison of LipNet and LipType in terms of a) word error rate, b) words per minute, and c) computation time. Reported values are the average of all values. Values inside the brackets are standard deviations (SD). Error bars represent &plusmn;1 standard deviation.</span>
                        </figcaption>
                    </figure>
                </section>
                <section id="sec-16">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">4.4</span>
                                 Results
                            </h3>
                        </div>
                    </header>
                    <p>
                        In the experiment, LipType outperformed LipNet in terms of input speed, accuracy, and computation time. LipType achieved 2.6% WER, 6.4 WPM, and 6.3 seconds CT (Fig.&nbsp;
                        <a class="fig" href="#fig3">3</a>
                        ). In comparison with LipNet, it exhibited a 47% reduction in WER, 39% increase in WPM, and 8.6 seconds reduction in CT. These findings confirm our intuition that extracting spatiotemporal features using the hybrid of a shallow 3D-CNN and a deep 2D-CNN integrated with SE blocks, instead of only 3D-CNN, will reduce the overall computational complexity and improve performance.
                    </p>
                </section>
            </section>
            <section id="sec-17">
                <header>
                    <div class="title-info">
                        <h2>
                            <span class="section-number">5</span>
                             REPAIR MODEL: LIGHT ENHANCEMENT AND ERROR REDUCTION
                        </h2>
                    </div>
                </header>
                <p>
                    This section presents a new repair model, a multi-stage pipeline that accounts for poor lighting conditions in input videos and potential errors in the recognition. It includes a 
                    <em>pre-processing</em>
                     step to enhance videos with poor lighting conditions and a 
                    <em>post-processing</em>
                     step to automatically detect and correct potential errors generated by the recognizer. A key consideration for this model was its independence, to make sure it is not reliant on a specific recognizer so that it can be used with a variety of speech and silent speech recognition models.
                </p>
                <section id="sec-18">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">5.1</span>
                                 Pre-Processing: Light Enhancement
                            </h3>
                        </div>
                    </header>
                    <p>There are various factors that can affect the performance of silent speech recognition, for example, uncontrolled lighting, blur, low-resolution, compression artifacts, occlusions, viewing angles, accent, pace of speech, etc. However, most of the factors can be mitigated by replacing the hardware (blur, low-resolution, compression artifacts, etc.) or by the user (occlusions, viewing angles, pace of speech, etc.). Lighting, in contrast, is one the factor that cannot always be controlled.</p>
                    <p>
                        Making recognition more reliable under uncontrolled lighting conditions is one of the major challenges for practical silent speech recognition models. Existing models do not account for lighting variations, making them unreliable in poorly lit places. We tackle this by adding a pre-processing step to the LipType recognition model. For this, we improved GLADNet [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0105">105</a>
                        ], a low-light image enhancement network, and adapted it for enhancing input videos. We used GLADNet because it demonstrated a much better performance with actual under-exposed images compared to the other models, both in terms of quality [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0056">56</a>
                        ] and computation complexity [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0067">67</a>
                        , 
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0106">106</a>
                        ].
                    </p>
                    <section id="sec-19">
                        <p>
                            <em>
                                <span class="section-number">5.1.1</span>
                                 The Network.
                            </em>
                             The light enhancement network learns an end-to-end mappings from low-light images to normal-light images. It processes videos in a frame-by-frame manner, as illustrated in Fig.&nbsp;
                            <a class="fig" href="#fig4">4</a>
                            . The architecture of the network comprises of two adjacent steps: the first is for 
                            <em>global illumination estimation</em>
                             and the second is for 
                            <em>detail reconstruction</em>
                            . 
                        </p>
                        <figure id="fig4">
                            <img src="https://dl.acm.org/cms/attachment/42a874fd-b0d4-4dce-88b9-073d4da59132/chi21-522-fig4.jpg" class="img-responsive" alt="Network architecture of preprocessing module" aria-describedby="fig005"/>
                            <p style="display:none" id="fig005">The figure describes the network architecture of preprocessing module for low-light image enhancement. It shows a sequence of low-light images fed through GLADNet, comprising of Illumination Estimation and Detail Reconstruction, to output enhanced images. These images are compared with the normal-light images to compute the loss, which is fed back to GLADNet.</p>
                            <figcaption>
                                <span class="figure-number">Figure 4:</span>
                                <span class="figure-title">Architecture of the pre-processing (light enhancement) network: a sequence of low-light images is fed through the network where the enhanced images are compared with the normal-light images to compute the loss, which is then backpropagated to fine-tune and optimize the model weights and biases.</span>
                            </figcaption>
                        </figure>
                        <p></p>
                        <p>
                            In the global illumination estimation step, input is down-sampled to a fixed size feature map using nearest-neighbor interpolation. Then, it is passed through an encoder-decoder network
                            <a class="fn" href="#fn5" id="foot-fn5">
                                <sup>5</sup>
                            </a>
                             to estimate the global illumination of the input. The estimated feature maps are then re-scaled to the original size using a resize convolution block. Then, the re-scaled feature maps are passed to the detail reconstruction step comprising of three convolutional layers. This step adjusts the illumination of the input image by assembling predicted global illumination and input image information, and fills in the details lost during the down- and up-sampling processes. Inspired by a previous work [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0109">109</a>
                            ], we investigated the consequences of replacing the L1 loss function used in the training of GLADNet with alternative loss functions. Given a collection of 
                            <em>N</em>
                             training sample pairs 
                            <em>
                                X
                                <sub>i</sub>
                            </em>
                             , 
                            <em>
                                Y
                                <sub>i</sub>
                            </em>
                            , where 
                            <em>
                                X
                                <sub>i</sub>
                            </em>
                             is low-light input image and 
                            <em>
                                Y
                                <sub>i</sub>
                            </em>
                             is normal-light ground truth image, the following loss functions can be defined.
                        </p>
                        <ol class="list-no-style">
                            <li id="list4" label="(1)">
                                <strong>L1 Loss</strong>
                                 (or mean-absolute-error loss) minimizes the sum of the absolute differences between the predicted or generated image and the ground truth. 
                                         
                                <div class="table-responsive" id="Xeq3">
                                    <div class="display-equation">
                                        <span class="tex mytex">\begin{equation} L1(X,Y) = \frac{1}{N}\sum _{i=1}^{N} (X_i - Y_i) \end{equation} </span>
                                        <br/>
                                        <span class="equation-number">(3)</span>
                                    </div>
                                </div>
                                <br/>
                            </li>
                            <li id="list5" label="(2)">
                                <strong>L2 Loss</strong>
                                 (or mean-squared-error loss) minimizes the sum of the squared differences between the predicted or generated image and the ground truth. 
                                         
                                <div class="table-responsive" id="Xeq4">
                                    <div class="display-equation">
                                        <span class="tex mytex">\begin{equation} L2(X,Y) = \frac{1}{N}\sum _{i=1}^{N} (X_i - Y_i)^{2} \end{equation} </span>
                                        <br/>
                                        <span class="equation-number">(4)</span>
                                    </div>
                                </div>
                                <br/>
                            </li>
                            <li id="list6" label="(3)">
                                <strong>Multi-scale structural similarity loss</strong>
                                 (MSSSIM) [
                                <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0109">109</a>
                                ] minimizes the loss related to the sum of structural-similarity scores across all image pixels, in terms of luminance, contrast, and structure. 
                                         
                                <div class="table-responsive" id="Xeq5">
                                    <div class="display-equation">
                                        <span class="tex mytex">\begin{equation} MSSSIM(X,Y) = -\sum _{i=1}^{N} MSSSIM(X_i - Y_i) \end{equation} </span>
                                        <br/>
                                        <span class="equation-number">(5)</span>
                                    </div>
                                </div>
                                <br/>
                            </li>
                            <li id="list7" label="(4)">
                                <strong>MSSSIM-L1 loss</strong>
                                 captures MSSSIM's ability to preserve the contrast in high-frequency regions and L1’s ability to preserves colors and luminance. In the equation below, 
                                <em>G</em>
                                 is the Gaussian filter, 
                                <em>α</em>
                                 is the weighting factor to roughly balance the contribution of the two losses. We empirically set 
                                <em>α</em>
                                 = 0.81
                                <a class="fn" href="#fn6" id="foot-fn6">
                                    <sup>6</sup>
                                </a>
                                . 
                                         
                                <div class="table-responsive" id="Xeq6">
                                    <div class="display-equation">
                                        <span class="tex mytex">\begin{equation} \text{MSSSIM-L1}(X,Y) = \alpha \cdot MSSSIM + (1-\alpha)\cdot G_{\sigma }\cdot L1 \end{equation} </span>
                                        <br/>
                                        <span class="equation-number">(6)</span>
                                    </div>
                                </div>
                                <br/>
                            </li>
                            <li id="list8" label="(5)">
                                <strong>MSSSIM-L2 loss</strong>
                                 captures MSSSIM's ability to preserve the contrast in high-frequency regions and L2’s ability to remove noise and ringing artifacts. Like MSSSIM-L1, 
                                <em>α</em>
                                 = 0.81 and 
                                <em>G</em>
                                 is the Gaussian filter. 
                                         
                                <div class="table-responsive" id="Xeq7">
                                    <div class="display-equation">
                                        <span class="tex mytex">\begin{equation} \text{MSSSIM-L2}(X,Y) = \alpha \cdot MSSSIM + (1-\alpha)\cdot G_{\sigma }\cdot L2 \end{equation} </span>
                                        <br/>
                                        <span class="equation-number">(7)</span>
                                    </div>
                                </div>
                                <br/>
                            </li>
                        </ol>
                    </section>
                </section>
                <section id="sec-20">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">5.2</span>
                                 Experiment 2: Light Enhancement Network
                            </h3>
                        </div>
                    </header>
                    <p>We evaluated the performance of the light enhancement network trained with the above five loss functions.</p>
                    <section id="sec-21">
                        <p>
                            <em>
                                <span class="section-number">5.2.1</span>
                                 Dataset.
                            </em>
                             We trained and validated the network on the GLADNet dataset [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0105">105</a>
                            ] that comprises of 5,000 image pairs of low and normal light images. We used 4,000 pairs for training and the remaining 1,000 pairs for testing.
                        </p>
                    </section>
                    <section id="sec-22">
                        <p>
                            <em>
                                <span class="section-number">5.2.2</span>
                                 Performance Metrics.
                            </em>
                             We used the following two standard image quality metrics [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0049">49</a>
                            ].
                        </p>
                        <ul class="list-no-style">
                            <li id="list9" label="•">
                                <strong>Peak signal to noise ratio (PSNR)</strong>
                                 computes the peak signal-to-noise ratio between two images in decibels. This ratio is used as a quality measurement between the original and an enhanced image. The higher the PSNR the better the quality of the enhanced image.
                                <br/>
                            </li>
                            <li id="list10" label="•">
                                <strong>Structural similarity metric (SSIM)</strong>
                                 measures the perceptual difference between two similar images. Unlike PSNR, SSIM is based on visible structures in the image. The lower the SSIM the better the quality of the enhanced image.
                                <br/>
                            </li>
                        </ul>
                    </section>
                    <section id="sec-23">
                        <p>
                            <em>
                                <span class="section-number">5.2.3</span>
                                 Implementation.
                            </em>
                             We trained the network for 70 epochs with a batch size of 32. It was optimized using Adam [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0063">63</a>
                            ]. The learning rate was set to 10
                            <sup>− 3</sup>
                            . The network was implemented on the Keras deep-learning platform with TensorFlow [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>
                            ] as the backend. We trained and tested the network on NVIDIA GeForce 1080Ti GPU board.
                        </p>
                    </section>
                    <section id="sec-24">
                        <p>
                            <em>
                                <span class="section-number">5.2.4</span>
                                 Results.
                            </em>
                             Table 
                            <a class="tbl" href="#tab1">1</a>
                             presents the performance comparison of GLADNet trained on the aforementioned five loss functions in terms of averaged PSNR and SSIM. It can be seen that MSSSIM-L1 achieved the highest PSNR and outperformed other loss functions substantially in the SSIM measure. Therefore, we used GLADNet trained with MSSSIM-L1 loss function to enhance poor lighting input videos for more reliable silent speech recognition.
                        </p>
                        <div class="table-responsive" id="tab1">
                            <div class="table-caption">
                                <span class="table-number">Table 1:</span>
                                <span class="table-title">Averaged peak signal to noise ratio (PSNR) and structural similarity metric (SSIM) for the five investigated loss functions. For MSSSIM, the reported values are obtained as averages of the three color channels (RGB). The best results are highlighted in bold.</span>
                            </div>
                            <table class="table">
                                <thead>
                                    <tr>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Metric</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Low-Light Image</strong>
                                        </th>
                                        <th style="text-align:center;border-bottom: 2pt solid #000000;" colspan="5">
                                            <strong>Enhanced Image</strong>
                                        </th>
                                    </tr>
                                    <tr>
                                        <th style="text-align:center;border-right: 2pt solid #000000;"></th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;"></th>
                                        <th style="text-align:center;border-bottom: 2pt solid #000000;" colspan="5">Loss Function</th>
                                    </tr>
                                    <tr>
                                        <th style="text-align:center;border-right: 2pt solid #000000;"></th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;"></th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <em>L1</em>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <em>L2</em>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <em>MSSSIM</em>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <em>
                                                <strong>MSSSIM-L1</strong>
                                            </em>
                                        </th>
                                        <th>
                                            <em>MSSSIM-L2</em>
                                        </th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">PSNR</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">19.74</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">26.22</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">25.66</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">26.11</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>27.34</strong>
                                        </td>
                                        <td>26.13</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">SSIM</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">0.46</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">0.7822</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">0.7574</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">0.7890</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>0.8091</strong>
                                        </td>
                                        <td>0.7911</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </section>
                </section>
                <section id="sec-25">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">5.3</span>
                                 Post-Processing: Error Reduction
                            </h3>
                        </div>
                    </header>
                    <p>
                        This section presents a new algorithm for predicting and automatically correcting potential recognition errors by a speech or silent speech recognizer. It comprises of two sub-modules: an 
                        <em>error minimization</em>
                         module that corrects potential errors in the recognized character sequence using deep denoising autoencoder (DDA) [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0101">101</a>
                        ] and a 
                        <em>sequence decoder</em>
                         module that converts corrected character sequence to meaningful word sequences using spell-checker and a custom language model. The architecture of the network is illustrated in Fig.&nbsp;
                        <a class="fig" href="#fig5">5</a>
                        . 
                    </p>
                    <figure id="fig5">
                        <img src="https://dl.acm.org/cms/attachment/4c9cadf0-1ef6-4130-a3b1-eb64a7fa28fb/chi21-522-fig5.jpg" class="img-responsive" alt="Network architecture of post-processing module" aria-describedby="fig006"/>
                        <p style="display:none" id="fig006">The figure shows the post-processing steps, illustrating the steps that correct the potential errors in the recognized output. It starts with the conversion of a recognized raw sequence to an incorrect one-hot matrix and then passed to Deep Denoising Autoencoder to get the corrected one-hot matrix. The corrected matrix is then fed to the custom language model.</p>
                        <figcaption>
                            <span class="figure-number">Figure 5:</span>
                            <span class="figure-title">Architecture of post-processing (error reduction) network: the predicted raw sequence is fed to DDA, followed by spell checker and a custom language model.</span>
                        </figcaption>
                    </figure>
                    <p></p>
                    <section id="sec-26">
                        <p>
                            <em>
                                <span class="section-number">5.3.1</span>
                                 Error Reduction.
                            </em>
                             DDA has been successful in the context of reconstructing a noisy signal [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>
                            , 
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0069">69</a>
                            ]. In this work, we used DDA to correct the character sequence predicted by the recognizer. The predicted sequence is represented in the form of a matrix, where each row is a one-hot
                            <a class="fn" href="#fn7" id="foot-fn7">
                                <sup>7</sup>
                            </a>
                             encoded vector, pointing to a particular character out of all. An input to autoencoder is converted to a fixed length sequence: 28 in this case (26 letters of the English alphabet, 1 space character, and 1 newline character), either by subdividing the sequence or by appending zero vectors, depending on the length of the sequence. This fixed length matricized sequence is fed-forwarded through a DDA to obtain an improved character sequence. The DDA is trained with the matricized incorrect character sequence as input and the matricized correct sequence as the labels. This helped in reconstructing the sequence, thus reducing the errors. In order to quantify the errors between incorrect sequence and the ground truth, we used cross-entropy loss [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0108">108</a>
                            ], which is given by the following equation, where 
                            <em>x</em>
                             represents the matricized incorrect character sequence and 
                            <em>z</em>
                             represents the matricized ground truth sequence. 
                        </p>
                        <div class="table-responsive" id="Xeq8">
                            <div class="display-equation">
                                <span class="tex mytex">\begin{equation} Loss(x,z) = -\sum _{k=1}^d[x_klogz_k + (1-x_k)log(1-z_k)] \end{equation} </span>
                                <br/>
                                <span class="equation-number">(8)</span>
                            </div>
                        </div>
                        <p></p>
                    </section>
                    <section id="sec-27">
                        <p>
                            <em>
                                <span class="section-number">5.3.2</span>
                                 Sequence Decoder.
                            </em>
                             The corrected character sequence embedded with the space and newline characters is first combined to form a sequence of words. The resultant word sequence is then passed to the spell checker
                            <a class="fn" href="#fn8" id="foot-fn8">
                                <sup>8</sup>
                            </a>
                             to be checked for spelling correctness for auto-correction, when necessary. In addtion, a language model (LM) was used to get the most probable sequence of words. We used a traditional count-based LM
                            <a class="fn" href="#fn9" id="foot-fn9">
                                <sup>9</sup>
                            </a>
                            . Typically, n-gram analysis in count-based LM is a forward n-gram. However, we explored and evaluated the advantage of a bidirectional n-gram modeling that accounts for both forward and backward directions. Formally, we consider a string of 
                            <em>n</em>
                             words, 
                            <em>W</em>
                             = 
                            <em>w</em>
                            <sub>1</sub>
                            , 
                            <em>w</em>
                            <sub>2</sub>
                            , ..., 
                            <em>
                                w
                                <sub>n</sub>
                            </em>
                            . In a forward n-gram, the probability of each word is estimated depending on the preceding words: 
                        </p>
                        <div class="table-responsive" id="eq1">
                            <div class="display-equation">
                                <span class="tex mytex">\begin{equation} \begin{split} P_{forward}(W) = &amp; P (w_1 | &lt; start&gt;) * P (w_2 | w_1) * \\ &amp; P(w_3 | w_2) * ... * P (&lt; end&gt; | w_n) \end{split} \end{equation} </span>
                                <br/>
                                <span class="equation-number">(9)</span>
                            </div>
                        </div>
                         In contrast, in a backward n-gram the probability of each word is estimated depending on the succeeding words: 
                               
                        <div class="table-responsive" id="eq2">
                            <div class="display-equation">
                                <span class="tex mytex">\begin{equation} \begin{split} P_{backward}(W) = &amp; P (&lt; start&gt; | w_1) * P (w_1 | w_2) * \\ &amp; P (w_2 | w_3) * ... * P (w_n | &lt; end&gt;) \end{split} \end{equation} </span>
                                <br/>
                                <span class="equation-number">(10)</span>
                            </div>
                        </div>
                         The combined probability of a sentence, thus, is computed by multiplying the forward and backward n-gram probability of each word: 
                               
                        <div class="table-responsive" id="Xeq9">
                            <div class="display-equation">
                                <span class="tex mytex">\begin{equation} \begin{split} P_{combined}(W) = &amp; (P_{forward}(W_1)*P_{backward}(W_1)) *\\ &amp; (P_{forward}(W_2)*P_{backward}(W_2)) *\\ &amp; ...*\\ &amp; (P_{forward}(W_n)*P_{backward}(W_n)) \end{split} \end{equation} </span>
                                <br/>
                                <span class="equation-number">(11)</span>
                            </div>
                        </div>
                         Applying the values from Equations 
                               
                        <a class="eqn" href="#eq1">9</a>
                         and 
                               
                        <a class="eqn" href="#eq2">10</a>
                        , we get: 
                               
                        <div class="table-responsive" id="eq3">
                            <div class="display-equation">
                                <span class="tex mytex">\begin{equation} \begin{split} P_{combined}(W) = &amp; (P (w_1 | &lt; start&gt;) * P (&lt; start&gt; | w_1)) * \\ &amp; (P (w_2 | w_1) * P (w_1 | w_2)) * \\ &amp; (P(w_3 | w_2) * P (w_2 | w_3)) * \\ &amp; ...*\\ &amp; (P (&lt; end&gt; | w_n) * (w_n | &lt; end&gt;)) \end{split} \end{equation} </span>
                                <br/>
                                <span class="equation-number">(12)</span>
                            </div>
                        </div>
                        <p></p>
                        <p>
                            Finally, the network predicts and corrects potential errors committed by the language model in the following three steps. (1) Compare the combined probability of each word, 
                            <em>
                                P
                                <sub>combined</sub>
                                w
                                <sub>n</sub>
                            </em>
                             = 
                            <em>P</em>
                            (
                            <em>
                                w
                                <sub>n</sub>
                            </em>
                            |
                            <em>w</em>
                            <sub>
                                <em>n</em>
                                 − 1
                            </sub>
                            )*
                            <em>P</em>
                            (
                            <em>w</em>
                            <sub>
                                <em>n</em>
                                 − 1
                            </sub>
                            |
                            <em>
                                w
                                <sub>n</sub>
                            </em>
                            ) (Equation 
                            <a class="eqn" href="#eq3">12</a>
                            ), with a pre-defined threshold 
                            <em>τ</em>
                            <sub>1</sub>
                            . If 
                            <em>
                                P
                                <sub>combined</sub>
                                w
                                <sub>n</sub>
                            </em>
                             is less than 
                            <em>τ</em>
                            <sub>1</sub>
                            , the word is considered erroneous. (2) compute edit distance (
                            <em>ED</em>
                            ) between an erroneous word 
                            <em>
                                w
                                <sub>n</sub>
                            </em>
                             and each dictionary word 
                            <em>d</em>
                             to create a list 
                            <em>d</em>
                            ′ of all dictionary words that have an 
                            <em>ED</em>
                             less than a predefined threshold 
                            <em>τ</em>
                            <sub>2</sub>
                            . (3) Replace each word in 
                            <em>d</em>
                            ′ with 
                            <em>
                                P
                                <sub>combined</sub>
                                w
                                <sub>n</sub>
                            </em>
                             in a sentence and output the most frequent word sequence from the dictionary.
                        </p>
                        <p>
                            We conducted an extensive study to select the best combinations of 
                            <em>τ</em>
                            <sub>1</sub>
                             and 
                            <em>τ</em>
                            <sub>2</sub>
                             by analyzing the performance of the proposed LM in the defined context.
                        </p>
                    </section>
                </section>
                <section id="sec-28">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">5.4</span>
                                 Experiment 3: Error Reduction Model
                            </h3>
                        </div>
                    </header>
                    <p>We evaluated each sub-module of the post-processing step. First, we evaluated the architecture for the DDA network. Second, we evaluated the performance of the proposed LM. Finally, we identified the best thresholds values for computing numerical similarities.</p>
                    <section id="sec-29">
                        <p>
                            <em>
                                <span class="section-number">5.4.1</span>
                                 Dataset.
                            </em>
                             We used LIBRISPEECH LM corpus [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                            ] to train and evaluate the post-processing modules. The dataset contains text from 14,500 public domain books. We first filtered out all punctuation, casing, and non-alphanumeric tokens from the original text and extracted the top 200,000 sentences as vocabulary.
                        </p>
                    </section>
                    <section id="sec-30">
                        <p>
                            <em>
                                <span class="section-number">5.4.2</span>
                                 Training and Evaluation of Various DDA Architectures.
                            </em>
                             For training DDA, we randomly divided the dataset into 100,000 sentences as correct set and remaining 100,000 as incorrect set. We then injected one character-level error to each word of each phrase. To inject errors, we simulated the following four types of error to each word in the following sequence: one deletion error (removal of one letter), one transposition error (swapping of two adjacent letters), one replacement error (changing one letter with another), and one insertion error (one additional letter). Table 
                            <a class="tbl" href="#tab2">2</a>
                             presents the statistics of the dataset used for training DDA. It was divided into a split of 80:20% as training:testing set.
                        </p>
                        <div class="table-responsive" id="tab2">
                            <div class="table-caption">
                                <span class="table-number">Table 2:</span>
                                <span class="table-title">Statistics of dataset used for training DDA. The values are the total.</span>
                            </div>
                            <table class="table">
                                <thead>
                                    <tr>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Phrase</strong>
                                        </th>
                                        <th style="text-align:left;border-right: 2pt solid #000000;">
                                            <strong>Word</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Char</strong>
                                        </th>
                                        <th style="text-align:left;border-right: 2pt solid #000000;">
                                            <strong>Correct Word</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Correct Char</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Incorrect Word</strong>
                                        </th>
                                        <th style="text-align:center;">
                                            <strong>Incorrect Char</strong>
                                        </th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">200,000</td>
                                        <td style="text-align:left;border-right: 2pt solid #000000;">6,027,754</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">18,527,816</td>
                                        <td style="text-align:left;border-right: 2pt solid #000000;">2,906,117</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">9,279,253</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">3,121,637</td>
                                        <td style="text-align:center;">9,248,563</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <p>
                            To select the best network architecture for DDA, we trained and evaluated four different architectures (Table 
                            <a class="tbl" href="#tab3">3</a>
                            ). All networks were implemented on the Keras deep-learning platform with TensorFlow [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>
                            ] as the backend and an NVIDIA GeForce 1080Ti as the GPU board. We used Adam [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0063">63</a>
                            ] as the optimization method for training. We trained the networks for 50 epochs with learning rate of 10
                            <sup>− 3</sup>
                            , batch size of 128. Results revealed that the DDA architecture with 5-layers having [128 64 32 64 128] nodes performed the best (Table 
                            <a class="tbl" href="#tab3">3</a>
                            ). Hence, we used the DDA trained with this architecture to minimize potential errors in the recognized output.
                        </p>
                        <div class="table-responsive" id="tab3">
                            <div class="table-caption">
                                <span class="table-number">Table 3:</span>
                                <span class="table-title">Evaluation of various DDA architectures in terms of word error rate (WER).</span>
                            </div>
                            <table class="table">
                                <thead>
                                    <tr>
                                        <th style="text-align:left;border-right: 2pt solid #000000;">
                                            <strong>DDA Architecture</strong>
                                        </th>
                                        <th>
                                            <strong>WER</strong>
                                        </th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:left;border-right: 2pt solid #000000;">Number of Layers: [ Number of Nodes]</td>
                                        <td>Mean (%)</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:left;border-right: 2pt solid #000000;">5: [256 128 64 128 256]</td>
                                        <td>21.8</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:left;border-right: 2pt solid #000000;">
                                            <strong>5: [128 64 32 64 128]</strong>
                                        </td>
                                        <td>
                                            <strong>16.4</strong>
                                        </td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:left;border-right: 2pt solid #000000;">3: [128 64 128]</td>
                                        <td>19.1</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:left;border-right: 2pt solid #000000;">3: [64 32 64]</td>
                                        <td>26.3</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </section>
                    <section id="sec-31">
                        <p>
                            <em>
                                <span class="section-number">5.4.3</span>
                                 Training and Evaluation of N-Gram Language Model.
                            </em>
                             We evaluated the directional advantage of a count-based n-gram LM with state-of-the-art bi-directional neural LM in terms of sentence error rate (SER)
                            <a class="fn" href="#fn10" id="foot-fn10">
                                <sup>10</sup>
                            </a>
                            , perplexity
                            <a class="fn" href="#fn11" id="foot-fn11">
                                <sup>11</sup>
                            </a>
                            , and computation time. For a fair comparison, we trained both models from scratch using the LIBRISPEECH dataset (Section 
                            <a class="sec" href="#sec-29">5.4.1</a>
                            ). We divided the dataset in a split of 80:20% as training: testing set. Count-based n-grams models were trained using the Natural Language Toolkit (NLTK)
                            <a class="fn" href="#fn12" id="foot-fn12">
                                <sup>12</sup>
                            </a>
                             with Kneser-Ney smoothing [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>
                            , 
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>
                            ] to better estimate probabilities of unseen n-grams. Bi-directional neural LM (Bi-LSTM) was trained using LSTM based recurrent units that have two recurrent layers with 4,096 LSTM nodes in each layer, an input projection layer of size 128, and an output softmax layer over vocabulary. The model was trained end-to-end using cross-entropy loss [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0108">108</a>
                            ] with Adam [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0063">63</a>
                            ] as the optimization method. The model was trained for 60 epochs with batch size of 64 and learning rate of 1
                            <em>e</em>
                            <sup>− 3</sup>
                            . It was implemented based on the Keras deep-learning platform with TensorFlow [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>
                            ] as the backend. Both LMs were trained and tested on NVIDIA GeForce 1080Ti GPU.
                        </p>
                        <div class="table-responsive" id="tab4">
                            <div class="table-caption">
                                <span class="table-number">Table 4:</span>
                                <span class="table-title">Comparison between forward, backward and combination of both (forward + backward) n-gram LM with Bi-LSTM LM. Reported sentence error rate (SER), perplexity, and computation time are average of all values. The proposed repair model uses the combined trigram model.</span>
                            </div>
                            <table class="table">
                                <tbody>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td colspan="7" style="text-align:center;">
                                            <strong>Sentence Error Rate (SER) %</strong>
                                        </td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td colspan="3" style="text-align:center;border-right: 2pt solid #000000;">Bigram</td>
                                        <td colspan="3" style="text-align:center;border-right: 2pt solid #000000;">Trigram</td>
                                        <td>Bi-LSTM</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Forward</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Backward</td>
                                        <td style="text-align:left;border-right: 2pt solid #000000;">Combined</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Forward</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Backward</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Combined</strong>
                                        </td>
                                        <td></td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">27.4</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">30.9</td>
                                        <td style="text-align:left;border-right: 2pt solid #000000;">26.7</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">24.4</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">27.6</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>16.5</strong>
                                        </td>
                                        <td>15.3</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td colspan="7" style="text-align:center;">
                                            <strong>Perplexity</strong>
                                        </td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td colspan="3" style="text-align:center;border-right: 2pt solid #000000;">Bigram</td>
                                        <td colspan="3" style="text-align:center;border-right: 2pt solid #000000;">Trigram</td>
                                        <td>Bi-LSTM</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Forward</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Backward</td>
                                        <td style="text-align:left;border-right: 2pt solid #000000;">Combined</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Forward</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Backward</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Combined</strong>
                                        </td>
                                        <td></td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">51.3</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">60.1</td>
                                        <td style="text-align:left;border-right: 2pt solid #000000;">48.7</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">44.1</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">48.3</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>41.4</strong>
                                        </td>
                                        <td>39.8</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td colspan="7" style="text-align:center;">
                                            <strong>Computation Time (Second)</strong>
                                        </td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td colspan="3" style="text-align:center;border-right: 2pt solid #000000;">Bigram</td>
                                        <td colspan="3" style="text-align:center;border-right: 2pt solid #000000;">Trigram</td>
                                        <td>Bi-LSTM</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Forward</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Backward</td>
                                        <td style="text-align:left;border-right: 2pt solid #000000;">Combined</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Forward</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">Backward</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Combined</strong>
                                        </td>
                                        <td></td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">1.8</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">1.7</td>
                                        <td style="text-align:left;border-right: 2pt solid #000000;">3.1</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">1.5</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">1.9</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>3.4</strong>
                                        </td>
                                        <td>9.2</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <p>
                            In the experiment, Bi-LSTM performed better than the count-based LMs in terms of SER and perplexity (Table 
                            <a class="tbl" href="#tab4">4</a>
                            ). However, it required extra computation time. Among count-based LMs, the combined trigram LM (forward and backward) performed much better. Besides, it yielded a 7.27% and 3.86% higher SER and perplexity, respectively, and a 5.8 seconds (∼ 170.5%) lower computation time than Bi-LSTM. Hence, considering the negligible percentage differences in SER and perplexity and a large difference in computation time, we decided to use the combination of forward and backward trigram LM in our repair model.
                        </p>
                    </section>
                    <section id="sec-32">
                        <p>
                            <em>
                                <span class="section-number">5.4.4</span>
                                 Selection of Best Combinations of 
                                <em>τ</em>
                                <sub>1</sub>
                                 and 
                                <em>τ</em>
                                <sub>2</sub>
                                 to Compute Numerical Similarity.
                            </em>
                             To select the best combinations of 
                            <em>τ</em>
                            <sub>1</sub>
                             and 
                            <em>τ</em>
                            <sub>2</sub>
                            , we evaluated the proposed LM for various combinations of 
                            <em>τ</em>
                            <sub>1</sub>
                             and 
                            <em>τ</em>
                            <sub>2</sub>
                            , in terms of true positive rate (TPR) and false positive rate (FPR), defined as: 
                        </p>
                        <div class="table-responsive" id="Xeq10">
                            <div class="display-equation">
                                <span class="tex mytex">\begin{equation} TPR = \frac{TP}{TP+FN} \quad \text{and}\quad FPR = \frac{FP}{FP+TN} \end{equation} </span>
                                <br/>
                                <span class="equation-number">(13)</span>
                            </div>
                        </div>
                        <em>TP</em>
                        : True positive is the total number of correct words identified as correct.
                               
                        <p></p>
                        <p>
                            <em>FP</em>
                            : False positive is the total number of incorrect words identified as correct.
                        </p>
                        <p>
                            <em>TN</em>
                            : True negative is the total number of incorrect words identified as incorrect.
                        </p>
                        <p>
                            <em>FN</em>
                            : False negative is the total number of correct words identified as incorrect.
                        </p>
                        <p>
                            Each curve in Fig. 
                            <a class="fig" href="#fig6">6</a>
                             signify TPR vs. FPR for different sets of 
                            <em>τ</em>
                            <sub>1</sub>
                             and 
                            <em>τ</em>
                            <sub>2</sub>
                            . It can be clearly seen that the LM with 
                            <em>τ</em>
                            <sub>1</sub>
                             = 0.7, 
                            <em>τ</em>
                            <sub>2</sub>
                             = 2 performed best among all cases since it has a much higher TPR and a lower FPR. 
                        </p>
                        <figure id="fig6">
                            <img src="https://dl.acm.org/cms/attachment/c61175fd-450f-4c18-bded-bd1808352a3b/chi21-522-fig6.jpg" class="img-responsive" alt="Performance evaluation of Language model" aria-describedby="fig007"/>
                            <p style="display:none" id="fig007">Curve plot showing True Positive Rate (TPR) on the Y-axis against False Positive Rate (FPR) on the X-axis for (τ1 = 0.6, τ2 = 1), (τ1 = 0.6, τ2 = 2), (τ1 = 0.6, τ2 = 3), (τ1 = 0.7, τ2 = 1), (τ1 = 0.7, τ2 = 2), (τ1 = 0.7, τ2 = 3), (τ1 = 0.8, τ2 = 1), (τ1 = 0.8, τ2 = 2), (τ1 = 0.8, τ2 = 3).</p>
                            <figcaption>
                                <span class="figure-number">Figure 6:</span>
                                <span class="figure-title">
                                    Performance comparison in terms of TPR and FPR of proposed LM for various values of 
                                    <em>τ</em>
                                    <sub>1</sub>
                                     and 
                                    <em>τ</em>
                                    <sub>2</sub>
                                </span>
                            </figcaption>
                        </figure>
                        <p></p>
                        <p>
                            To summarize, the post-processing step include: 5-layer DDA with bi-directional count-based trigram LM, followed by numerical similarity with 
                            <em>τ</em>
                            <sub>1</sub>
                             = 0.7, 
                            <em>τ</em>
                            <sub>2</sub>
                             = 2.
                        </p>
                    </section>
                </section>
            </section>
            <section id="sec-33">
                <header>
                    <div class="title-info">
                        <h2>
                            <span class="section-number">6</span>
                             EXPERIMENT 4: INDEPENDENCE OF THE REPAIR MODEL
                        </h2>
                    </div>
                </header>
                <p>Since our goal was to develop a repair model that can be used with a range of speech and silent speech recognizers, we evaluated its effectiveness with both LipType and several other popular speech and silent speech recognizers. Particularly, we picked the following six pre-trained models.</p>
                <section id="sec-34">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">6.1</span>
                                 Silent Speech Recognizers
                            </h3>
                        </div>
                    </header>
                    <ol class="list-no-style">
                        <li id="list11" label="(1)">
                            <strong>LipNet</strong>
                             [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>
                            ] model uses a neural network architecture for lip reading that maps variable-length sequences of video frames to text sequences, making use of deep 3-dimensional convolutions, a recurrent network, and the connectionist temporal classification loss [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0040">40</a>
                            ], trained entirely end-to-end. It was trained on the GRID dataset [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                            ] which comprises of short and formulaic videos that show a well-lit person's face while uttering a highly constrained vocabulary in a specific order.
                            <br/>
                        </li>
                        <li id="list12" label="(2)">
                            <strong>LipType</strong>
                             model follows the same architecture as LipNet except it replaces deep 3-dimensional convolutions with a combination of shallow 3-dimensional convolutions (1-layer) and deep 2-dimensional convolutions (34-layer ResNet) integrated with squeeze and excitation (SE) blocks (SE-ResNet). It was also trained on the GRID dataset.
                            <br/>
                        </li>
                        <li id="list13" label="(3)">
                            <strong>Transformer</strong>
                             [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                            ] model comprises of two sub-modules: a 
                            <em>spatio-temporal visual frontend</em>
                             that takes a sequence of video frames to extract one feature vector per frame and a 
                            <em>sequence processing backend</em>
                             comprised of encoder-decoder structure with multi-head attention layers [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0100">100</a>
                            ] that generates character probabilities over the vocabulary. It was trained on Lip Reading in the Wild (LRW) [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>
                            ] and the Lip Reading Sentences 2 (LRS2) [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                            ] datasets.
                            <br/>
                        </li>
                    </ol>
                </section>
                <section id="sec-35">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">6.2</span>
                                 Speech Recognizers
                            </h3>
                        </div>
                    </header>
                    <ol class="list-no-style">
                        <li id="list14" label="(1)">
                            <strong>DeepSpeech</strong>
                             [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0043">43</a>
                            ] is a speech recognition model developed using end-to-end training of a large recurrent neural network (RNN). It converts an input speech spectrograms into a sequence of character probabilities. It was trained on the Wall Street Journal (WSJ) [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0078">78</a>
                            ], Switchboard [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>
                            ], and Fisher [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>
                            ] datasets.
                            <br/>
                        </li>
                        <li id="list15" label="(2)">
                            <strong>Kaldi</strong>
                             [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0085">85</a>
                            ] is an open-source toolkit for speech recognition written in C++, which uses Finite State Transducer (OpenFST) library [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0087">87</a>
                            ] for training recognition models. It comprises of multiple speech recognition recipes. For our work, We used a pre-trained chain English model (Api.ai) recipe, trained on the LIBRISPEECH dataset [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                            ].
                            <br/>
                        </li>
                        <li id="list16" label="(3)">
                            <strong>Wave2Letter</strong>
                             [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>
                            ] is an end-to-end model for speech recognition, that combines a convolutional network-based acoustic model and a graph decoding. It is trained to output letters without the need for force aligning them. It was trained on the LIBRISPEECH [
                            <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                            ] dataset.
                            <br/>
                        </li>
                    </ol>
                    <p>
                        We evaluated these models on seen and unseen data. For seen data, we randomly selected 30 phrases from each model's training dataset, for unseen data, we randomly selected 30 phrases from MacKenzie and Soukoreff dataset [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0071">71</a>
                        ]. Unseen data was common for all models. All selected phrases are listed in the Appendix A.
                    </p>
                </section>
                <section id="sec-36">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">6.3</span>
                                 Experimental Conditions
                            </h3>
                        </div>
                    </header>
                    <p>We evaluated the silent speech models under three lighting conditions. Due to the spread of COVID-19, all conditions were simulated in a private room without any artificial light sources.</p>
                    <ul class="list-no-style">
                        <li id="list17" label="•">
                            <strong>Dark light</strong>
                            : video recorded during nighttime (9:00–11:00 PM).
                            <br/>
                        </li>
                        <li id="list18" label="•">
                            <strong>Dusky light</strong>
                            : video recorded during evening time (6:00–8:00 PM).
                            <br/>
                        </li>
                        <li id="list19" label="•">
                            <strong>Daylight</strong>
                            : video recorded during daytime (1:00–3:00 PM).
                            <br/>
                        </li>
                    </ul>
                    <p>Likewise, speech models were evaluated under three noisy conditions, simulated in a private room.</p>
                    <ul class="list-no-style">
                        <li id="list20" label="•">
                            <strong>Indoor noise</strong>
                            : audio recording with an indoor noise, simulated by playing a prerecorded CNN news report in the background.
                            <br/>
                        </li>
                        <li id="list21" label="•">
                            <strong>Outdoor noise</strong>
                            : audio recording in a public place, simulated by playing a prerecorded busy marketplace noise.
                            <br/>
                        </li>
                        <li id="list22" label="•">
                            <strong>Quiet</strong>
                            : audio recording in a quiet room.
                            <br/>
                        </li>
                    </ul>
                </section>
                <section id="sec-37">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">6.4</span>
                                 Apparatus
                            </h3>
                        </div>
                    </header>
                    <p>
                        We developed a custom Android application with Android Studio 3.1.4 for data collection. The application included a 
                        <em>landing</em>
                         page and a 
                        <em>data collection</em>
                         page. The landing page included a drop-down menu to select recording conditions and a Start button to start a session. The data collection page included a video viewer to display the device's front camera, an area to presented phrases, and a Record/Stop toggle button to start and stop recording. The application recorded all videos and automatically logged the duration of a session, device specification (display and camera resolution, etc.), light intensity, and sound level.
                    </p>
                </section>
                <section id="sec-38">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">6.5</span>
                                 Participants
                            </h3>
                        </div>
                    </header>
                    <p>
                        Twelve volunteers aged 19–-54 years (M = 27.9, SD = 9.15) took part in the study (Fig.&nbsp;
                        <a class="fig" href="#fig7">7</a>
                        ). They were all proficient in the English language. Five of them identified themselves as women and seven identified as men. They all had at least five years of experience with smartphones. All of them were Android-based smartphone users, and users of a voice assistant system for at least one year. Most of them had experience with multiple voice assistants, including Amazon Alexa, Google Assistant, and Apple Siri. They all received US 
                        <font style="normal">$</font>
                        20 for participating in the study. 
                    </p>
                    <figure id="fig7">
                        <img src="https://dl.acm.org/cms/attachment/a86b43bf-7add-4c1f-b1a6-17442b094119/chi21-522-fig7.jpg" class="img-responsive" alt="Study participants" aria-describedby="fig008"/>
                        <p style="display:none" id="fig008">Four images show user study participants recording silent speech data on the smartphone under varying lighting conditions. First image shows a female participant recording in dark light. Second image shows a male participant recording in dusky light. Third image shows a male participant recording in day light. Last image shows a male participant showing custom application in day light.</p>
                        <figcaption>
                            <span class="figure-number">Figure 7:</span>
                            <span class="figure-title">Four volunteers participating in the user study.</span>
                        </figcaption>
                    </figure>
                    <p></p>
                </section>
                <section id="sec-39">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">6.6</span>
                                 Design
                            </h3>
                        </div>
                    </header>
                    <p>We used the following within-subjects design for the study:</p>
                    <ul class="list-no-style">
                        <li id="uid81">
                            12 participants &times;
                            <br/>
                        </li>
                        <li id="uid82">
                            2 methods (speech, silent speech) &times;
                            <br/>
                        </li>
                        <li id="uid83">
                            3 conditions (indoor, outdoor, quiet / dark, dusky, day), counterbalanced &times;
                            <br/>
                        </li>
                        <li id="uid84">
                            2 data types (seen, unseen) &times;
                            <br/>
                        </li>
                        <li id="uid85">
                            3 models (DeepSpeech, Kaldi, Wave2Letter / LipNet, LipType, Transformer), counterbalanced &times;
                            <br/>
                        </li>
                        <li id="uid86">
                            30 phrases = 12,960 phrases in total.
                            <br/>
                        </li>
                    </ul>
                </section>
                <section id="sec-40">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">6.7</span>
                                 Procedure
                            </h3>
                        </div>
                    </header>
                    <p>
                        The study was conducted remotely due to the spread of COVID-19. We explained the purpose of the study and scheduled individual Zoom
                        <a class="fn" href="#fn13" id="foot-fn13">
                            <sup>13</sup>
                        </a>
                         video calls with each participant ahead of time. We instructed them to join the call from a quiet room to avoid any interruptions during the study. In the first call, we demonstrated the application and collected their consents and demographics using electronic forms. We then shared the application (APK file) with them and guided them through the installation process on their smartphones. The first session started shortly after that. The application displayed one phrase at a time. Participants pressed the Record button, spoke or silently spoke
                        <a class="fn" href="#fn14" id="foot-fn14">
                            <sup>14</sup>
                        </a>
                         the phrase, then pressed the Stop button to see the next phrase. In the noisy conditions (Section&nbsp;
                        <a class="sec" href="#sec-36">6.3</a>
                        ), we shared the respective audio clips with the participants and instructed them to play the clips slightly louder than a normal conversation. Log analysis reveled that, on average, participants played the indoor noise at 48.75 db (min = 42 db, max = 58 db) and outdoor noise at 55.25 db (min = 49 db, max = 66 db). To simulate different lighting conditions, silent speech sessions were scheduled at different times of the day. Log analysis revealed that, on average, room light intensity was 0.93 lux (min = 0 lux, max = 2 lux) in the dark light condition, 7.86 lux (min = 6 lux, max = 11 lux) in the dusky light condition, and 58.0 lux (min = 52 lux, max = 61 lux) in the daylight condition. All sessions followed the same format, expect for demonstration and installation. Upon completion of each session, participants shared the logged data with us by uploading those to a cloud storage under our supervision. In total, there were 24 recording sessions (Table&nbsp;
                        <a class="tbl" href="#tab5">5</a>
                        ). A researcher monitored all sessions via Zoom.
                    </p>
                    <div class="table-responsive" id="tab5">
                        <div class="table-caption">
                            <span class="table-number">Table 5:</span>
                            <span class="table-title">Recording sessions for different noisy and lighting conditions with their corresponding recognition models and datasets.</span>
                        </div>
                        <table class="table">
                            <thead>
                                <tr>
                                    <th style="text-align:center;" colspan="4">
                                        <strong>Speech</strong>
                                    </th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">
                                        <strong>Session</strong>
                                    </td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">
                                        <strong>Condition</strong>
                                    </td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">
                                        <strong>Model</strong>
                                    </td>
                                    <td>
                                        <strong>Dataset</strong>
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">1</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Indoor</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">DeepSpeech</td>
                                    <td>
                                        Fisher [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">2</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Outdoor</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">DeepSpeech</td>
                                    <td>
                                        Fisher [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">3</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Quiet</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">DeepSpeech</td>
                                    <td>
                                        Fisher [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">4</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Indoor</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Kaldi</td>
                                    <td>
                                        LIBRISPEECH [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">5</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Outdoor</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Kaldi</td>
                                    <td>
                                        LIBRISPEECH [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">6</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Quiet</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Kaldi</td>
                                    <td>
                                        LIBRISPEECH [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">7</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Indoor</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Wave2Letter</td>
                                    <td>
                                        LIBRISPEECH [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">8</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Outdoor</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Wave2Letter</td>
                                    <td>
                                        LIBRISPEECH [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">9</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Quiet</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Wave2Letter</td>
                                    <td>
                                        LIBRISPEECH [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">10</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Indoor</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">DeepSpeech/Kaldi/Wave2Letter</td>
                                    <td>
                                        Mackenzie and Soukoreff [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0071">71</a>
                                        ] (unseen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">11</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Outdoor</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">DeepSpeech/Kaldi/Wave2Letter</td>
                                    <td>
                                        Mackenzie and Soukoreff [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0071">71</a>
                                        ] (unseen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">12</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Quiet</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">DeepSpeech/Kaldi/Wave2Letter</td>
                                    <td>
                                        Mackenzie and Soukoreff [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0071">71</a>
                                        ] (unseen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td colspan="4" style="text-align:center;">
                                        <strong>Silent Speech</strong>
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">13</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Dark</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">LipNet</td>
                                    <td>
                                        Grid [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">14</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Dusky</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">LipNet</td>
                                    <td>
                                        Grid [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">15</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Day</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">LipNet</td>
                                    <td>
                                        Grid [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">16</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Dark</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">LipType</td>
                                    <td>
                                        Grid [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">17</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Dusky</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">LipType</td>
                                    <td>
                                        Grid [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">18</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Day</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">LipType</td>
                                    <td>
                                        Grid [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">19</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Dark</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Transformer</td>
                                    <td>
                                        LRS [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">20</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Dusky</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Transformer</td>
                                    <td>
                                        LRS [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">21</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Day</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Transformer</td>
                                    <td>
                                        LRS [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                                        ] (seen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">22</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Dark</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">LipNet/Transformer/LipType</td>
                                    <td>
                                        Mackenzie and Soukoreff [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0071">71</a>
                                        ] (unseen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">23</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Dusky</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">LipNet/Transformer/LipType</td>
                                    <td>
                                        Mackenzie and Soukoreff [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0071">71</a>
                                        ] (unseen)
                                    </td>
                                </tr>
                                <tr style="border-bottom: 2pt solid #000000;">
                                    <td style="text-align:center;border-right: 2pt solid #000000;">24</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">Day</td>
                                    <td style="text-align:center;border-right: 2pt solid #000000;">LipNet/Transformer/LipType</td>
                                    <td>
                                        Mackenzie and Soukoreff [
                                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0071">71</a>
                                        ] (unseen)
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <p>Upon completion of the study, we evaluated the repair model with the six recognition models using the collected audio and video clips. For speech, first, we passed the recorded audio to a speech recognizer, then we post-processed the output to auto-correct errors. We did not pre-process the data since speech only utilizes audio information, thus, is not affected by poor lighting conditions. For silent speech, first, we processed each recorded video with the pre-processing technique to enhance the lighting of the clips, then we passed the processed videos to a silent speech recognizer, finally we post-processed the output to auto-correct errors.</p>
                </section>
                <section id="sec-41">
                    <header>
                        <div class="title-info">
                            <h3>
                                <span class="section-number">6.8</span>
                                 Results
                            </h3>
                        </div>
                    </header>
                    <p>For evaluation, we considered all pre-trained models as baselines and compared with their respective repaired versions in terms of WER, WPM, and CT. To ensure a fair comparison of computation time, we evaluated all models on NVIDIA GeForce 1080Ti GPU board. Results revealed that the proposed repair model significantly reduce error rates of all pre-trained models regardless of data type and experimental conditions. </p>
                    <figure id="fig8">
                        <img src="https://dl.acm.org/cms/attachment/9eacf4f0-6be9-429a-b53b-45825c722a05/chi21-522-fig8.jpg" class="img-responsive" alt="Performance evaluation of repaired speech recognition models" aria-describedby="fig009"/>
                        <p style="display:none" id="fig009">Six barplots labeled (a), (b), (c), (d), (e), and (f). Plot (a) shows mean Word Error Rate (WER) for seen data from 0 to 45 on the Y-axis against indoor, outdoor, and quiet on the X-axis per category for Deepspeech, repaired Deepspeech, Kaldi, repaired Kaldi, Wave2letter, and repaired Wave2letter. Plot (b) shows mean Word Error Rate (WER) for unseen data from 0 to 60 on the Y-axis against indoor, outdoor, and quiet on the X-axis per category for Deepspeech, repaired Deepspeech, Kaldi, repaired Kaldi, Wave2letter, and repaired Wave2letter. Plot (c) shows mean Words per Minute (WPM) for seen data from 0 to 10 on the Y-axis against indoor, outdoor, and quiet on the X-axis per category for Deepspeech, repaired Deepspeech, Kaldi, repaired Kaldi, Wave2letter, and repaired Wave2letter. Plot (d) shows mean Words per Minute (WPM) for unseen data from 0 to 10 on the Y-axis against indoor, outdoor, and quiet on the X-axis per category for Deepspeech, repaired Deepspeech, Kaldi, repaired Kaldi, Wave2letter, and repaired Wave2letter. Plot (e) shows mean Computation Time (CT) for seen data from 0 to 25 on the Y-axis against indoor, outdoor, and quiet on the X-axis per category for Deepspeech, repaired Deepspeech, Kaldi, repaired Kaldi, Wave2letter, and repaired Wave2letter. Plot (f) shows mean Computation Time (CT) for unseen data from 0 to 25 on the Y-axis against indoor, outdoor, and quiet on the X-axis per category for Deepspeech, repaired Deepspeech, Kaldi, repaired Kaldi, Wave2letter, and repaired Wave2letter.</p>
                        <figcaption>
                            <span class="figure-number">Figure 8:</span>
                            <span class="figure-title">Performance evaluation of the three investigated speech recognition models without/with the proposed repair model in terms of a) WER-Seen, b) WER-Unseen, c) WPM-Seen, d) WPM-Unseen, e) CT-Seen, and f) CT-Unseen. Each condition has 360 data points. Reported values are the average of all values. The values inside the brackets are standard deviations (SD). Error bars represent &plusmn; 1 SD.</span>
                        </figcaption>
                    </figure>
                    <p></p>
                    <p>
                        Fig.&nbsp; 
                        <a class="fig" href="#fig8">8</a>
                         shows the effectiveness of repair model on the three examined speech recognition models. It can be clearly observed that the repair model resulted in substantial reductions in error rates for all pre-trained models under all noisy conditions. With DeepSpeech, it showed 37.5% reduction in WER for seen data and 26.7% reduction for unseen data. With Kaldi, it showed 31.5% reduction in WER for seen data and 38% reduction for unseen data. With Wave2Letter, it showed 26.8% reduction in WER for seen data and 38.3% reduction for unseen data. On average, for all models, we observed 8.4% reduction in WPM and 5.9 seconds increase in CT on both seen and unseen data. Overall, Repaired Kaldi performed the best among all pre-trained models. 
                    </p>
                    <figure id="fig9">
                        <img src="https://dl.acm.org/cms/attachment/8958a1f9-81ae-4ea3-85a2-97ff547af2a2/chi21-522-fig9.jpg" class="img-responsive" alt="Performance evaluation of repaired silent speech recognition models" aria-describedby="fig010"/>
                        <p style="display:none" id="fig010">Six barplots labeled (a), (b), (c), (d), (e), and (f). Plot (a) shows mean Word Error Rate (WER) for seen data from 0 to 120 on the Y-axis against dark, dusky, and day on the X-axis per category for LipNet, repaired LipNet, LipType, repaired LipType, Transformer, and repaired Transformer. Plot (b) shows mean Word Error Rate (WER) for unseen data from 0 to 120 on the Y-axis against dark, dusky, and day on the X-axis per category for LipNet, repaired LipNet, LipType, repaired LipType, Transformer, and repaired Transformer. Plot (c) shows mean Words per Minute (WPM) for seen data from 0 to 8 on the Y-axis against dark, dusky, and day on the X-axis per category for LipNet, repaired LipNet, LipType, repaired LipType, Transformer, and repaired Transformer. Plot (d) shows mean Words per Minute (WPM) for unseen data from 0 to 8 on the Y-axis against dark, dusky, and day on the X-axis per category for LipNet, repaired LipNet, LipType, repaired LipType, Transformer, and repaired Transformer. Plot (e) shows mean Computation Time (CT) for seen data from 0 to 25 on the Y-axis against dark, dusky, and day on the X-axis per category for LipNet, repaired LipNet, LipType, repaired LipType, Transformer, and repaired Transformer. Plot (f) shows mean Computation Time (CT) for unseen data from 0 to 25 on the Y-axis against dark, dusky, and day on the X-axis per category for LipNet, repaired LipNet, LipType, repaired LipType, Transformer, and repaired Transformer.</p>
                        <figcaption>
                            <span class="figure-number">Figure 9:</span>
                            <span class="figure-title">Performance evaluation of the three examined silent speech recognition models without/with the proposed repair model in terms of a) WER-Seen, b) WER-Unseen, c) WPM-Seen, d) WPM-Unseen, e) CT-Seen, and f) CT-Unseen. Each condition has 360 data points. Reported values are the average of all values. The values inside the brackets are standard deviations (SD). Error bars represent &plusmn; 1 SD.</span>
                        </figcaption>
                    </figure>
                    <p></p>
                    <p>
                        Fig.&nbsp; 
                        <a class="fig" href="#fig9">9</a>
                         shows the effectiveness of the repair model on silent speech recognition models. The performance of the repair model followed a similar trend as the speech models. It showed substantial reductions in error rates for all lighting conditions. With LipNet, it showed 58.1% reduction in WER for seen data and 15.5% reduction for unseen data. With LipType, it showed 61.9% reduction in WER for seen data and 16.3% reduction for unseen data. With Transformer, it showed 51.5% reduction in WER for seen data and 38.5% reduction for unseen data. On average, for all models, we observed 10.9% reduction in WPM and 8 seconds increase in CT on both seen and unseen data. For unseen data, we observed a negligible reduction in WER for LipNet and LipType compared to the Transformer model. We speculate that this is because LipNet and LipType are trained on a relatively small GRID dataset [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                        ] that has a smaller number of word-level classes (shorter phrases). This resulted in a much better performance for their repair models with seen data as most of the silently spoken words were in its vocabulary. Likewise, it did not perform as well with unseen data as many of the silently spoken words were not in its vocabulary (thus could not be fully processed by the language model). Transformer, in contrast, is trained on LRS dataset [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                        ] that has a larger number of word-level classes (longer phrases). This resulted in a much lower WER for repaired Transformer with unseen data as it provided the language model with more accurate words than LipType. Note that the language model is part of the repair model not the recognizer. It is trained on a more comprehensive LIBRISPEECH dataset [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                        ]. But its effectiveness is reliant on the vocabulary of the recognizer.
                    </p>
                    <p>
                        We also performed extensive ablation studies on each submodule of our model to demonstrate their contribution to the overall performance gains. All results are detailed in Appendix 
                        <a class="sec" href="#sec-52">B</a>
                        .
                    </p>
                </section>
            </section>
            <section id="sec-42">
                <header>
                    <div class="title-info">
                        <h2>
                            <span class="section-number">7</span>
                             DISCUSSION
                        </h2>
                    </div>
                </header>
                <p>We developed LipType, an optimized version of LipNet for improved speed and accuracy. LipType demonstrated a significant improvement in the performance of LipNet. Results revealed 46.9% reduction in WER, 39.1% increase in WPM, and 8.6 seconds reduction in CT. We then developed an independent repair model that processes video input for poor lighting conditions and corrects potential errors in output for increased accuracy. We evaluated the repair model's effectiveness with various speech and silent speech recognizers. To demonstrate its benefit, we selected six pre-trained models, i.e., three for speech and three for silent speech. We then conducted a user study with twelve participants to collect diverse data under real-world conditions. For speech models, we collected data in indoor, outdoor, and quiet noisy conditions. For silent speech, we collected data in dark, dusky, and day lighting conditions. We then evaluated the impact of the repair model on each model's performance using the collected data. Results showed significant improvement in the performance of all models. Models augmented with the repair model outperformed the original models drastically for all experimental conditions. For speech, we observed 32% reduction in WER, 5.8 seconds increase in CT, and 8.1% reduction in WPM; whereas for silent speech, we observed 57.2% reduction in WER, 7.9 seconds increase in CT, and 10.3% reduction in WPM. Since speech models do not involve preprocessing, their repaired models showed 26.2% less CT than silent speech models.</p>
                <p>
                    On comparing the performance of LipNet and LipType from Fig. 
                    <a class="fig" href="#fig3">3</a>
                     and Fig. 
                    <a class="fig" href="#fig9">9</a>
                    (a):Day, we observed a 45-50% reduction in their WER. We speculate that this is because the dataset used to evaluate both models for seen speakers comprises of uniform visual attributes (same skin tone, accent, pace of speech, etc.) (Fig. 
                    <a class="fig" href="#fig3">3</a>
                    ). However, the dataset for final evaluation used new speakers’ data that solicited more variability in terms of speaker characteristics (Fig. 
                    <a class="fig" href="#fig9">9</a>
                    (a):Day). We also observed that the repaired Transformer performed much better than the other silent speech models on unseen data. We speculate that this is because Transformer is trained on LRS dataset that has a larger number of word-level classes (longer phrases). This resulted in a much lower WER for repaired Transformer with unseen data as it provided the language model with more accurate words than LipType.
                </p>
                <p>Overall, empirical results exhibit the effectiveness of repair model on all recognition models for improving accuracy with a slight increase in CT. These findings show the potential of the developed framework as a medium for communication with various computer systems, incorporated in day-to-day usage. This approach could also enable people with speech disorder, muteness, and blindness to input and interact with computer systems, increasing their access to technologies. We also envision the potential of this framework on other platforms like head-mounted displays (HMDs) and smart eyewear.</p>
            </section>
            <section id="sec-43">
                <header>
                    <div class="title-info">
                        <h2>
                            <span class="section-number">8</span>
                             CONCLUSION
                        </h2>
                    </div>
                </header>
                <p>We developed LipType, an optimized version of LipNet for improved speed and accuracy. We then developed an independent repair model that compensates for poor lighting conditions and corrects potential errors in output using a custom language model. We evaluated the repair model's effectiveness with both LipType and other speech and silent speech recognizers. Empirical results showed that it significantly reduces error rates for all recognizers. The findings confirm that the model can be used independently with a range of recognizers. In the future, we will extend this work to further optimize the algorithm to make it faster and adapt it for people with various speech disorders.</p>
            </section>
        </section>
        <section class="back-matter">
            <appendix>
                <section id="sec-44">
                    <header>
                        <div class="title-info">
                            <h2>
                                <span class="section-number">A</span>
                                 TEST DATASET: PRETRAINED MODEL
                            </h2>
                        </div>
                    </header>
                    <p>
                        In this appendix, we provide details about the selected phrases for seen and unseen data. For seen data, we randomly selected 30 phrases from each pretrained models’ training dataset. For unseen data, we randomly selected 30 phrases from MacKenzie and Soukoreff [
                        <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0071">71</a>
                        ] dataset, which is common for all models.
                    </p>
                    <section id="sec-45">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">A.1</span>
                                     LipNet: Seen data (Grid data [
                                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                                    ])
                                </h3>
                            </div>
                        </header>
                        <ol class="list-no-style">
                            <li id="list23" label="(1)">
                                bin blue at c one again
                                <br/>
                            </li>
                            <li id="list24" label="(2)">
                                set blue in f four soon
                                <br/>
                            </li>
                            <li id="list25" label="(3)">
                                set blue with l eight now
                                <br/>
                            </li>
                            <li id="list26" label="(4)">
                                bin green by a four soon
                                <br/>
                            </li>
                            <li id="list27" label="(5)">
                                place blue by t nine soon
                                <br/>
                            </li>
                            <li id="list28" label="(6)">
                                place red with a four please
                                <br/>
                            </li>
                            <li id="list29" label="(7)">
                                place green by p five again
                                <br/>
                            </li>
                            <li id="list30" label="(8)">
                                lay red with k seven soon
                                <br/>
                            </li>
                            <li id="list31" label="(9)">
                                set blue in g four now
                                <br/>
                            </li>
                            <li id="list32" label="(10)">
                                set red with m zero soon
                                <br/>
                            </li>
                            <li id="list33" label="(11)">
                                bin white at q six soon
                                <br/>
                            </li>
                            <li id="list34" label="(12)">
                                place blue at n six now
                                <br/>
                            </li>
                            <li id="list35" label="(13)">
                                bin white with f seven soon
                                <br/>
                            </li>
                            <li id="list36" label="(14)">
                                place blue at m seven now
                                <br/>
                            </li>
                            <li id="list37" label="(15)">
                                bin red by j five please
                                <br/>
                            </li>
                            <li id="list38" label="(16)">
                                bin white at a one please
                                <br/>
                            </li>
                            <li id="list39" label="(17)">
                                set red by b one now
                                <br/>
                            </li>
                            <li id="list40" label="(18)">
                                place blue at i one soon
                                <br/>
                            </li>
                            <li id="list41" label="(19)">
                                place blue in n two please
                                <br/>
                            </li>
                            <li id="list42" label="(20)">
                                lay red by d seven please
                                <br/>
                            </li>
                            <li id="list43" label="(21)">
                                bin white by z three now
                                <br/>
                            </li>
                            <li id="list44" label="(22)">
                                place white with e three again
                                <br/>
                            </li>
                            <li id="list45" label="(23)">
                                bin red in j four now
                                <br/>
                            </li>
                            <li id="list46" label="(24)">
                                set blue in e four again
                                <br/>
                            </li>
                            <li id="list47" label="(25)">
                                lay green at p four again
                                <br/>
                            </li>
                            <li id="list48" label="(26)">
                                bin red with z eight please
                                <br/>
                            </li>
                            <li id="list49" label="(27)">
                                place red with n two please
                                <br/>
                            </li>
                            <li id="list50" label="(28)">
                                lay blue with b two please
                                <br/>
                            </li>
                            <li id="list51" label="(29)">
                                set green with v eight now
                                <br/>
                            </li>
                            <li id="list52" label="(30)">
                                bin white at j nine now
                                <br/>
                            </li>
                        </ol>
                    </section>
                    <section id="sec-46">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">A.2</span>
                                     LipType: Seen data (Grid data [
                                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>
                                    ])
                                </h3>
                            </div>
                        </header>
                        <ol class="list-no-style">
                            <li id="list53" label="(1)">
                                set green at f four soon
                                <br/>
                            </li>
                            <li id="list54" label="(2)">
                                bin green by h zero please
                                <br/>
                            </li>
                            <li id="list55" label="(3)">
                                bin white at f zero again
                                <br/>
                            </li>
                            <li id="list56" label="(4)">
                                place blue with j five again
                                <br/>
                            </li>
                            <li id="list57" label="(5)">
                                place white in g two please
                                <br/>
                            </li>
                            <li id="list58" label="(6)">
                                bin white by d eight again
                                <br/>
                            </li>
                            <li id="list59" label="(7)">
                                bin blue in q seven please
                                <br/>
                            </li>
                            <li id="list60" label="(8)">
                                lay red with f zero again
                                <br/>
                            </li>
                            <li id="list61" label="(9)">
                                place white at p eight now
                                <br/>
                            </li>
                            <li id="list62" label="(10)">
                                lay red with k three now
                                <br/>
                            </li>
                            <li id="list63" label="(11)">
                                lay red in j one soon
                                <br/>
                            </li>
                            <li id="list64" label="(12)">
                                lay white at j nine soon
                                <br/>
                            </li>
                            <li id="list65" label="(13)">
                                lay red at v eight again
                                <br/>
                            </li>
                            <li id="list66" label="(14)">
                                place green in u zero now
                                <br/>
                            </li>
                            <li id="list67" label="(15)">
                                lay red with c eight again
                                <br/>
                            </li>
                            <li id="list68" label="(16)">
                                place green at u two now
                                <br/>
                            </li>
                            <li id="list69" label="(17)">
                                place white by v four now
                                <br/>
                            </li>
                            <li id="list70" label="(18)">
                                bin red in x one now
                                <br/>
                            </li>
                            <li id="list71" label="(19)">
                                bin green at e zero again
                                <br/>
                            </li>
                            <li id="list72" label="(20)">
                                lay white by p six please
                                <br/>
                            </li>
                            <li id="list73" label="(21)">
                                bin red with x nine again
                                <br/>
                            </li>
                            <li id="list74" label="(22)">
                                place red at c three now
                                <br/>
                            </li>
                            <li id="list75" label="(23)">
                                set green at o seven please
                                <br/>
                            </li>
                            <li id="list76" label="(24)">
                                bin red at s eight again
                                <br/>
                            </li>
                            <li id="list77" label="(25)">
                                place red in s three please
                                <br/>
                            </li>
                            <li id="list78" label="(26)">
                                bin green by n four again
                                <br/>
                            </li>
                            <li id="list79" label="(27)">
                                place green by y two please
                                <br/>
                            </li>
                            <li id="list80" label="(28)">
                                place green by k one please
                                <br/>
                            </li>
                            <li id="list81" label="(29)">
                                lay blue at c one please
                                <br/>
                            </li>
                            <li id="list82" label="(30)">
                                place red by n one please
                                <br/>
                            </li>
                        </ol>
                    </section>
                    <section id="sec-47">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">A.3</span>
                                     Transformer: Seen data (LRS data [
                                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>
                                    ])
                                </h3>
                            </div>
                        </header>
                        <ol class="list-no-style">
                            <li id="list83" label="(1)">
                                the whole gardens are extraordinary and
                                <br/>
                            </li>
                            <li id="list84" label="(2)">
                                like hundreds of thousands of people do every year
                                <br/>
                            </li>
                            <li id="list85" label="(3)">
                                but now there is more protection
                                <br/>
                            </li>
                            <li id="list86" label="(4)">
                                we have a lot less atmosphere above us
                                <br/>
                            </li>
                            <li id="list87" label="(5)">
                                and a couple of weeks ago
                                <br/>
                            </li>
                            <li id="list88" label="(6)">
                                enjoy the summer
                                <br/>
                            </li>
                            <li id="list89" label="(7)">
                                are they relatives of yours
                                <br/>
                            </li>
                            <li id="list90" label="(8)">
                                no longer dependent on the sun
                                <br/>
                            </li>
                            <li id="list91" label="(9)">
                                but the waldorf astoria
                                <br/>
                            </li>
                            <li id="list92" label="(10)">
                                they would be able to go back
                                <br/>
                            </li>
                            <li id="list93" label="(11)">
                                not just a hotel
                                <br/>
                            </li>
                            <li id="list94" label="(12)">
                                not just in this town
                                <br/>
                            </li>
                            <li id="list95" label="(13)">
                                every september this place would be transformed into what
                                <br/>
                            </li>
                            <li id="list96" label="(14)">
                                now they are gathering
                                <br/>
                            </li>
                            <li id="list97" label="(15)">
                                so from his vantage point
                                <br/>
                            </li>
                            <li id="list98" label="(16)">
                                there is no air so there is no sound
                                <br/>
                            </li>
                            <li id="list99" label="(17)">
                                with one of the rooms upstairs
                                <br/>
                            </li>
                            <li id="list100" label="(18)">
                                maybe more of steel and iron
                                <br/>
                            </li>
                            <li id="list101" label="(19)">
                                we have run out of time
                                <br/>
                            </li>
                            <li id="list102" label="(20)">
                                before we all get too excited about that prospect
                                <br/>
                            </li>
                            <li id="list103" label="(21)">
                                it can be quite expensive
                                <br/>
                            </li>
                            <li id="list104" label="(22)">
                                in the form of a dessert plate
                                <br/>
                            </li>
                            <li id="list105" label="(23)">
                                on the face of it
                                <br/>
                            </li>
                            <li id="list106" label="(24)">
                                it could be your passport to a small fortune
                                <br/>
                            </li>
                            <li id="list107" label="(25)">
                                some issues with potential damp
                                <br/>
                            </li>
                            <li id="list108" label="(26)">
                                a great place for him to be
                                <br/>
                            </li>
                            <li id="list109" label="(27)">
                                we have to pay for that
                                <br/>
                            </li>
                            <li id="list110" label="(28)">
                                so rather than just relying on this information
                                <br/>
                            </li>
                            <li id="list111" label="(29)">
                                all of the brain is combining all the different senses
                                <br/>
                            </li>
                            <li id="list112" label="(30)">
                                he ordered them back inside
                                <br/>
                            </li>
                        </ol>
                    </section>
                    <section id="sec-48">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">A.4</span>
                                     DeepSpeech: Seen data (Fisher English-conversational [
                                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>
                                    ])
                                </h3>
                            </div>
                        </header>
                        <ol class="list-no-style">
                            <li id="list113" label="(1)">
                                can you hear me okay by the way
                                <br/>
                            </li>
                            <li id="list114" label="(2)">
                                oh good as long as you can hear me
                                <br/>
                            </li>
                            <li id="list115" label="(3)">
                                yeah i can hear you
                                <br/>
                            </li>
                            <li id="list116" label="(4)">
                                yeah that would be interesting
                                <br/>
                            </li>
                            <li id="list117" label="(5)">
                                like ten minutes with a head set on i might as well exercise
                                <br/>
                            </li>
                            <li id="list118" label="(6)">
                                yeah thats great
                                <br/>
                            </li>
                            <li id="list119" label="(7)">
                                listening to the music anyway so um
                                <br/>
                            </li>
                            <li id="list120" label="(8)">
                                i actually think its actually going out
                                <br/>
                            </li>
                            <li id="list121" label="(9)">
                                fifth wheel dating show
                                <br/>
                            </li>
                            <li id="list122" label="(10)">
                                i also watch that show the fifth wheel third and fourth wheel
                                <br/>
                            </li>
                            <li id="list123" label="(11)">
                                and i have seen i remember when survivor first started
                                <br/>
                            </li>
                            <li id="list124" label="(12)">
                                i saw that like a couple things
                                <br/>
                            </li>
                            <li id="list125" label="(13)">
                                cause my roommate where watching it
                                <br/>
                            </li>
                            <li id="list126" label="(14)">
                                yeah my roommates are you in college too
                                <br/>
                            </li>
                            <li id="list127" label="(15)">
                                i am in graduate school
                                <br/>
                            </li>
                            <li id="list128" label="(16)">
                                oh yeah okay i just graduated from um
                                <br/>
                            </li>
                            <li id="list129" label="(17)">
                                first time graduate last year
                                <br/>
                            </li>
                            <li id="list130" label="(18)">
                                and how about what school are you in
                                <br/>
                            </li>
                            <li id="list131" label="(19)">
                                that was great performance tonight
                                <br/>
                            </li>
                            <li id="list132" label="(20)">
                                it would be it would be cool to be on it
                                <br/>
                            </li>
                            <li id="list133" label="(21)">
                                thats very cool
                                <br/>
                            </li>
                            <li id="list134" label="(22)">
                                popular everyone talks about it
                                <br/>
                            </li>
                            <li id="list135" label="(23)">
                                somebody from my high school one something too
                                <br/>
                            </li>
                            <li id="list136" label="(24)">
                                he won he was like on that
                                <br/>
                            </li>
                            <li id="list137" label="(25)">
                                greatest bachelor show
                                <br/>
                            </li>
                            <li id="list138" label="(26)">
                                it was before these millionaire the millionaire guy ones
                                <br/>
                            </li>
                            <li id="list139" label="(27)">
                                it was like a pageant for men
                                <br/>
                            </li>
                            <li id="list140" label="(28)">
                                i didnt see it but i think i know what you were talking about
                                <br/>
                            </li>
                            <li id="list141" label="(29)">
                                yeah he was in my old high school
                                <br/>
                            </li>
                            <li id="list142" label="(30)">
                                going to rat on the other one
                                <br/>
                            </li>
                        </ol>
                    </section>
                    <section id="sec-49">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">A.5</span>
                                     Kaldi: Seen data (LIBRISPEECH audiobooks [
                                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                                    ])
                                </h3>
                            </div>
                        </header>
                        <ol class="list-no-style">
                            <li id="list143" label="(1)">
                                he was in a mood for music was he not
                                <br/>
                            </li>
                            <li id="list144" label="(2)">
                                give not so earnest a mind to these mummeries child
                                <br/>
                            </li>
                            <li id="list145" label="(3)">
                                a golden fortune and a happy life
                                <br/>
                            </li>
                            <li id="list146" label="(4)">
                                he was like my father in a way and yet was not my father
                                <br/>
                            </li>
                            <li id="list147" label="(5)">
                                also there was a stripling page who turned into a maid
                                <br/>
                            </li>
                            <li id="list148" label="(6)">
                                this was so sweet a lady sir and in some manner i do think
                                <br/>
                            </li>
                            <li id="list149" label="(7)">
                                but then the picture was gone as quickly as it came
                                <br/>
                            </li>
                            <li id="list150" label="(8)">
                                sister nell do you hear these marvels
                                <br/>
                            </li>
                            <li id="list151" label="(9)">
                                take your place and let us see what the crystal can show you
                                <br/>
                            </li>
                            <li id="list152" label="(10)">
                                like as not young master though i am an old man
                                <br/>
                            </li>
                            <li id="list153" label="(11)">
                                he was going home after victory
                                <br/>
                            </li>
                            <li id="list154" label="(12)">
                                it was almost buried now in flowers and foliage
                                <br/>
                            </li>
                            <li id="list155" label="(13)">
                                But I wrestled with this fellow
                                <br/>
                            </li>
                            <li id="list156" label="(14)">
                                but he saw nothing that moved no signal lights twinkled
                                <br/>
                            </li>
                            <li id="list157" label="(15)">
                                and why should that disturb me let him enter
                                <br/>
                            </li>
                            <li id="list158" label="(16)">
                                there was not a single note of gloom
                                <br/>
                            </li>
                            <li id="list159" label="(17)">
                                boats put out both from the fort and the shore
                                <br/>
                            </li>
                            <li id="list160" label="(18)">
                                his excellency madam the prefect
                                <br/>
                            </li>
                            <li id="list161" label="(19)">
                                so i did push this fellow
                                <br/>
                            </li>
                            <li id="list162" label="(20)">
                                what do i care for food
                                <br/>
                            </li>
                            <li id="list163" label="(21)">
                                shame on you citizens cried he i blush for my fellows
                                <br/>
                            </li>
                            <li id="list164" label="(22)">
                                surely we can submit with good grace
                                <br/>
                            </li>
                            <li id="list165" label="(23)">
                                fine for you to talk old man answered the lean
                                <br/>
                            </li>
                            <li id="list166" label="(24)">
                                at the same time every avenue of the throne was assaulted
                                <br/>
                            </li>
                            <li id="list167" label="(25)">
                                vintage years have much to do with the quality of wines
                                <br/>
                            </li>
                            <li id="list168" label="(26)">
                                come to me men here here he raised his voice still louder
                                <br/>
                            </li>
                            <li id="list169" label="(27)">
                                dry and of magnificent bouquet
                                <br/>
                            </li>
                            <li id="list170" label="(28)">
                                pour mayonnaise over all chill and serve
                                <br/>
                            </li>
                            <li id="list171" label="(29)">
                                set into a cold place to chill and become firm
                                <br/>
                            </li>
                            <li id="list172" label="(30)">
                                when thickened strain and cool
                                <br/>
                            </li>
                        </ol>
                    </section>
                    <section id="sec-50">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">A.6</span>
                                     Wave2Letter: Seen data (LIBRISPEECH audiobooks [
                                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0076">76</a>
                                    ])
                                </h3>
                            </div>
                        </header>
                        <ol class="list-no-style">
                            <li id="list173" label="(1)">
                                last two days of the voyage bartley found almost intolerable
                                <br/>
                            </li>
                            <li id="list174" label="(2)">
                                i never dreamed it would be you bartley
                                <br/>
                            </li>
                            <li id="list175" label="(3)">
                                the cuisine is the best and the chefs rank at the top of the art
                                <br/>
                            </li>
                            <li id="list176" label="(4)">
                                he pulled up a window as if the air were heavy
                                <br/>
                            </li>
                            <li id="list177" label="(5)">
                                it it hasnt always made you miserable has it
                                <br/>
                            </li>
                            <li id="list178" label="(6)">
                                always but its worse now
                                <br/>
                            </li>
                            <li id="list179" label="(7)">
                                it's unbearable it tortures me every minute
                                <br/>
                            </li>
                            <li id="list180" label="(8)">
                                i get nothing but misery out of either
                                <br/>
                            </li>
                            <li id="list181" label="(9)">
                                there is this deception between me and everything
                                <br/>
                            </li>
                            <li id="list182" label="(10)">
                                he dropped back heavily into his chair by the fire
                                <br/>
                            </li>
                            <li id="list183" label="(11)">
                                i have thought about it until i am worn out
                                <br/>
                            </li>
                            <li id="list184" label="(12)">
                                after the very first
                                <br/>
                            </li>
                            <li id="list185" label="(13)">
                                we never planned to meet and when we met
                                <br/>
                            </li>
                            <li id="list186" label="(14)">
                                i dont know what becomes of the ladies
                                <br/>
                            </li>
                            <li id="list187" label="(15)">
                                but now it doesnt seem to matter very much
                                <br/>
                            </li>
                            <li id="list188" label="(16)">
                                presently it stole back to his coat sleeve
                                <br/>
                            </li>
                            <li id="list189" label="(17)">
                                yes hilda i know that he said simply
                                <br/>
                            </li>
                            <li id="list190" label="(18)">
                                i understand bartley i was wrong
                                <br/>
                            </li>
                            <li id="list191" label="(19)">
                                season with salt and pepper and a little sugar to taste
                                <br/>
                            </li>
                            <li id="list192" label="(20)">
                                you want me to say it she whispered
                                <br/>
                            </li>
                            <li id="list193" label="(21)">
                                what alternative was there for her
                                <br/>
                            </li>
                            <li id="list194" label="(22)">
                                its got to be a clean break hilda
                                <br/>
                            </li>
                            <li id="list195" label="(23)">
                                oh bartley what am i to do
                                <br/>
                            </li>
                            <li id="list196" label="(24)">
                                you ask me to stay away from you because you want me
                                <br/>
                            </li>
                            <li id="list197" label="(25)">
                                i will ask the least imaginable but i must have something
                                <br/>
                            </li>
                            <li id="list198" label="(26)">
                                you see the treatment is a trifle fanciful
                                <br/>
                            </li>
                            <li id="list199" label="(27)">
                                he protected her and she strengthened him
                                <br/>
                            </li>
                            <li id="list200" label="(28)">
                                and then you came back not caring very much
                                <br/>
                            </li>
                            <li id="list201" label="(29)">
                                dont cry dont cry he whispered
                                <br/>
                            </li>
                            <li id="list202" label="(30)">
                                a little attack of nerves possibly
                                <br/>
                            </li>
                        </ol>
                    </section>
                    <section id="sec-51">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">A.7</span>
                                     Common unseen data (MacKenzie and Soukoreff dataset [
                                    <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0071">71</a>
                                    ])
                                </h3>
                            </div>
                        </header>
                        <ol class="list-no-style">
                            <li id="list203" label="(1)">
                                my watch fell in the water
                                <br/>
                            </li>
                            <li id="list204" label="(2)">
                                prevailing wind from the east
                                <br/>
                            </li>
                            <li id="list205" label="(3)">
                                never too rich and never too thin
                                <br/>
                            </li>
                            <li id="list206" label="(4)">
                                breathing is difficult
                                <br/>
                            </li>
                            <li id="list207" label="(5)">
                                I can see the rings on Saturn
                                <br/>
                            </li>
                            <li id="list208" label="(6)">
                                physics and chemistry are hard
                                <br/>
                            </li>
                            <li id="list209" label="(7)">
                                my bank account is overdrawn
                                <br/>
                            </li>
                            <li id="list210" label="(8)">
                                elections bring out the best
                                <br/>
                            </li>
                            <li id="list211" label="(9)">
                                you are a wonderful example
                                <br/>
                            </li>
                            <li id="list212" label="(10)">
                                do not squander your time
                                <br/>
                            </li>
                            <li id="list213" label="(11)">
                                do not drink too much
                                <br/>
                            </li>
                            <li id="list214" label="(12)">
                                take a coffee break
                                <br/>
                            </li>
                            <li id="list215" label="(13)">
                                popularity is desired by all
                                <br/>
                            </li>
                            <li id="list216" label="(14)">
                                the music is better than it sounds
                                <br/>
                            </li>
                            <li id="list217" label="(15)">
                                I agree with you
                                <br/>
                            </li>
                            <li id="list218" label="(16)">
                                do not say anything
                                <br/>
                            </li>
                            <li id="list219" label="(17)">
                                play it again Sam
                                <br/>
                            </li>
                            <li id="list220" label="(18)">
                                the force is with you
                                <br/>
                            </li>
                            <li id="list221" label="(19)">
                                we went grocery shopping
                                <br/>
                            </li>
                            <li id="list222" label="(20)">
                                the assignment is due today
                                <br/>
                            </li>
                            <li id="list223" label="(21)">
                                what you see is what you get
                                <br/>
                            </li>
                            <li id="list224" label="(22)">
                                for your information only
                                <br/>
                            </li>
                            <li id="list225" label="(23)">
                                a quarter of a century
                                <br/>
                            </li>
                            <li id="list226" label="(24)">
                                the store will close at ten
                                <br/>
                            </li>
                            <li id="list227" label="(25)">
                                head shoulders knees and toes
                                <br/>
                            </li>
                            <li id="list228" label="(26)">
                                always cover all the bases
                                <br/>
                            </li>
                            <li id="list229" label="(27)">
                                this is a very good idea
                                <br/>
                            </li>
                            <li id="list230" label="(28)">
                                can we play cards tonight
                                <br/>
                            </li>
                            <li id="list231" label="(29)">
                                get rid of that immediately
                                <br/>
                            </li>
                            <li id="list232" label="(30)">
                                public transit is much faster
                                <br/>
                            </li>
                        </ol>
                    </section>
                </section>
                <section id="sec-52">
                    <header>
                        <div class="title-info">
                            <h2>
                                <span class="section-number">B</span>
                                 ABLATION STUDIES
                            </h2>
                        </div>
                    </header>
                    <p>In this appendix, we present the results of various ablation studies performed to demonstrate the contribution of each submodule of our model to the overall performance gains.</p>
                    <section id="sec-53">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">B.1</span>
                                     With only Pre-processing
                                </h3>
                            </div>
                        </header>
                        <p>
                            The purpose of this study was to analyze the effects of pre-processing on silent speech recognition model's performance in terms of WER, WPM, CT. For evaluation, we considered all pre-trained models as baselines and compared with their conjunction with pre-processing. Results revealed that the proposed pre-processing module substantially reduced the error rates of all pre-trained models (Table 
                            <a class="tbl" href="#tab6">6</a>
                            ). In the study, pre-processing with LipNet showed 15% reduction in WER with seen and 7% reduction with unseen data. With LipType, it showed 12% reduction in WER with seen and 5.5% reduction with unseen data. With Transformer, it showed 24% reduction in WER with seen and 8% reduction with unseen data. On average, for all models, there were 5% reduction in WPM and 2 sec. increase in CT with both seen and unseen data. Note that the performance of these models with pre-processing and post-processing (repaired) are shown in Fig. 
                            <a class="fig" href="#fig9">9</a>
                            .
                        </p>
                        <div class="table-responsive" id="tab6">
                            <div class="table-caption">
                                <span class="table-number">Table 6:</span>
                                <span class="table-title">Performance evaluation of the three examined silent speech recognition models without/with the Pre-processing (PP) module in terms of WER, WPM, and CT for seen and unseen data.</span>
                            </div>
                            <table class="table">
                                <thead>
                                    <tr>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Model</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;border-bottom: 2pt solid #000000;" colspan="3">
                                            <strong>Seen</strong>
                                        </th>
                                        <th style="text-align:center;border-bottom: 2pt solid #000000;" colspan="3">
                                            <strong>Unseen</strong>
                                        </th>
                                    </tr>
                                    <tr>
                                        <th style="text-align:center;border-right: 2pt solid #000000;"></th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>WER</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>WPM</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>CT</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>WER</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>WPM</strong>
                                        </th>
                                        <th>
                                            <strong>CT</strong>
                                        </th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>LipNet</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">49.4</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">4.9</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">14.3</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">96.5</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">4.8</td>
                                        <td>14.1</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>PP + LipNet</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">42.0</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">4.8</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">16.4</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">89.5</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">4.6</td>
                                        <td>15.9</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>LipType</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">45.9</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">6.5</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">6.0</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">94.1</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">6.2</td>
                                        <td>6.2</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>PP + LipType</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">40.9</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">5.6</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">8.5</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">88.9</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">6.0</td>
                                        <td>8.1</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Transformer</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">56.2</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">6.0</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">14.9</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">82.4</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">5.9</td>
                                        <td>14.5</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>PP + Transfomer</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">42.5</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">5.9</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">16.4</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">76.0</td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">5.6</td>
                                        <td>15.8</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </section>
                    <section id="sec-54">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">B.2</span>
                                     Effects of Individual Error Correction Module
                                </h3>
                            </div>
                        </header>
                        <p>
                            We also analyzed the effects of individual error correction modules with the LipType model in terms of WER and CT. All the presented results are calculated with seen data. Results demonstrated that each submodule made a significant contribution to the overall performance improvement of the repair model (Table 
                            <a class="tbl" href="#tab7">7</a>
                            ).
                        </p>
                        <div class="table-responsive" id="tab7">
                            <div class="table-caption">
                                <span class="table-number">Table 7:</span>
                                <span class="table-title">Effect of individual error correction module on LipType's WER and CT with seen data (Pre-processing: PP; DDA: Deep denoising autoencoder; SC: Spell Checker; LM: Language Model; ED: Edit Distance). We considered DDA + SC + LM + ED as the post-processing module.</span>
                            </div>
                            <table class="table">
                                <thead>
                                    <tr>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>Method</strong>
                                        </th>
                                        <th style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>WER</strong>
                                        </th>
                                        <th>
                                            <strong>CT</strong>
                                        </th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>LipType</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">45.9</td>
                                        <td>6.0</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>PP + LipType</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">40.9</td>
                                        <td>8.3</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>PP + LipType + DDA</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">29.7</td>
                                        <td>11.1</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>PP + LipType + DDA + SC</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">27.5</td>
                                        <td>11.7</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>PP + LipType + DDA + SC + LM</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">24.1</td>
                                        <td>14.2</td>
                                    </tr>
                                    <tr style="border-bottom: 2pt solid #000000;">
                                        <td style="text-align:center;border-right: 2pt solid #000000;">
                                            <strong>PP + LipType + DDA + SC + LM + ED</strong>
                                        </td>
                                        <td style="text-align:center;border-right: 2pt solid #000000;">20.5</td>
                                        <td>15.1</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </section>
                    <section id="sec-55">
                        <header>
                            <div class="title-info">
                                <h3>
                                    <span class="section-number">B.3</span>
                                     Correction Classification
                                </h3>
                            </div>
                        </header>
                        <p>In this study, we analyzed the types of correction made by the post-processing module. For this, we classified all errors by the following criteria:</p>
                        <ul class="list-no-style">
                            <li id="list233" label="•">
                                Whether the correct word is substituted with other word(s), substitution error.
                                <br/>
                            </li>
                            <li id="list234" label="•">
                                Whether the new word(s) is inserted, insertion error.
                                <br/>
                            </li>
                            <li id="list235" label="•">
                                Whether the correct word(s) is deleted, deletion error.
                                <br/>
                            </li>
                        </ul>
                        <p>After analysis, we observed that Silent Speech has 2% insertion, 27% deletion, 71% substitution, (34% of these were on short words &lt;= 3 chars, 11% of these were on long words &gt; 3 chars, 29% of these were in starting of the phrase &lt;= length(phrase)/2-1, 14% of these were in ending of the phrase &gt; length(phrase)/2). However, speech has 38% insertion, 21% deletion, 41% substitution (12% of these were on short words &lt;= 3 chars, 18% of these were on long words &gt; 3 chars, 25% of these were in starting of the phrase &lt;= length(phrase)/2-1, 11% of these were in ending of the phrase &gt; length(phrase)/2).</p>
                        <p>Silent speech has 94.7% fewer insertion errors than speech. We speculate that this is because, for speech input, the recognition model captures background noises and recognizes them as words which resulted in more insertion errors. Unlike speech recognition, silent speech recognition just uses visual information for recognition which does not get affected by background noise. Besides, silent speech has 73.1% more substitution errors. We hypothesize that this is because it is more difficult to distinguish between homophones with just visual information due to ambiguity in lip movements, i.e., different characters that produce exactly the same lip sequence (e.g. ‘p’ and ‘b’). This may have resulted in more substituted words.</p>
                    </section>
                </section>
            </appendix>
            <section id="ref-001">
                <header>
                    <div class="title-info">
                        <h2 class="page-brake-head">REFERENCES</h2>
                    </div>
                </header>
                <ul class="bibUl">
                    <li id="BibPLXBIB0001" label="[1]">
                        Mart&iacute;n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg&nbsp;S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Tensorflow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. (March 2016). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1603.04467">http://arxiv.org/abs/1603.04467</a>
                    </li>
                    <li id="BibPLXBIB0002" label="[2]">
                        Mahmoud Afifi, Konstantinos&nbsp;G. Derpanis, Bj&ouml;rn Ommer, and Michael&nbsp;S. Brown. 2020. Learning to Correct Overexposed and Underexposed Photos. (March 2020). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/2003.11596">http://arxiv.org/abs/2003.11596</a>
                    </li>
                    <li id="BibPLXBIB0003" label="[3]">
                        Triantafyllos Afouras, Joon&nbsp;Son Chung, and Andrew Zisserman. 2018. Deep Lip Reading: A Comparison of Models and an Online Application. (June 2018). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1806.06053">http://arxiv.org/abs/1806.06053</a>
                    </li>
                    <li id="BibPLXBIB0004" label="[4]">
                        Abien&nbsp;Fred Agarap. 2019. Deep Learning using Rectified Linear Units (ReLU). (2019). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1803.08375">http://arxiv.org/abs/1803.08375</a>
                    </li>
                    <li id="BibPLXBIB0005" label="[5]">
                        Alexandre Allauzen. 2007. Error Detection in Confusion Network. In 
                        <em>INTERSPEECH</em>
                        .
                    </li>
                    <li id="BibPLXBIB0006" label="[6]">
                        Ibrahim Almajai, Stephen Cox, Richard Harvey, and Yuxuan Lan. 2016. Improved Speaker Independent Lip Reading Using Speaker Adaptive Training and Deep Neural Networks. In 
                        <em>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
                        . 2722–2726. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2016.7472172">https://doi.org/10.1109/ICASSP.2016.7472172</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0007" label="[7]">
                        Tarik Arici, Salih Dikbas, and Yucel Altunbasak. 2009. A Histogram Modification Framework and Its Application for Image Contrast Enhancement. 
                        <em>
                            <em>IEEE Transactions on Image Processing</em>
                        </em>
                         18, 9 (Sept. 2009), 1921–1935. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/TIP.2009.2021548">https://doi.org/10.1109/TIP.2009.2021548</a>
                         Conference Name: IEEE Transactions on Image Processing.
                    </li>
                    <li id="BibPLXBIB0008" label="[8]">
                        Ahmed&nbsp;Sabbir Arif and Wolfgang Stuerzlinger. 2009. Analysis of Text Entry Performance Metrics. In 
                        <em>2009 IEEE Toronto International Conference Science and Technology for Humanity (TIC-STH)</em>
                        . 100–105. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/TIC-STH.2009.5444533">https://doi.org/10.1109/TIC-STH.2009.5444533</a>
                    </li>
                    <li id="BibPLXBIB0009" label="[9]">
                        Yannis&nbsp;M. Assael, Brendan Shillingford, Shimon Whiteson, and Nando de Freitas. 2016. Lipnet: End-to-End Sentence-Level Lipreading. (Dec. 2016). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1611.01599">http://arxiv.org/abs/1611.01599</a>
                    </li>
                    <li id="BibPLXBIB0010" label="[10]">
                        Jess Bartels, D. Andreasen, P. Ehirim, Hui Mao, and P. Kennedy. 2008. Neurotrophic Electrode: Method of Assembly and Implantation into Human Motor Speech Cortex. 
                        <em>
                            <em>Journal of Neuroscience Methods</em>
                        </em>
                        (2008). 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.jneumeth.2008.06.030">https://doi.org/10.1016/j.jneumeth.2008.06.030</a>
                    </li>
                    <li id="BibPLXBIB0011" label="[11]">
                        Youssef Bassil and Paul Semaan. 2012. Asr Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset. (March 2012). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1203.5262">http://arxiv.org/abs/1203.5262</a>
                    </li>
                    <li id="BibPLXBIB0012" label="[12]">
                        Helen&nbsp;L. Bear and Richard Harvey. 2019. Alternative Visual Units for an Optimized Phoneme-Based Lipreading System. 18 (2019), 3870. 
                        <a class="link-inline force-break" href="https://doi.org/10.3390/app9183870">https://doi.org/10.3390/app9183870</a>
                    </li>
                    <li id="BibPLXBIB0013" label="[13]">
                        Jonathan&nbsp;S. Brumberg, Alfonso Nieto-Castanon, Philip&nbsp;R. Kennedy, and Frank&nbsp;H. Guenther. 2010. Brain-Computer Interfaces for Speech Communication. 
                        <em>
                            <em>Speech Communication</em>
                        </em>
                         52, 4 (April 2010), 367–379. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.specom.2010.01.001">https://doi.org/10.1016/j.specom.2010.01.001</a>
                    </li>
                    <li id="BibPLXBIB0014" label="[14]">
                        Turgay Celik and Tardi Tjahjadi. 2011. Contextual and Variational Contrast Enhancement. 
                        <em>
                            <em>IEEE Transactions on Image Processing</em>
                        </em>
                         20, 12 (Dec. 2011), 3431–3441. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/TIP.2011.2157513">https://doi.org/10.1109/TIP.2011.2157513</a>
                         Conference Name: IEEE Transactions on Image Processing.
                    </li>
                    <li id="BibPLXBIB0015" label="[15]">
                        Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. 2018. Learning to See in the Dark. (May 2018). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1805.01934">http://arxiv.org/abs/1805.01934</a>
                    </li>
                    <li id="BibPLXBIB0016" label="[16]">
                        Stanley&nbsp;F. Chen and Joshua Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In 
                        <em>Proceedings of the 34th annual meeting on Association for Computational Linguistics</em>
                        (
                        <em>ACL ’96</em>
                        ). Association for Computational Linguistics, USA, 310–318. 
                        <a class="link-inline force-break" href="https://doi.org/10.3115/981863.981904">https://doi.org/10.3115/981863.981904</a>
                    </li>
                    <li id="BibPLXBIB0017" label="[17]">
                        Wei Chen, Sankaranarayanan Ananthakrishnan, Rohit Kumar, Rohit Prasad, and Prem Natarajan. 2013. Asr Error Detection in a Conversational Spoken Language Translation System. In 
                        <em>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</em>
                        . 7418–7422. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2013.6639104">https://doi.org/10.1109/ICASSP.2013.6639104</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0018" label="[18]">
                        Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. (2014). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1412.3555">http://arxiv.org/abs/1412.3555</a>
                    </li>
                    <li id="BibPLXBIB0019" label="[19]">
                        Joon&nbsp;Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. 2017. Lip reading sentences in the wild. In 
                        <em>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>
                        . IEEE, 3444–3453.
                    </li>
                    <li id="BibPLXBIB0020" label="[20]">
                        Joon&nbsp;Son Chung and Andrew Zisserman. 2016. Out of Time: Automated Lip Sync in the Wild. In 
                        <em>ACCV Workshops</em>
                        . 
                        <a class="link-inline force-break" href="https://doi.org/10.1007/978-3-319-54427-4_19">https://doi.org/10.1007/978-3-319-54427-4_19</a>
                    </li>
                    <li id="BibPLXBIB0021" label="[21]">
                        Joon&nbsp;Son Chung and Andrew Zisserman. 2017. Lip Reading in Profile. In 
                        <em>BMVC</em>
                        . 
                        <a class="link-inline force-break" href="https://doi.org/10.5244/C.31.155">https://doi.org/10.5244/C.31.155</a>
                    </li>
                    <li id="BibPLXBIB0022" label="[22]">
                        Joon&nbsp;Son Chung and Andrew Zisserman. 2017. Lip Reading in the Wild. In Computer Vision – ACCV 2016(Lecture Notes in Computer Science), Shang-Hong Lai, Vincent Lepetit, Ko&nbsp;Nishino, and Yoichi Sato(Eds.). Springer International Publishing, Cham, 87–103. 
                        <a class="link-inline force-break" href="https://doi.org/10.1007/978-3-319-54184-6_6">https://doi.org/10.1007/978-3-319-54184-6_6</a>
                    </li>
                    <li id="BibPLXBIB0023" label="[23]">
                        Joon&nbsp;Son Chung and Andrew Zisserman. 2018. Learning to Lip Read Words by Watching Videos. 
                        <em>
                            <em>Computer Vision and Image Understanding</em>
                        </em>
                         173 (Aug. 2018), 76–85. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.cviu.2018.02.001">https://doi.org/10.1016/j.cviu.2018.02.001</a>
                    </li>
                    <li id="BibPLXBIB0024" label="[24]">
                        C. Cieri, D. Miller, and K. Walker. 2004. The Fisher Corpus: A Resource for the Next Generations of Speech-to-Text. In 
                        <em>LREC</em>
                        .
                    </li>
                    <li id="BibPLXBIB0025" label="[25]">
                        Ronan Collobert, Awni Hannun, and Gabriel Synnaeve. 2019. A Fully Differentiable Beam Search Decoder. (2019). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1902.06022">http://arxiv.org/abs/1902.06022</a>
                    </li>
                    <li id="BibPLXBIB0026" label="[26]">
                        Ronan Collobert, Christian Puhrsch, and Gabriel Synnaeve. 2016. Wav2letter: An End-to-End Convnet-Based Speech Recognition System. (Sept. 2016). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1609.03193">http://arxiv.org/abs/1609.03193</a>
                    </li>
                    <li id="BibPLXBIB0027" label="[27]">
                        Martin Cooke, Jon Barker, Stuart Cunningham, and Xu Shao. 2006. An Audio-Visual Corpus for Speech Perception and Automatic Speech Recognition. 
                        <em>
                            <em>The Journal of the Acoustical Society of America</em>
                        </em>
                         120, 5 (Oct. 2006), 2421–2424. 
                        <a class="link-inline force-break" href="https://doi.org/10.1121/1.2229005">https://doi.org/10.1121/1.2229005</a>
                         Publisher: Acoustical Society of America.
                    </li>
                    <li id="BibPLXBIB0028" label="[28]">
                        Charles&nbsp;S. DaSalla, Hiroyuki Kambara, Yasuharu Koike, and Makoto Sato. 2009. Spatial Filtering and Single-Trial Classification of Eeg During Vowel Speech Imagery. In 
                        <em>Proceedings of the 3rd International Convention on Rehabilitation Engineering &amp; Assistive Technology</em>
                        (
                        <em>i-CREATe ’09</em>
                        ). Association for Computing Machinery, New York, NY, USA, 1–4. 
                        <a class="link-inline force-break" href="https://doi.org/10.1145/1592700.1592731">https://doi.org/10.1145/1592700.1592731</a>
                    </li>
                    <li id="BibPLXBIB0029" label="[29]">
                        B. Denby, Y. Oussar, G. Dreyfus, and M. Stone. 2006. Prospects for a Silent Speech Interface Using Ultrasound Imaging. In 
                        <em>2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings</em>
                        , Vol.&nbsp;1. I–I. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2006.1660033">https://doi.org/10.1109/ICASSP.2006.1660033</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0030" label="[30]">
                        B. Denby and M. Stone. 2004. Speech Synthesis from Real Time Ultrasound Images of the Tongue. In 
                        <em>2004 IEEE International Conference on Acoustics, Speech, and Signal Processing</em>
                        , Vol.&nbsp;1. I–685. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2004.1326078">https://doi.org/10.1109/ICASSP.2004.1326078</a>
                         ISSN: 1520-6149.
                    </li>
                    <li id="BibPLXBIB0031" label="[31]">
                        Ali Diba, Mohsen Fayyaz, Vivek Sharma, M.&nbsp;Mahdi Arzani, Rahman Yousefzadeh, Juergen Gall, and Luc Van&nbsp;Gool. 2019. Spatio-Temporal Channel Correlation Networks for Action Classification. (Feb. 2019). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1806.07754">http://arxiv.org/abs/1806.07754</a>
                    </li>
                    <li id="BibPLXBIB0032" label="[32]">
                        Xuan Dong, Guan Wang, Yi Pang, Weixin Li, Jiangtao Wen, Wei Meng, and Yao Lu. 2011. Fast Efficient Algorithm for Enhancement of Low Lighting Video. In 
                        <em>2011 IEEE International Conference on Multimedia and Expo</em>
                        . 1–6. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICME.2011.6012107">https://doi.org/10.1109/ICME.2011.6012107</a>
                         ISSN: 1945-788X.
                    </li>
                    <li id="BibPLXBIB0033" label="[33]">
                        M.&nbsp;J. Fagan, S.&nbsp;R. Ell, J.&nbsp;M. Gilbert, E. Sarrazin, and P.&nbsp;M. Chapman. 2008. Development of a (silent) Speech Recognition System for Patients Following Laryngectomy. 
                        <em>
                            <em>Medical Engineering &amp; Physics</em>
                        </em>
                         30, 4 (May 2008), 419–425. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.medengphy.2007.05.003">https://doi.org/10.1016/j.medengphy.2007.05.003</a>
                    </li>
                    <li id="BibPLXBIB0034" label="[34]">
                        Xue Feng, Yaodong Zhang, and James Glass. 2014. Speech Feature Denoising and Dereverberation Via Deep Autoencoders for Noisy Reverberant Speech Recognition. In 
                        <em>2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
                        . 1759–1763. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2014.6853900">https://doi.org/10.1109/ICASSP.2014.6853900</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0035" label="[35]">
                        Victoria&nbsp;M. Florescu, L. Crevier-Buchman, B. Denby, T. Hueber, Antonia Colazo-Simon, Claire Pillot-Loiseau, P. Roussel-Ragot, C. Gendrot, and S. Quattrocchi. 2010. Silent Vs Vocalized Articulation for a Portable Ultrasound-Based Silent Speech Interface. In 
                        <em>INTERSPEECH</em>
                        .
                    </li>
                    <li id="BibPLXBIB0036" label="[36]">
                        Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. 2016. A Weighted Variational Model for Simultaneous Reflectance and Illumination Estimation. In 
                        <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>
                        . 2782–2790. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/CVPR.2016.304">https://doi.org/10.1109/CVPR.2016.304</a>
                         ISSN: 1063-6919.
                    </li>
                    <li id="BibPLXBIB0037" label="[37]">
                        Yohei Fusayasu, Katsuyuki Tanaka, Tetsuya Takiguchi, and Yasuo Ariki. 2015. Word-Error Correction of Continuous Speech Recognition Based on Normalized Relevance Distance. In 
                        <em>Proceedings of the 24th International Conference on Artificial Intelligence</em>
                        (
                        <em>IJCAI’15</em>
                        ). AAAI Press, Buenos Aires, Argentina, 1257–1262.
                    </li>
                    <li id="BibPLXBIB0038" label="[38]">
                        James&nbsp;M. Gilbert, Sergey&nbsp;I. Rybchenko, Robin Hofe, Stephen&nbsp;R. Ell, Michael&nbsp;J. Fagan, Roger&nbsp;K. Moore, and Phil&nbsp;D. Green. 2010. Isolated Word Recognition of Silent Speech Using Magnetic Implants and Sensors.
                        <em>
                            <em>Medical engineering &amp; physics</em>
                        </em>
                        10 (2010), 1189–1197. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.medengphy.2010.08.011">https://doi.org/10.1016/j.medengphy.2010.08.011</a>
                    </li>
                    <li id="BibPLXBIB0039" label="[39]">
                        J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992. Switchboard: Telephone Speech Corpus for Research and Development. In 
                        <em>[Proceedings]ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing</em>
                        , Vol.&nbsp;1. 517–520 vol.1. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.1992.225858">https://doi.org/10.1109/ICASSP.1992.225858</a>
                         ISSN: 1520-6149.
                    </li>
                    <li id="BibPLXBIB0040" label="[40]">
                        Alex Graves, Santiago Fern&aacute;ndez, Faustino Gomez, and J&uuml;rgen Schmidhuber. 2006. Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. In Proceedings of the 23rd international conference on Machine learning (Pittsburgh, Pennsylvania, USA, 2006-06-25) (ICML ’06). Association for Computing Machinery, 369–376. 
                        <a class="link-inline force-break" href="https://doi.org/10.1145/1143844.1143891">https://doi.org/10.1145/1143844.1143891</a>
                    </li>
                    <li id="BibPLXBIB0041" label="[41]">
                        Alex Graves and Navdeep Jaitly. 2014. Towards End-to-End Speech Recognition with Recurrent Neural Networks. In 
                        <em>Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32</em>
                        (
                        <em>ICML’14</em>
                        ). JMLR.org, Beijing, China, II–1764–II–1772.
                    </li>
                    <li id="BibPLXBIB0042" label="[42]">
                        Xiaojie Guo, Yu Li, and Haibin Ling. 2017. Lime: Low-Light Image Enhancement Via Illumination Map Estimation. 
                        <em>
                            <em>IEEE Transactions on Image Processing</em>
                        </em>
                         26, 2 (Feb. 2017), 982–993. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/TIP.2016.2639450">https://doi.org/10.1109/TIP.2016.2639450</a>
                         Conference Name: IEEE Transactions on Image Processing.
                    </li>
                    <li id="BibPLXBIB0043" label="[43]">
                        Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew&nbsp;Y. Ng. 2014. Deep Speech: Scaling up End-to-End Speech Recognition. (Dec. 2014). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1412.5567">http://arxiv.org/abs/1412.5567</a>
                    </li>
                    <li id="BibPLXBIB0044" label="[44]">
                        Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In 
                        <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>
                        . 770–778. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>
                         ISSN: 1063-6919.
                    </li>
                    <li id="BibPLXBIB0045" label="[45]">
                        Kenneth Heafield, Ivan Pouzyrevsky, J. Clark, and Philipp Koehn. 2013. Scalable Modified Kneser-Ney Language Model Estimation. In 
                        <em>ACL</em>
                        .
                    </li>
                    <li id="BibPLXBIB0046" label="[46]">
                        Panikos Heracleous and Norihiro Hagita. 2011. Automatic Recognition of Speech Without Any Audio Information. In 
                        <em>2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
                        . 2392–2395. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2011.5946965">https://doi.org/10.1109/ICASSP.2011.5946965</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0047" label="[47]">
                        Panikos Heracleous, Tomomi Kaino, H. Saruwatari, and K. Shikano. 2007. Unvoiced Speech Recognition Using Tissue-Conductive Acoustic Sensor. 
                        <em>
                            <em>EURASIP J. Adv. Signal Process.</em>
                        </em>
                        (2007). 
                        <a class="link-inline force-break" href="https://doi.org/10.1155/2007/94068">https://doi.org/10.1155/2007/94068</a>
                    </li>
                    <li id="BibPLXBIB0048" label="[48]">
                        Tatsuya Hirahara, Makoto Otani, Shota Shimizu, Tomoki Toda, Keigo Nakamura, Yoshitaka Nakajima, and Kiyohiro Shikano. 2010. Silent-Speech Enhancement Using Body-Conducted Vocal-Tract Resonance Signals. 
                        <em>
                            <em>Speech Communication</em>
                        </em>
                         52, 4 (April 2010), 301–313. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.specom.2009.12.001">https://doi.org/10.1016/j.specom.2009.12.001</a>
                    </li>
                    <li id="BibPLXBIB0049" label="[49]">
                        Alain Hor&eacute; and Djemel Ziou. 2010. Image Quality Metrics: Psnr Vs. Ssim. In 
                        <em>2010 20th International Conference on Pattern Recognition</em>
                        . 2366–2369. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICPR.2010.579">https://doi.org/10.1109/ICPR.2010.579</a>
                         ISSN: 1051-4651.
                    </li>
                    <li id="BibPLXBIB0050" label="[50]">
                        Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. 2019. Squeeze-and-Excitation Networks. (May 2019). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1709.01507">http://arxiv.org/abs/1709.01507</a>
                    </li>
                    <li id="BibPLXBIB0051" label="[51]">
                        T. Hueber, G. Aversano, G. Chollet, B. Denby, G. Dreyfus, Y. Oussar, P. Roussel, and M. Stone. 2007. Eigentongue Feature Extraction for an Ultrasound-Based Silent Speech Interface. In 
                        <em>2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP ’07</em>
                        , Vol.&nbsp;1. I–1245–I–1248. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2007.366140">https://doi.org/10.1109/ICASSP.2007.366140</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0052" label="[52]">
                        Thomas Hueber, Elie-Laurent Benaroya, G&eacute;rard Chollet, Bruce Denby, G&eacute;rard Dreyfus, and Maureen Stone. 2010. Development of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips. 
                        <em>
                            <em>Speech Communication</em>
                        </em>
                         52, 4 (April 2010), 288–300. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.specom.2009.11.004">https://doi.org/10.1016/j.specom.2009.11.004</a>
                    </li>
                    <li id="BibPLXBIB0053" label="[53]">
                        Thomas Hueber, Elie-Laurent Benaroya, G&eacute;rard Chollet, Bruce Denby, G&eacute;rard Dreyfus, and Maureen Stone. 2010. Development of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips. 
                        <em>
                            <em>Speech Communication</em>
                        </em>
                         52, 4 (April 2010), 288–300. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.specom.2009.11.004">https://doi.org/10.1016/j.specom.2009.11.004</a>
                    </li>
                    <li id="BibPLXBIB0054" label="[54]">Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37 (Lille, France, 2015-07-06) (ICML’15). JMLR.org, 448–456.</li>
                    <li id="BibPLXBIB0055" label="[55]">
                        Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. [n.d.]. 3D Convolutional Neural Networks for Human Action Recognition. 35, 1([n.&nbsp;d.]), 221–231. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/TPAMI.2012.59">https://doi.org/10.1109/TPAMI.2012.59</a>
                         Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.
                    </li>
                    <li id="BibPLXBIB0056" label="[56]">
                        D.J. Jobson, Z. Rahman, and G.A. Woodell. 1997. A Multiscale Retinex for Bridging the Gap Between Color Images and the Human Observation of Scenes. 
                        <em>
                            <em>IEEE Transactions on Image Processing</em>
                        </em>
                         6, 7 (July 1997), 965–976. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/83.597272">https://doi.org/10.1109/83.597272</a>
                         Conference Name: IEEE Transactions on Image Processing.
                    </li>
                    <li id="BibPLXBIB0057" label="[57]">
                        Charles Jorgensen and Sorin Dusan. 2010. Speech Interfaces Based Upon Surface Electromyography. 
                        <em>
                            <em>Speech Communication</em>
                        </em>
                         52, 4 (April 2010), 354–366. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.specom.2009.11.003">https://doi.org/10.1016/j.specom.2009.11.003</a>
                    </li>
                    <li id="BibPLXBIB0058" label="[58]">
                        C. Jorgensen, D.D. Lee, and S. Agabont. 2003. Sub Auditory Speech Recognition Based on Emg Signals. In 
                        <em>Proceedings of the International Joint Conference on Neural Networks, 2003.</em>
                        , Vol.&nbsp;4. 3128–3133 vol.4. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/IJCNN.2003.1224072">https://doi.org/10.1109/IJCNN.2003.1224072</a>
                         ISSN: 1098-7576.
                    </li>
                    <li id="BibPLXBIB0059" label="[59]">
                        S. Jou, Tanja Schultz, Matthias Walliczek, F. Kraft, and Alexander&nbsp;H. Waibel. 2006. Towards Continuous Speech Recognition Using Surface Electromyography. In 
                        <em>INTERSPEECH</em>
                        .
                    </li>
                    <li id="BibPLXBIB0060" label="[60]">
                        Arnav Kapur, Shreyas Kapur, and Pattie Maes. 2018. Alterego: A Personalized Wearable Silent Speech Interface. In 
                        <em>23rd International Conference on Intelligent User Interfaces</em>
                        (
                        <em>IUI ’18</em>
                        ). Association for Computing Machinery, New York, NY, USA, 43–53. 
                        <a class="link-inline force-break" href="https://doi.org/10.1145/3172944.3172977">https://doi.org/10.1145/3172944.3172977</a>
                    </li>
                    <li id="BibPLXBIB0061" label="[61]">
                        Naoki Kimura, Michinari Kono, and Jun Rekimoto. 2019. SottoVoce: An Ultrasound Imaging-Based Silent Speech Interaction Using Deep Neural Networks. In 
                        <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>
                        (
                        <em>CHI ’19</em>
                        ). Association for Computing Machinery, New York, NY, USA, 1–11. 
                        <a class="link-inline force-break" href="https://doi.org/10.1145/3290605.3300376">https://doi.org/10.1145/3290605.3300376</a>
                    </li>
                    <li id="BibPLXBIB0062" label="[62]">
                        Davis&nbsp;E. King. 2009. Dlib-Ml: A Machine Learning Toolkit. 
                        <em>
                            <em>The Journal of Machine Learning Research</em>
                        </em>
                         10 (Dec. 2009), 1755–1758.
                    </li>
                    <li id="BibPLXBIB0063" label="[63]">
                        Diederik&nbsp;P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Optimization. (2017). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a>
                    </li>
                    <li id="BibPLXBIB0064" label="[64]">
                        Oscar Koller, Hermann Ney, and Richard Bowden. 2015. Deep Learning of Mouth Shapes for Sign Language. In 
                        <em>2015 IEEE International Conference on Computer Vision Workshop (ICCVW)</em>
                        . 477–483. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICCVW.2015.69">https://doi.org/10.1109/ICCVW.2015.69</a>
                    </li>
                    <li id="BibPLXBIB0065" label="[65]">
                        E.&nbsp;H. Land and J. McCann. 1971. Lightness and Retinex Theory.
                        <em>
                            <em>Journal of the Optical Society of America</em>
                        </em>
                        (1971). 
                        <a class="link-inline force-break" href="https://doi.org/10.1364/JOSA.61.000001">https://doi.org/10.1364/JOSA.61.000001</a>
                    </li>
                    <li id="BibPLXBIB0066" label="[66]">
                        Chulwoo Lee, Chul Lee, and Chang-Su Kim. 2013. Contrast Enhancement Based on Layered Difference Representation of 2d Histograms. 
                        <em>
                            <em>IEEE Transactions on Image Processing</em>
                        </em>
                         22, 12 (Dec. 2013), 5372–5384. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/TIP.2013.2284059">https://doi.org/10.1109/TIP.2013.2284059</a>
                         Conference Name: IEEE Transactions on Image Processing.
                    </li>
                    <li id="BibPLXBIB0067" label="[67]">
                        Chongyi Li, J. Guo, F. Porikli, and Y. Pang. 2018. Lightennet: A Convolutional Neural Network for Weakly Illuminated Image Enhancement. 
                        <em>
                            <em>Pattern Recognit. Lett.</em>
                        </em>
                        (2018). 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.patrec.2018.01.010">https://doi.org/10.1016/j.patrec.2018.01.010</a>
                    </li>
                    <li id="BibPLXBIB0068" label="[68]">
                        Yuan Liang, Koji Iwano, and Koichi Shinoda. 2014. An Efficient Error Correction Interface for Speech Recognition on Mobile Touchscreen Devices. In 
                        <em>2014 IEEE Spoken Language Technology Workshop (SLT)</em>
                        . 454–459. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/SLT.2014.7078617">https://doi.org/10.1109/SLT.2014.7078617</a>
                    </li>
                    <li id="BibPLXBIB0069" label="[69]">
                        X. Lu, Y. Tsao, S. Matsuda, and C. Hori. 2013. Speech Enhancement Based on Deep Denoising Autoencoder. In 
                        <em>INTERSPEECH</em>
                        .
                    </li>
                    <li id="BibPLXBIB0070" label="[70]">
                        Andrew&nbsp;L. Maas, Ziang Xie, Dan Jurafsky, and A. Ng. 2015. Lexicon-Free Conversational Speech Recognition with Neural Networks. In 
                        <em>HLT-NAACL</em>
                        . 
                        <a class="link-inline force-break" href="https://doi.org/10.3115/v1/N15-1038">https://doi.org/10.3115/v1/N15-1038</a>
                    </li>
                    <li id="BibPLXBIB0071" label="[71]">
                        I.&nbsp;Scott MacKenzie and R.&nbsp;William Soukoreff. 2003. Phrase Sets for Evaluating Text Entry Techniques. In 
                        <em>CHI ’03 Extended Abstracts on Human Factors in Computing Systems</em>
                        (
                        <em>CHI EA ’03</em>
                        ). Association for Computing Machinery, New York, NY, USA, 754–755. 
                        <a class="link-inline force-break" href="https://doi.org/10.1145/765891.765971">https://doi.org/10.1145/765891.765971</a>
                    </li>
                    <li id="BibPLXBIB0072" label="[72]">
                        L. Maier-Hein, F. Metze, T. Schultz, and A. Waibel. 2005. Session Independent Non-Audible Speech Recognition Using Surface Electromyography. In 
                        <em>IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.</em>
                        331–336. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ASRU.2005.1566521">https://doi.org/10.1109/ASRU.2005.1566521</a>
                    </li>
                    <li id="BibPLXBIB0073" label="[73]">
                        Y. Nakajima, H. Kashioka, K. Shikano, and N. Campbell. 2003. Non-Audible Murmur Recognition Input Interface Using Stethoscopic Microphone Attached to the Skin. In 
                        <em>2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP ’03).</em>
                        , Vol.&nbsp;5. V–708. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2003.1200069">https://doi.org/10.1109/ICASSP.2003.1200069</a>
                         ISSN: 1520-6149.
                    </li>
                    <li id="BibPLXBIB0074" label="[74]">
                        L.C. Ng, G.C. Burnett, J.F. Holzrichter, and T.J. Gable. 2000. Denoising of Human Speech Using Combined Acoustic and Em Sensor Signal Processing. In 
                        <em>2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)</em>
                        , Vol.&nbsp;1. 229–232 vol.1. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2000.861925">https://doi.org/10.1109/ICASSP.2000.861925</a>
                         ISSN: 1520-6149.
                    </li>
                    <li id="BibPLXBIB0075" label="[75]">
                        K. Noda, Y. Yamaguchi, K. Nakadai, H. Okuno, and Tetsuya Ogata. 2014. Lipreading Using Convolutional Neural Network. In 
                        <em>INTERSPEECH</em>
                        .
                    </li>
                    <li id="BibPLXBIB0076" label="[76]">
                        Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An Asr Corpus Based on Public Domain Audio Books. In 
                        <em>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
                        . 5206–5210. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2015.7178964">https://doi.org/10.1109/ICASSP.2015.7178964</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0077" label="[77]">
                        Sanjay&nbsp;A. Patil and John H.&nbsp;L. Hansen. 2010. The Physiological Microphone (pmic): A Competitive Alternative for Speaker Assessment in Stress Detection and Speaker Verification. 
                        <em>
                            <em>Speech Communication</em>
                        </em>
                         52, 4 (April 2010), 327–340. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.specom.2009.11.006">https://doi.org/10.1016/j.specom.2009.11.006</a>
                    </li>
                    <li id="BibPLXBIB0078" label="[78]">
                        Douglas&nbsp;B. Paul and Janet&nbsp;M. Baker. 1992. The Design for the Wall Street Journal-Based Csr Corpus. In 
                        <em>Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992</em>
                        . 
                        <a class="link-inline force-break" href="https://www.aclweb.org/anthology/H92-1073">https://www.aclweb.org/anthology/H92-1073</a>
                    </li>
                    <li id="BibPLXBIB0079" label="[79]">
                        Thomas Pellegrini and Isabel Trancoso. 2009. Error Detection in Broadcast News Asr Using Markov Chains. In 
                        <em>Proceedings of the 4th conference on Human language technology: challenges for computer science and linguistics</em>
                        (
                        <em>LTC’09</em>
                        ). Springer-Verlag, Berlin, Heidelberg, 59–69.
                    </li>
                    <li id="BibPLXBIB0080" label="[80]">
                        T. Pellegrini and I. Trancoso. 2010. Improving Asr Error Detection with Non-Decoder Based Features. In 
                        <em>INTERSPEECH</em>
                        .
                    </li>
                    <li id="BibPLXBIB0081" label="[81]">
                        Stavros Petridis and Maja Pantic. 2016. Deep Complementary Bottleneck Features for Visual Speech Recognition. In 
                        <em>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
                        . 2304–2308. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2016.7472088">https://doi.org/10.1109/ICASSP.2016.7472088</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0082" label="[82]">
                        Stavros Petridis and Maja Pantic. 2016. Deep Complementary Bottleneck Features for Visual Speech Recognition. In 
                        <em>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
                        . 2304–2308. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2016.7472088">https://doi.org/10.1109/ICASSP.2016.7472088</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0083" label="[83]">
                        Anne Porbadnigk, Marek Wester, Jan Calliess, and Tanja Schultz. 2009. EEG-based Speech Recognition - Impact of Temporal Effects. In 
                        <em>BIOSIGNALS</em>
                        . 
                        <a class="link-inline force-break" href="https://doi.org/10.5220/0001554303760381">https://doi.org/10.5220/0001554303760381</a>
                    </li>
                    <li id="BibPLXBIB0084" label="[84]">
                        Anne Porbadnigk, Marek Wester, Jan Calliess, and Tanja Schultz. 2009. EEG-Based Speech Recognition - Impact of Temporal Effects. In 
                        <em>BIOSIGNALS</em>
                        . 
                        <a class="link-inline force-break" href="https://doi.org/10.5220/0001554303760381">https://doi.org/10.5220/0001554303760381</a>
                    </li>
                    <li id="BibPLXBIB0085" label="[85]">
                        Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recognition Toolkit. 
                        <a class="link-inline force-break" href="https://infoscience.epfl.ch/record/192584">https://infoscience.epfl.ch/record/192584</a>
                         Conference Name: IEEE 2011 Workshop on Automatic Speech Recognition and Understanding Number: CONF Publisher: IEEE Signal Processing Society.
                    </li>
                    <li id="BibPLXBIB0086" label="[86]">
                        T.F. Quatieri, K. Brady, D. Messing, J.P. Campbell, W.M. Campbell, M.S. Brandstein, C.J. Weinstein, J.D. Tardelli, and P.D. Gatewood. 2006. Exploiting Nonacoustic Sensors for Speech Encoding. 
                        <em>
                            <em>IEEE Transactions on Audio, Speech, and Language Processing</em>
                        </em>
                         14, 2 (March 2006), 533–544. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/TSA.2005.855838">https://doi.org/10.1109/TSA.2005.855838</a>
                         Conference Name: IEEE Transactions on Audio, Speech, and Language Processing.
                    </li>
                    <li id="BibPLXBIB0087" label="[87]">
                        Michael Riley, Cyril Allauzen, and Martin Jansche. 2009. Openfst: An Open-Source, Weighted Finite-State Transducer Library and Its Applications to Speech and Language. In 
                        <em>Proceedings of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies (NAACL HLT) 2009 conference, Tutorials</em>
                        . 
                        <a class="link-inline force-break" href="http://aclweb.org/anthology/N09-4005">http://aclweb.org/anthology/N09-4005</a>
                    </li>
                    <li id="BibPLXBIB0088" label="[88]">
                        Christos Sagonas, Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 2013. 300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge. In 
                        <em>2013 IEEE International Conference on Computer Vision Workshops</em>
                        . 397–403. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICCVW.2013.59">https://doi.org/10.1109/ICCVW.2013.59</a>
                    </li>
                    <li id="BibPLXBIB0089" label="[89]">
                        Arup Sarma and David&nbsp;D. Palmer. 2004. Context-Based Speech Recognition Error Detection and Correction. In 
                        <em>Proceedings of HLT-NAACL 2004: Short Papers</em>
                        (
                        <em>HLT-NAACL-Short ’04</em>
                        ). Association for Computational Linguistics, USA, 85–88.
                    </li>
                    <li id="BibPLXBIB0090" label="[90]">
                        Tanja Schultz and Michael Wand. 2010. Modeling Coarticulation in Emg-Based Continuous Speech Recognition. 
                        <em>
                            <em>Speech Communication</em>
                        </em>
                         52, 4 (April 2010), 341–353. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.specom.2009.12.002">https://doi.org/10.1016/j.specom.2009.12.002</a>
                    </li>
                    <li id="BibPLXBIB0091" label="[91]">
                        A.R. Setlur, R.A. Sukkar, and J. Jacob. 1996. Correcting Recognition Errors Via Discriminative Utterance Verification. In 
                        <em>Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP ’96</em>
                        , Vol.&nbsp;2. 602–605 vol.2. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICSLP.1996.607433">https://doi.org/10.1109/ICSLP.1996.607433</a>
                    </li>
                    <li id="BibPLXBIB0092" label="[92]">Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. 15, 1 (2014), 1929–1958.</li>
                    <li id="BibPLXBIB0093" label="[93]">
                        Themos Stafylakis and Georgios Tzimiropoulos. 2017. Combining Residual Networks with Lstms for Lipreading. 
                        <em>
                            <em>INTERSPEECH</em>
                        </em>
                         (2017). 
                        <a class="link-inline force-break" href="https://doi.org/10.21437/INTERSPEECH.2017-85">https://doi.org/10.21437/INTERSPEECH.2017-85</a>
                    </li>
                    <li id="BibPLXBIB0094" label="[94]">
                        Ke Sun, Chun Yu, Weinan Shi, Lan Liu, and Yuanchun Shi. 2018. Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands. In 
                        <em>Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology</em>
                        (
                        <em>UIST ’18</em>
                        ). Association for Computing Machinery, New York, NY, USA, 581–593. 
                        <a class="link-inline force-break" href="https://doi.org/10.1145/3242587.3242599">https://doi.org/10.1145/3242587.3242599</a>
                    </li>
                    <li id="BibPLXBIB0095" label="[95]">
                        P. Suppes, B. Han, and Z. Lu. 1998. Brain-Wave Recognition of Sentences.
                        <em>
                            <em>Proceedings of the National Academy of Sciences of the United States of America</em>
                        </em>
                         (1998). 
                        <a class="link-inline force-break" href="https://doi.org/10.1073/pnas.95.26.15861">https://doi.org/10.1073/pnas.95.26.15861</a>
                    </li>
                    <li id="BibPLXBIB0096" label="[96]">
                        P. Suppes, Z. Lu, and B. Han. 1997. Brain Wave Recognition of Words.
                        <em>
                            <em>Proceedings of the National Academy of Sciences of the United States of America</em>
                        </em>
                         (1997). 
                        <a class="link-inline force-break" href="https://doi.org/10.1073/pnas.94.26.14965">https://doi.org/10.1073/pnas.94.26.14965</a>
                    </li>
                    <li id="BibPLXBIB0097" label="[97]">
                        Satoshi Tamura, Hiroshi Ninomiya, Norihide Kitaoka, Shin Osuga, Yurie Iribe, Kazuya Takeda, and Satoru Hayamizu. 2015. Audio-Visual Speech Recognition Using Deep Bottleneck Features and High-Performance Lipreading. In 
                        <em>2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)</em>
                        . 575–582. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/APSIPA.2015.7415335">https://doi.org/10.1109/APSIPA.2015.7415335</a>
                    </li>
                    <li id="BibPLXBIB0098" label="[98]">
                        Ingo&nbsp;R. Titze, Brad&nbsp;H. Story, Gregory&nbsp;C. Burnett, John&nbsp;F. Holzrichter, Lawrence&nbsp;C. Ng, and Wayne&nbsp;A. Lea. 1999. Comparison Between Electroglottography and Electromagnetic Glottography. 
                        <em>
                            <em>The Journal of the Acoustical Society of America</em>
                        </em>
                         107, 1 (Dec. 1999), 581–588. 
                        <a class="link-inline force-break" href="https://doi.org/10.1121/1.428324">https://doi.org/10.1121/1.428324</a>
                         Publisher: Acoustical Society of America.
                    </li>
                    <li id="BibPLXBIB0099" label="[99]">
                        Naoya Ukai, Takumi Seko, Satoshi Tamura, and Satoru Hayamizu. 2012. Gif-Lr: GA-Based Informative Feature for Lipreading. In 
                        <em>Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference</em>
                        . 1–4.
                    </li>
                    <li id="BibPLXBIB0100" label="[100]">
                        Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. (Dec. 2017). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>
                    </li>
                    <li id="BibPLXBIB0101" label="[101]">
                        Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and Composing Robust Features with Denoising Autoencoders. In 
                        <em>Proceedings of the 25th international conference on Machine learning</em>
                        (
                        <em>ICML ’08</em>
                        ). Association for Computing Machinery, New York, NY, USA, 1096–1103. 
                        <a class="link-inline force-break" href="https://doi.org/10.1145/1390156.1390294">https://doi.org/10.1145/1390156.1390294</a>
                    </li>
                    <li id="BibPLXBIB0102" label="[102]">
                        Michael Wand, Jan Koutn&iacute;k, and J&uuml;rgen Schmidhuber. 2016. Lipreading with Long Short-Term Memory. In 
                        <em>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
                        . 6115–6119. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/ICASSP.2016.7472852">https://doi.org/10.1109/ICASSP.2016.7472852</a>
                         ISSN: 2379-190X.
                    </li>
                    <li id="BibPLXBIB0103" label="[103]">
                        Michael Wand and Tanja Schultz. 2011. Session-Independent Emg-Based Speech Recognition. In 
                        <em>BIOSIGNALS</em>
                        . 
                        <a class="link-inline force-break" href="https://doi.org/10.5220/0003169702950300">https://doi.org/10.5220/0003169702950300</a>
                    </li>
                    <li id="BibPLXBIB0104" label="[104]">
                        Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. 2013. Naturalness Preserved Enhancement Algorithm for Non-Uniform Illumination Images. 
                        <em>
                            <em>IEEE Transactions on Image Processing</em>
                        </em>
                         22, 9 (Sept. 2013), 3538–3548. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/TIP.2013.2261309">https://doi.org/10.1109/TIP.2013.2261309</a>
                         Conference Name: IEEE Transactions on Image Processing.
                    </li>
                    <li id="BibPLXBIB0105" label="[105]">
                        Wenjing Wang, Chen Wei, Wenhan Yang, and Jiaying Liu. 2018. Gladnet: Low-Light Enhancement Network with Global Awareness. In 
                        <em>2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)</em>
                        . 751–755. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/FG.2018.00118">https://doi.org/10.1109/FG.2018.00118</a>
                    </li>
                    <li id="BibPLXBIB0106" label="[106]">
                        Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. 2018. Deep Retinex Decomposition for Low-Light Enhancement. (Aug. 2018). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1808.04560">http://arxiv.org/abs/1808.04560</a>
                    </li>
                    <li id="BibPLXBIB0107" label="[107]">
                        Kai Xu, Dawei Li, Nick Cassimatis, and Xiaolong Wang. 2018. LCANet: End-to-end Lipreading with Cascaded Attention-CTC. In 
                        <em>2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</em>
                        . IEEE, 548–555.
                    </li>
                    <li id="BibPLXBIB0108" label="[108]">
                        Zhilu Zhang and Mert&nbsp;R. Sabuncu. 2018. Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels. (Nov. 2018). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1805.07836">http://arxiv.org/abs/1805.07836</a>
                    </li>
                    <li id="BibPLXBIB0109" label="[109]">
                        Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. 2018. Loss Functions for Neural Networks for Image Processing. (April 2018). 
                        <a class="link-inline force-break" href="http://arxiv.org/abs/1511.08861">http://arxiv.org/abs/1511.08861</a>
                    </li>
                    <li id="BibPLXBIB0110" label="[110]">
                        Lina Zhou, Yongmei Shi, Jinjuan Feng, and A. Sears. 2005. Data Mining for Detecting Errors in Dictation Speech Recognition. 
                        <em>
                            <em>IEEE Transactions on Speech and Audio Processing</em>
                        </em>
                         13, 5 (Sept. 2005), 681–688. 
                        <a class="link-inline force-break" href="https://doi.org/10.1109/TSA.2005.851874">https://doi.org/10.1109/TSA.2005.851874</a>
                         Conference Name: IEEE Transactions on Speech and Audio Processing.
                    </li>
                    <li id="BibPLXBIB0111" label="[111]">
                        Ziheng Zhou, Guoying Zhao, Xiaopeng Hong, and Matti Pietik&auml;inen. 2014. A Review of Recent Advances in Visual Speech Decoding. 
                        <em>
                            <em>Image and Vision Computing</em>
                        </em>
                         32, 9 (Sept. 2014), 590–605. 
                        <a class="link-inline force-break" href="https://doi.org/10.1016/j.imavis.2014.06.004">https://doi.org/10.1016/j.imavis.2014.06.004</a>
                    </li>
                </ul>
            </section>
        </section>
        <section id="foot-001" class="footnote">
            <header>
                <div class="title-info">
                    <h2>FOOTNOTE</h2>
                </div>
            </header>
            <p id="fn1">
                <a href="#foot-fn1">
                    <sup>1</sup>
                </a>
                Statistics on voice, speech, and language, 
                <a class="link-inline force-break" href="https://www.nidcd.nih.gov/health/statistics/statistics-voice-speech-and-language">https://www.nidcd.nih.gov/health/statistics/statistics-voice-speech-and-language</a>
            </p>
            <p id="fn2">
                <a href="#foot-fn2">
                    <sup>2</sup>
                </a>
                Source code, 
                <a class="link-inline force-break" href="https://github.com/hci-ucm/LipType">https://github.com/hci-ucm/LipType</a>
            </p>
            <p id="fn3">
                <a href="#foot-fn3">
                    <sup>3</sup>
                </a>
                Dataset, 
                <a class="link-inline force-break" href="https://www.asarif.com/resources/LipType_Data.zip">https://www.asarif.com/resources/LipType_Data.zip</a>
            </p>
            <p id="fn4">
                <a href="#foot-fn4">
                    <sup>4</sup>
                </a>
                Visemes are visual equivalent of phonemes. A viseme represents the position of the face and mouth when making a sound.
            </p>
            <p id="fn5">
                <a href="#foot-fn5">
                    <sup>5</sup>
                </a>
                In order to reduce computation, we changed the GLADNet network dimension from five down- and five up-sampling blocks to three down- and three up-sampling blocks. A preliminary investigation did not identify a significant effect on variations in layer dimensions on the network's performance.
            </p>
            <p id="fn6">
                <a href="#foot-fn6">
                    <sup>6</sup>
                </a>
                In an investigation, results were not affected by small variations in 
                <em>α</em>
                .
            </p>
            <p id="fn7">
                <a href="#foot-fn7">
                    <sup>7</sup>
                </a>
                Encodes categorical data using a one-of-K scheme.
            </p>
            <p id="fn8">
                <a href="#foot-fn8">
                    <sup>8</sup>
                </a>
                How to write a spelling corrector, 
                <a class="link-inline force-break" href="http://norvig.com/spell-correct.html">http://norvig.com/spell-correct.html</a>
            </p>
            <p id="fn9">
                <a href="#foot-fn9">
                    <sup>9</sup>
                </a>
                A count-based LM follows the general idea of making 
                <em>
                    n
                    <sup>th</sup>
                </em>
                 order Markov assumptions and calculating the n-gram probabilities through the means of counting.
            </p>
            <p id="fn10">
                <a href="#foot-fn10">
                    <sup>10</sup>
                </a>
                Sentence error rate (SER) signifies the percentage of recognized sentences that are not an exact match of the ground truth.
            </p>
            <p id="fn11">
                <a href="#foot-fn11">
                    <sup>11</sup>
                </a>
                Perplexity is the multiplicative inverse of the probability assigned to the sentence by the language model, normalized by the number of words in the sentence. The lower the perplexity the better the language model.
            </p>
            <p id="fn12">
                <a href="#foot-fn12">
                    <sup>12</sup>
                </a>
                Natural Language Toolkit (NLTK), 
                <a class="link-inline force-break" href="https://www.nltk.org/api/nltk.lm.html">https://www.nltk.org/api/nltk.lm.html</a>
            </p>
            <p id="fn13">
                <a href="#foot-fn13">
                    <sup>13</sup>
                </a>
                Zoom, 
                <a class="link-inline force-break" href="https://zoom.us">https://zoom.us</a>
            </p>
            <p id="fn14">
                <a href="#foot-fn14">
                    <sup>14</sup>
                </a>
                Uttering phrases without vocalizing any sound
            </p>
            <div class="bibStrip">
                <p>
                    <img src="https://www.acm.org/binaries/content/gallery/acm/publications/cc-by/cc-by.jpg" class="img-responsive" alt="CC-BY license image"/>
                    <br/>
                     This work is licensed under a 
                    <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution International 4.0 License</a>
                    .
                </p>
                <p>
                    <em>CHI '21, May 08–13, 2021, Yokohama, Japan</em>
                </p>
                <p>
                    &copy; 2021 Copyright held by the owner/author(s).
                    <br/>
                     ACM ISBN 978-1-4503-8096-6/21/05.
                    <br/>
                    DOI: 
                    <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3411764.3445565">https://doi.org/10.1145/3411764.3445565</a>
                </p>
            </div>
        </section>
    </main>
    </div>  
  </body>
</html>